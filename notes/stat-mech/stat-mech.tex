\documentclass{article}

\usepackage{coursenotes}

\set{AuthorName}{TC Fraser}
\set{Email}{tcfraser@tcfraser.com}
\set{Website}{www.tcfraser.com}
\set{ClassName}{Statistical Mechanics}
\set{School}{University of Waterloo}
\set{CourseCode}{Phys 359}
\set{InstructorName}{Michel Gingras}
\set{Term}{Winter 2016}
\set{Version}{1.1}

\pgfplotsset{
    standardplot/.style={
        axis x line=middle,
        axis y line=middle,
        enlarge x limits=0.05,
        enlarge y limits=0.05,
        every axis x label/.style={at={(current axis.right of origin)},anchor=north east},
        every axis y label/.style={at={(current axis.above origin)},anchor=north east},
        ytick=\empty,
        xtick=\empty,
        width=3in,
        height=2.5in,
        axis line style=thick,
    }
}

% Used to draw engine diagrams (only used once) TODO clean this up a bit.
\usetikzlibrary{shapes,arrows}
\tikzset{%
pics/.cd,
nodea/.style args={#1#2#3}{
    code={\node[minimum height=2cm] (#3) {\color{#1}#2};
             \draw[thick] (#3.south west) -| (#3.north east)--(#3.north west);
    }
},
%pics/.cd,
nodeb/.style args={#1#2#3}{
    code={\node[minimum height=2cm] (#3) {\color{#1}#2};
             \draw[thick] (#3.south east) -| (#3.north west)--(#3.north east);
    }
},
%pics/.cd,
nodec/.style args={#1#2#3}{
    code={\node[draw,thick,shape=circle,inner sep=1cm] (#3) {\color{#1}#2};
    }
},
}

\begin{document}

\titlePage

\tableOfContents

\disclaimer

\section{Introduction}

\subsection{What is Statistical Mechanics}

Statistical Mechanics is the area of Physics interested in systems with a large number of degrees of freedom $n$. Note that these variables can be interacting or not. \\

There are two distinct class of Statistical Mechanics: equilibrium and non-equilibrium. \\

The Statistical part of Statistical Mechanics implies that it is inherently a study of probabilities and probability distributions. These laws must still remain fully consistent with physical laws. \\

Typically, systems are analyzed on a microscopic level. For a system of particles with charges $\bc{q_i}$ and their positions $\bc{\vec{r}_i}$, the dynamics are governed by the forces acting on each particle,

\[ \vec{F}_i = m_i \vec{a}_i = \sum_{i\neq j} \f{q_iq_j}{4\pi\ep_0} \f{1}{\abs{\vec{r}_{ij}}^2} \]

But does labeling the particles really matter? For the case of $N \ar \infty$, the global phenomenology is of interest.

\subsection{History}

\begin{itemize}
        \item [1738] Daniel Bernoulli
        \begin{itemize}
                \item molecules moving in container, they collide with one another
                \item collisions with walls explains pressure
        \end{itemize}
        \item [$~$1850] Gay Lussac, Joule, Thomson (Lord Kelvin), Carnot
        \item [1859] James Clerk Maxwell
        \begin{itemize}
                \item $D(\nu) \sim e^{-\f{\nu^2}{2k_B T}}$
        \end{itemize}
        \item [1884] Josiash Willard Gibbs
        \begin{itemize}
                \item ensemble averaging
        \end{itemize}
        \item [$~$1900] Planck, Einstein, Bose, Pauli, Fermi, Dirac
        \item [Today] Frontier is in non-equilibrium Statistical Mechanics
        \begin{itemize}
                \item cold atoms
                \item biology
                \item quantum information
        \end{itemize}
\end{itemize}

\section{Foundations}

\subsection{Essence of Statistical Mechanics}

\heading{Laws of Thermodynamics}

\begin{tabular}{|c|c|}
        \hline
        Pros & Cons \\
        \hline
        \tabitem great because they are totally general &
        \tabitem does not tell us how to compute anything \\
        \tabitem relationship's (Maxwell's relations) between $c_p, c_v, \al, \kappa$ &
        \tabitem does not tell us what entropy is \\
        \hline
\end{tabular} \\

The \nth{2} law of Thermodynamics reveals $\dif U = \indif Q - \indif W$ where $\indif Q = T \dif S$. \\

But what is $S$ and what does it \textbf{physically} mean? Boltzmann reveals the relation:

\[ S = k_B \ln (\Om) \]

Which we will come back to.

\subsection{Postulate of Statistical Mechanics}
\label{sec:postsm}
There is only one postulate of Statistical Mechanics:
\begin{displayquote}
        \textit{For an isolated system in equilibrium, all microstates accessible to the system are equally probable.}
\end{displayquote}

In order to digest this postulate, we will require some definitions.

\heading{Definitions}

\begin{itemize}
        \item system
        \begin{itemize}
                \item part of the universe we care about
                \item only weakly coupled to the rest of the universe
                \item the dynamics/mechanics are dominated by the internal degrees of freedom and forces
        \end{itemize}
        \item isolated
        \begin{itemize}
                \item idealization
                \item eliminates all external influences; no force, no energy/heat flux and no particle flux
                \item quantities such as the energy, number of particles and volume assumed constant forever $\dif U, \dif N, \dif V = 0$
        \end{itemize}
        \item equilibrium
        \begin{itemize}
                \item everything is no-longer changing
        \end{itemize}
        \item microstate
        \begin{itemize}
                \item a complete/total description of everything at the microscopic level $\bc{\vec{r}_i, \vec{p}_i}$ for each $i$
        \end{itemize}
        \item macrostate
        \begin{itemize}
                \item a description at the macroscopic level in accordance with the external constraints
                \item $U, P, T, \bar{M}$
        \end{itemize}
        \item equally probable
        \begin{itemize}
                \item we are dealing with probabilities and statistics
                \item microstates are somehow describing probabilistically the properties at the macroscopic level
        \end{itemize}
        \item accessible
        \begin{itemize}
                \item consistency with the macroscopic constraints imposed by the conservation laws (fixed energy, fixed number of particles)
        \end{itemize}
\end{itemize}

\heading{Postulate Follow-up}

\begin{displayquote}
        \textit{We assume that the observed/realized macrostate is the one with the most microstates.}
\end{displayquote}

\subsection{Perspective from Coin Tossing}

Consider $4$ coins tossed many, many times. What are the microstates describing this system? \\

\newcolumntype{C}{>{\centering\arraybackslash}p{2em}}
\begin{tabular}{|c|CC|CCCC|c|c|}

\hline
Macrostate Label & \multicolumn{2}{c|}{Macrostate} & \multicolumn{4}{c|}{Microstate} & Thermo Probability & True Probability \\
{} & $N_H$ & $N_T$ & A & B & C & D & {} & {} \\
\hline
1 & 4 & 0 & H & H & H & H & 1 & $1/16$ \\
\hline
2 & 3 & 1 & H & H & H & T & 4 & $4/16$ \\
    &   &   & H & H & T & H &   & $    $ \\
    &   &   & H & T & H & H &   & $    $ \\
    &   &   & T & H & H & H &   & $    $ \\
\hline
3 & 2 & 2 & H & H & T & T & 6 & $6/16$ \\
    &   &   & H & T & T & H &   & $    $ \\
    &   &   & T & T & H & H &   & $    $ \\
    &   &   & T & H & H & T &   & $    $ \\
    &   &   & H & T & H & T &   & $    $ \\
    &   &   & T & H & T & H &   & $    $ \\
\hline
4 & 1 & 3 & T & T & T & H & 4 & $4/16$ \\
    &   &   & T & T & H & T &   & $    $ \\
    &   &   & T & H & T & T &   & $    $ \\
    &   &   & H & T & T & T &   & $    $ \\
\hline
5 & 0 & 4 & T & T & T & T & 1 & $1/16$ \\
\hline
\end{tabular}

\vspace{0.1in}

We note that the most probable macrostate $3$ is the one with the most microstates $6$. \\

How do we deal with very large $N, N_H, N_T$ in order to locate the most likely macrostate? First likes get a general expression for $\Om$ were $\Om$ is the number of microstates. Since $N = N_H + N_T$ and $N$ is considered fixed, there is only one free parameter $N_H$ (taken by choice). Thus $\Om$ can be considered a function of $N_H$ and nothing else. \\

Recall from probability that the form for $\Om$ is given by,

\[ \Om = \f{N!}{N_H!\br{N - N_H}!} \]

The most likely macrostate is given when $\Om$ (the number of microstates) is maximized. This means that we are interested in finding values of $N_H$, namely $N_H^*$ where,

\[ \bre{\der{\Om}{N_H}}_{N_H = N_H^*} = 0 \qquad \bre{\dder{\Om}{N_H}}_{N_H = N_H^*} > 0 \]

In order to do this, we will need to explore some mathematics ideas.

\subsection{Stirlings Formula and Gaussian Integrals}

Consider the integral,

\[ I = \intl_0^\inf x^N e^{-x} \dx \]

This can be evaluated using integration by parts,

\[ I = N \intl_0^\inf x^{N-1} e^{-x} \dx = \cdots = N! \eq \label{eq:intbypartsNtimes}\]

\subsubsection{Differentiation Trick}

However, integration by parts $N$ times on \eqref{eq:intbypartsNtimes} is annoying. There is a nice trick. Notice that,

\[ \intl_0^\inf e^{-ax} \dx = \bre{-\f1a e^{-ax}}_0^\inf = \f1a \eq \label{eq:tricka} \]

One can treat $a$ as a \textit{dummy} variable, and examine \eqref{eq:tricka}'s derivative with respect to $a$,

\[ \pder{}{a} \intl_0^\inf e^{-ax} \dx = \intl_0^\inf \pder{}{a} e^{-ax} \dx = \intl_0^\inf -x e^{-ax} \dx = \pder{}{a}\br{\f1a} = -\f{1}{a^2}\]

The reason for doing this is to simplify the process of \eqref{eq:intbypartsNtimes}. \\

If one explores the $N\tsp{th}$ derivative of \eqref{eq:tricka} with respect to $a$, you will derive the expression,

\[ \bs{\br{-1}^N \pderk{N}{}{a} \intl_0^\inf e^{-ax} \dx}_{a=1} = N! \eq \label{eq:trick} \]

The $\br{-1}^N$ term is a result of the alternating sign induced by bringing down a $-x$ each time you take a derivative. \\

\subsubsection{Stirling's Formula}
\label{sec:stirling}

Looking back at the integral \eqref{eq:intbypartsNtimes},

\[ \intl_0^\inf x^N e^{-x} \dx = N! \eq \label{eq:stirlingstart}\]

How can we approximate $N!$ using the left had side of \eqref{eq:stirlingstart}? To derive Stirling's Formula, we need to make a change of variables $x = N + \sqrt{N} y$. Substituting into \eqref{eq:stirlingstart} gives,

\[ N! = \intl_0^\inf \sqrt{N} e^{-N} e^{N\ln\br{N+\sqrt{N}y}} e^{-\sqrt{N}y}\dy \]

The approximation begins by expanding the logarithm for large $N$,

\[ \ln\br{N + \sqrt{N}y} = \ln\br{N\bs{1+\f{y}{\sqrt{N}}}} = \ln\br{N} + \ln\br{1+\f{y}{\sqrt{N}}} \]

Take $\ep = \f{y}{\sqrt{N}} << 1$ and apply Taylor series,

\[ \ln\br{1+\ep} \approx \ep - \f{\ep^2}{2} \]

Thus,

\[ N! \approx \sqrt{N}e^{-N}N^N\intl_{-\sqrt{N}}^\inf e^{-\f{y^2}{2}}\dy \]

The lower bound can be approximated as $\inf$ since $N$ is so large,

\[ N! \approx \sqrt{N}e^{-N}N^N\intl_{-\inf}^\inf e^{-\f{y^2}{2}}\dy \]

Notice the remaining integral term. It is called the \textit{Gaussian Integral} and has solution (see \nameref{sec:gaussianintegrals}),

\[ \intl_{-\inf}^\inf e^{-\f{y^2}{2}}\dy = \sqrt{\f{\pi}{a}} \eq \label{eq:gaussian} \]

Thus letting $a = 1/2$,

\[ N! \approx \sqrt{2\pi N}e^{-N}N^N \eq \label{eq:stirlinglong} \]

Equation \eqref{eq:stirlinglong} is known as \textit{Stirling's Formula}. However, there is a much more useful form of Stirling's Formula. It is obtained by taking the logarithm of both sides,

\[ \ln\br{N!} \approx \br{N+\f12}\ln\br{N} - \br{N - \untext{\f12 \ln\br{2\pi}}{small compared to large $N$}}  \]
\[ \ln\br{N!} \approx N\ln N - N \eq \label{eq:stirling} \]

Note that the remaining $N$ is not dropped. This is because for $N \sim 10^{23}$, $N\ln N - N$ and $N\ln N$ differ by about $2\%$. \\

Now we can apply this to the problem of maximizing $\Om$ (which is equivalent to maximizing $\ln\Om$) because the logarithm is monotonically increasing.

\[ 0 = \pder{\ln\Om}{N_H} =\pder{}{N_H} \bs{\ln\br{\f{N!}{N_H!\br{N - N_H}!}}} \]

Through some manipulation, and applying \eqref{eq:stirling}, one obtains the expected result,

\[ N_H = \f{N}{2} \]

\subsubsection{Gaussian Integrals} \label{sec:gaussianintegrals}

Before continuing, we should take a moment to explore how \eqref{eq:gaussian} is solved. Let,

\[ I_x = \intl_{-\inf}^\inf e^{-ax^2}\dx \]

Here comes the trick. Multiply $I_x$ by itself and switch from rectangular coordinates to polar coordinates,

\[ I_xI_y = \intl_{-\inf}^\inf e^{-ax^2}\dx\intl_{-\inf}^\inf e^{-ay^2}\dy \]
\[ I^2 = \intl_{-\inf}^\inf\intl_{-\inf}^\inf e^{-a\br{x^2+y^2}}\dx\dy\]

Where we take $\R^2 (x, y) \mapsto \R^2 (r, \phi)$

\[ I^2 = \intl_{0}^{2\pi} \intl_{0}^\inf re^{-ar^2}\dif r\dif\phi\]

Which reveals that $I^2 = \pi/a$. Thus,

\[ I = \sqrt{\f{\pi}{a}} \]

\subsection{Connections between Thermodynamics and Statistical Mechanics}

Consider a lattice of $\text{Cu}^{2+}$ atoms. In a lattice the $\text{Cu}^{2+}$ atoms are distinguishable because they have unique locations. Now apply an external magnetic field.

\[ H\tsb{Zeeman} = - \vec{\mu} \cdot \vec{B} \]

Recall that $\vec{u} = g \mu_B \vec{s}$ has units $J/T$ where $T$ is Tesla. Where for an electron,

\[ \mu_B = \f{e\hbar}{2m} = \SI{9e-24}{\joule \tesla^{-1}} \qquad g \approx 2 \]

For $\vec{B} = B \hat{z}$, $H\tsb{Zeeman} = 2\mu_BBs_z \defined b s_z$. The splitting of the two spin states $s_z = \pm 1$ for $B = \SI{1}{\tesla}$ has characteristic temperature of,

\[ \f{H\tsb{Zeeman}}{k_B} = \f{\vep}{k_B} = \f{\SI{10e-23}{\joule}}{\SI{1.4e23}{\joule\kelvin^{-1}}} \approx \SI{0.6}{\kelvin} \]

Now consider $N$ electrons subject to the field $\vec{B}$ where there are $N_+$ spins ``up'' and $N_-$ spins ``down''. This is completely analogous to the coin flipping example. The total energy of the system is given by,

\[ U = - N_- \vep + N_+ \vep \]

Note that $N = N_+ + N_-$ and thus,

\[ \f{U}{N} = \vep - 2 \vep \f{N_-}{N} \]

Constraining $U$ and using the substitution,

\[ \f{N_-}{N} = \f{1-x}{2} \qquad \f{N_+}{N} = \f{1+x}{2}\]

Then the microstate measure is given by,

\[ \Om = \f{N!}{N_+!N_-!} \]

Becomes (after some manipulation as using \eqref{eq:stirling})

\[ \ln\Om = -N\bs{\br{\f{1+x}{2}}\ln\br{\f{1+x}{2}} + \br{\f{1-x}{2}}\ln\br{\f{1-x}{2}}} \]

Now recall that for fixed volume $\dif V = 0$,

\[ \f{1}{T} = \br{\pder{S}{U}}_V \]

But since $U$ depends only on $x$, we can write,

\[ \f{1}{T} = \br{\pder{S}{x}}\br{\pder{x}{U}} \]

Thus reveals a slight connection between $S$ the entropy and $\Om$ through $x$ in this example. Further analysis with motivate Boltzmann's equation,

\[ S = k_B \ln\Om + S_0 \]

\subsection{Example of a Physical System with Constraint}

Suppose you have $3$ particles called $A,B,C$ such that each particle can have $\vep_j = j \vep$ where $j = 0, 1, 2, 3, \ldots$. \\

How many microstates are there subject to the constraint that the total energy is $3\vep$? \\

\begin{tabular}{|c|CCCC|CCC|c|c|}

\hline
Macrostate Label & \multicolumn{4}{c|}{Macrostate} & \multicolumn{3}{c|}{Microstate} & Thermo Probability & True Probability \\
{} & $N_0$ & $N_1$ & $N_2$ & $N_3$ & A & B & C & {} & {} \\
\hline
1 & 2 & 0 & 0 & 1 & 0 & 0 & $3\vep$ & 3 & $3/10$ \\
    &   &   &   &   & 0 & $3\vep$ & 0 &   &        \\
    &   &   &   &   & $3\vep$ & 0 & 0 &   &        \\
\hline
2 & 0 & 1 & 1 & 0 & 0 & $\vep$ & $2\vep$ & 6 & $6/10$ \\
    &   &   &   &   & $\vep$ & $2\vep$ & 0 &   &        \\
    &   &   &   &   & $2\vep$ & 0 & $\vep$ &   &        \\
    &   &   &   &   & 0 & $2\vep$ & $\vep$ &   &        \\
    &   &   &   &   & $2\vep$ & $\vep$ & 0 &   &        \\
    &   &   &   &   & $\vep$ & 0 & $2\vep$ &   &        \\
\hline
3 & 0 & 3 & 0 & 0 & $\vep$ & $\vep$ & $\vep$ & 1 & $1/10$ \\
\hline
\end{tabular}

\section{Review of Thermodynamics}

\subsection{Definitions}

Recall Boyle's Law $PV = nRT$.

\begin{itemize}
        \item processes
        \begin{itemize}
                \item constant $T$, isothermal process
                \item constant $P$, isobaric process
                \item constant $V$, isochoric process or isovolumetric process
                \item constant $S$, adiabatic process
                \begin{itemize}
                        \item Comes from greek \textit{diabatos} which means \textit{to go through}
                        \item No heat exchange between system and surroundings
                        \item Can also be an approximation for processes that occur really quickly over a short period of time
                \end{itemize}
        \end{itemize}
        \item processes reversible/irreversible
        \begin{itemize}
                \item reversible process happens over a number of discrete steps and that are each reversible
                \item irreversible processes are like poking a hole in a balloon or a gas expanding in a vacuum
        \end{itemize}
        \item thermodynamic variables $T, P, V, U$ where $U$ is the \textit{total energy}
\end{itemize}

\subsection{Zeroth Law of Thermodynamics}

If systems $A$ and $B$ are in equilibrium with one another and systems $B$ and $C$ are in equilibrium then $A$ is in equilibrium with $C$.

\subsection{Functions of State}

Thermodynamic variables are not independent. They are often related by an equation of state. For example $PV = nRT$ for an ideal gas. Other variables might come into an equation of state. For example $\rho, \mathcal{T}, E$ are all important for the equation of state for a liquid crystal sample. \\

Equations of state with typically look like

\[ f\br{P, V, T} = 0 \]

Another important notion is the notion of \textit{function of state}. A quantity that depends only on the thermodynamic variables of the system and \textit{not it's history}, is called a \textit{function of state}. \\

We will first focus on $U\tsb{total energy}$ first and then $S\tsb{entropy}$ as our functions of state. \\

Mathematically, $G = g\br{x,y}$ where $x,y$ are the thermodynamic variables and $G$ is a function of state analytic everywhere and obeys some properties:

\begin{itemize}
        \item $\dif G = \br{\pder{G}{x}}_y \dx + \br{\pder{G}{y}}_x \dy $
        \item at most values of the thermodynamic variables $g(x,y)$ is ``smooth''.
        \item for example: $\br{\pdder{G}{x}}_y$ or $\br{\pdder{G}{y}}_x$ or $\br{\pder{}{x}\br{\pder{G}{y}}_x}_y$ are all continuous
        \item the order of discontinuities is determined by whether or not the system or substance is undergoing transitions of state or not
\end{itemize}

For functions of state that are analytical everywhere, the order of derivatives is inconsequential.

\[ \br{\pder{}{x}\br{\pder{G}{y}}} = \br{\pder{}{y}\br{\pder{G}{x}}}\]

What can we say about $G$ in cases where

\[ \dif G = \pder{G}{x} \dx + \pder{G}{y} \dy \eq \label{eq:dg} \]

in the case of functions of state like $U$, $ \dif U = \indif Q - \indif W  $ and the inexact differentials and how they relate to exact differentials like $\dif V$ and $\dif S$? In particular, when can we integrate $\dif U$?

Answer: equation \eqref{eq:dg} can be integrated in situations where

\[ \br{\pder{}{y}\br{\pder{G}{x}}_y}_x = \br{\pder{}{x}\br{\pder{G}{y}}_x}_y \eq \label{eq:exactcond}\]

When equation \eqref{eq:exactcond} is held for a physical system, one can say that $\dif G$ is an \textbf{exact differential}. Review 12,13 in notes on ``Review of Thermodynamics''. \\

The difference in the function $G(x,y)$ between two sufficiently close pairs of points $(x_1, y_1)$ and $(x_2, y_2)$ depends only on the difference in $G(x,y)$ evaluated at those two points.

\[ \Delta G = G(x_2, y_2) - G(x_1, y_1) \]

$\Delta G$ does not depend on the path from point $(x_1, y_1)$ to $(x_1, y_1)$. In practice, one can assign such a function $G$ to the values of the thermodynamic variables at the points $(x,y)$. For example, $U(P,V)$ is such a function of state. It depends only on the description through the thermodynamic variables and not the history. \\

By counter example, heat $Q$ is not a function of state. No one can say, ``that substance has $X$ units of heat in it''.

\subsection{Work}

There are two types of work. One is called \textit{configuration work} and the other is called \textit{dissipative work}. \\

\subsubsection{Configuration Work}

Configuational work is denoted $\indif W$ where the symbol $\indif$ represents that it \textbf{is not} and exact differential.

\[ \indif W = \sum_i y_i \dif x_i \]

where $y_i$ is an intensive variable (not proportional to $N, V$; examples: pressure, surface tension). It can be thought of as a generalized force. Here $\dif x_i$ is the generalized displacement which is an extensive variable.

\subsubsection{Dissipative Work}

Dissipative work can be thought of as ``stirring work''. Examples include a mixer in a liquid or an electrical wire/resistor.

\begin{itemize}
        \item electrical power:
        \begin{itemize}
                \item $P = V \cdot I$
                \item $\dif W\tsb{dis} = P \cdot \dif t = R I^2 \dif t$
        \end{itemize}
\end{itemize}

\subsubsection{Sign Convention}

\[ \indif W > 0 \note{work done by the system} \]
\[ \indif W < 0 \note{work done on the system} \]

Note:

\begin{itemize}
        \item work is \textbf{not} a property of the system
        \item work is \textbf{not} a function of state
        \item integration on a closed loop is not degenerate $\oint \indif W \neq 0$
\end{itemize}

\subsubsection{Abiabatic Work}

Adiabatic work occurs with no heat exchange.

\begin{center}
\begin{tikzpicture}[scale=1.0]
        % Draw axes
        \draw [<->,thick] (0,4) node (yaxis) [above] {$P$}
                |- (5,0) node (xaxis) [right] {$V$};
        \coordinate (f) at (4,1);
        \coordinate (i) at (1,3);
        \node [blue, label={[blue]10:$\indif Q = 0$}] (mid) at (2, 1.5) {};
        \draw [blue, thick, ->] plot [smooth, tension=1] coordinates { (i) (mid) (f)};
        \node [below] (vi) at (1,0) {$V_i$};
        \node [below] (vf) at (4,0) {$V_f$};
        \node [left] (pf) at (0,1) {$P_f$};
        \node [left] (pi) at (0,3) {$P_i$};
        \draw [gray, dashed] (i) -- (vi);
        \draw [gray, dashed] (i) -- (pi);
        \draw [gray, dashed] (f) -- (vf);
        \draw [gray, dashed] (f) -- (pf);
\end{tikzpicture}
\end{center}

Adiabatic work is done between any two equilibrium states and is \textit{independent of path}. One can define a function of state as the total adiabatic work done on a system. Lets define this a the total internal energy of the system.

\[ \dif U = - \dif W\tsb{adiabatic}\]

Note the minus `$-$' sign is due to the fact that the total energy of the system increases when work is done \textbf{on} this system. Along adiabatic paths, there is an exact differential associated with $\indif W$. It is called internal energy $\dif U$.

\subsection{First of Law of Thermodynamics}

\subsubsection{Heat}

In systems such as boiling a pot of water, there is no work performed on the system however the state of the system changes (not boiling to boiling). The mechanism that causes this change of state is heat ($Q$).

\[ \dif U = \indif W + \indif Q \eq \label{eq:firstlaw} \]

Equation \eqref{eq:firstlaw} is using the following convention:

\begin{align*}
        \indif Q &:< 0 \note{heat flow \textbf{in}} \\
        \indif Q &:> 0 \note{heat flow \textbf{out}} \\
        \indif W &:< 0 \note{work is done \textbf{by} system} \\
        \indif W &:> 0 \note{work is done \textbf{on} system} \\
\end{align*}

\subsubsection{Heat Capacities}

Use the convention that $C$ is the heat capacity (extensive) and $c$ is the specific heat capacity (intensive).

\[ c = \f{C}{n} \]

\[ C \defined \lim_{\Delta T \ar 0} \f{\Delta Q}{\Delta T} \]

Heat capacity $C$ is very important. Measuring $C$ in a metal at low temperatures reveals very important intrinsic properties of the material. Also note that heat capacity is always $C \sim \f1T$ and not some other power than $-1$.

\heading{Relationship Between $c_v, c_p$ for an Ideal Gas}

\[ PV = nRT \]

Using Boyle's law, work with $n = \SI{1}{\mole}$ for convenience. How can we relate $C$ to the properties of the substance? Let us write (noting the `-' convention),

\[ \dif U = \indif Q - \indif W \]

Switch to specific (per mole) quantities,

\[ \dif u = \indif q - \indif w \]

\[ \dif u = \pdert{u}{T}{v} \dif T + \pdert{u}{v}{T} \dif V \]

\[ \indif q = \dif u + \indif w \]
\[ \indif q = \pdert{u}{T}{v} \dif T + \bc{\pdert{u}{v}{T} \dif V + P\dif v} \]

However,

\[ c_v = \lim_{\dif T \ar 0} \f{\indif q}{\dif T} \note{At constant V} \]

Thus,

\[ c_v = \pdert{u}{T}{v} \eq \label{eq:heatcapacity} \]

Which gives us,

\[ \indif q = c_v \dif T  + P\dif v \]

Note that this formula uses the fact that,

\[ \pdert{U}{v}{T} = 0 \note{For ideal gas.} \]

Which we will prove momentarily, but can be taken as an experimental fact discovered by Guy-Lussac. Namely $U$ is only a function of $T$ for an ideal gas ($U = U(T)$).

\[ \indif q = c_v \dif T  + P\dif v + \br{ v \dif p  - v\dif p} \]
\[ \indif q = \br{c_v + R} \dif T  - v\dif p \]

Thus,

\[ \lim_{\dif T \ar 0} \bre{\f{\indif q}{\dif T}}_{p=\text{const.}} = c_v + R \]

Thus we can define a new quantity,

\[ c_p \defined \bre{\f{\indif q}{\dif T}}_{p=\text{const.}} = c_v + R \]

For an monoatomic gas ($U = \f32 RT$),

\[ \ga \defined \f{c_p}{c_v} = \f{5}{3} \]

\subsection{Adiabatic Work Continued}

Let us examine an isotherm (red) next to an adiabatic curve (blue),

\begin{center}
\begin{tikzpicture}[scale=1.0]
        % Draw axes
        \draw [<->,thick] (0,4) node (yaxis) [above] {$P$}
                |- (5,0) node (xaxis) [right] {$V$};
        \draw [blue, thick, ->] plot [smooth, tension=1] coordinates { (0.5,3) (1.5, 1.3) (3.5, 0.5)};
        \draw [red, thick, ->] plot [smooth, tension=1] coordinates { (0.6,3) (1.5, 1.8) (3.5, 1.2)};
        \node [blue] (adiabatic) at (4.2, 0.5) {adiabatic};
        \node [red] (isotherm) at (4.2, 1.2) {isotherm};
\end{tikzpicture}
\end{center}

\begin{align*}
        \indif q &= c_v \dif T + P \dif v \\
        \indif q &= c_P \dif T - v \dif P
\end{align*}

And let's set $\indif q = 0$ (adiabatic),

\[ c_v \dif T + P \dif v = c_P \dif T = v \dif P \]

Which gives,

\[ \f{\dif P}{P} = - \ga \f{\dif v}{v} \note{Using $\ga = c_P/c_v$} \]

Or integrating along a curve,

\[ \intl_1^2 \f{\dif P}{P} = - \intl_1^2 \ga \f{\dif v}{v} \]
\[ \ln\br{\f{P_2}{P_1}} = - \ga \ln\br{\f{v_2}{v_1}} \]

Which gives the relationship,

\[ P_1V_1^\ga = P_2V_2^\ga \]

Which holds true along an adiabatic curve. However, $PV = RT$. Thus using the expression $\dif U = - P \dif V |\tsb{adiabatic}$ gives the result,

\[ W = \bc{\f{P_2V_2 - P_1V_1}{1-\ga}}_{\ga > 1} \]

\subsubsection{Gay-Lussac Experiment}
\label{sec:guylussac}

For an ideal gas,

\[ \pdert{U}{V}{T} = 0 \]

\[ \indif Q = \pdert{U}{T}{V} \dif T + \bc{\pdert{U}{V}{T} + P}\dif V \]

\[ \f{\indif Q}{T} = \f{1}{T}\pdert{U}{T}{V} \dif T + \f{1}{T}\bc{\pdert{U}{V}{T} + P}\dif V \]

Employ an integrating factor for the inexact differentials. It will take us from and inexact differential to an exact differential.

\heading{Math Interlude}

\[ \indif G = A(x,y)\dx + B(x,y)\dy \]

Multiply $G$ by $\mu(x,y)$ and unknown function of $x$ and $y$,

\[ \dif \ti{G} = \mu \cdot \indif G\]

Thus,

\[ \dif \ti{G} = \mu A \dx + \mu B \dy \]

Making,

\[ \pder{\br{\mu A}}{y} = \pder{\br{\mu B}}{x} \eq \label{eq:intfact} \]

\heading{Example}

\[ \dif G = \sin(y) \dx + \cos(y) \dy \]
\[ \dif \ti{G} = \mu\sin(y) \dx + \mu\cos(y) \dy \]

Let us look for a $\mu$ such that $\mu(x,y) = \mu(x)$ and plugging into equation \eqref{eq:intfact},

\[ \pder{\br{\mu \sin(y)}}{y} = \pder{\br{\mu \cos(y)}}{x} \]
\[ \mu(x)\cos(y) = \der{\mu}{x}\cos(y) + \mu \der{\cos{y}}{x} \]
\[ \mu(x)\cancel{\cos(y)} = \der{\mu}{x}\cancel{\cos(y)} + \mu \cancelto{0}{\der{\cos{y}}{x}} \]

Thus,
\[ \mu(x) = \der{u}{x} \]
Which gives,
\[ \mu(x) = e^x \]

The purpose of introducing the notion of an integrating factor is to reveal that although $\indif Q$ is an inexact differential, we can introduce an integrating factor $\f1T$ that makes the quantity $\f{\indif Q}{T}$ an \textbf{exact} differential. This quantity is somehow more important that $Q$ as it describes a property of the system. We will see that this quantity is the motivation for entropy. Let us examine this,

\[ \f{\indif Q}{T} = \underbrace{\f{1}{T}\pdert{U}{T}{V}}_{A} \dif T + \underbrace{\f{1}{T}\bc{\pdert{U}{V}{T} + P}}_{B}\dif V \]

And thus,

\[ \pder{A}{V} = \pder{B}{T} \]

Gives,

\[ \pder{A}{V} = \pder{}{V}\bc{\f1T \pder{U}{T}} = \f1T \f{\partial^2 U}{\partial V \partial T} \]
\[ \pder{B}{T} = \pder{}{T}\bc{\f1T \br{\pder{U}{V} + P}} = -\f{1}{T^2} \br{\pder{U}{V} + P} + \f1T \f{\partial^2 U}{\partial T \partial V} + \f1T \pder{P}{T} \]

Equating these two expressions (noticing that $\f{\partial^2 U}{\partial T \partial V} = \f{\partial^2 U}{\partial V \partial T}$ and that $PV = RT \implies \f{P}{T^2} = \f{R}{TV}$),

\[ \f1T \f{\partial^2 U}{\partial V \partial T} = -\f{1}{T^2} \br{\pder{U}{V} + P} + \f1T \f{\partial^2 U}{\partial T \partial V} + \f1T \pder{P}{T} \]
\[ \cancel{\f1T \f{\partial^2 U}{\partial V \partial T}} = -\f{1}{T^2} \br{\pder{U}{V} + P} + \cancel{\f1T \f{\partial^2 U}{\partial T \partial V}} + \f1T \pder{P}{T} \]
\[ 0 = -\f{1}{T^2} \br{\pder{U}{V} + P} + \f1T \pder{P}{T} \]
\[ \f{1}{T^2} \br{\pder{U}{V} + P} = \f1T \pder{P}{T} \]
\[ \f{1}{T^2} \pder{U}{V} + \f{P}{T^2} = \f1T \pder{P}{T} \]
\[ \f{1}{T^2} \pder{U}{V} + \cancel{\f{R}{TV}} = \cancel{\f{R}{TV}} \]

Therefore Gay-Lussac was destined to find this relationship through experiment:
\[ \pder{U}{V} = 0 \]

\subsubsection{Joule-Thomson(Kelvin) Experiment}

Using $\f{1}{T}$ as the integrating factor, one can go through a similar derivation as in section \ref{sec:guylussac} to understand that the quantity $S$ is a state function of a system with,

\[ \dif S \defined \f{\indif Q}{T}  \]

\subsubsection{Summary of First Law}

In summary, the first law of thermodynamics is really just conservation of energy with expression,

\[ \dif U = \indif Q - \indif W \]

\subsection{Second Law of Thermodynamics}

Physical laws on the microscopic scale always are found to have an inherent time-reversal symmetry. The physics behaves the same running forward in time and backward in time. However, a semi-paradox of real-world phenomenology seems to suggest that there are some processes can not occur moving backward in time. They still satisfy the conservation of energy, but there is another law that governs these macroscopic phenomena.

\subsubsection{Heat Engines}

Imagine a an engine $E$ that runs from temperature $T_2$ to $T_1$ (two heat baths), where it extracts heat $Q_2$ and disposes heat $Q_1$ and does some work $W$.

\begin{center}
\begin{tikzpicture}[scale=2]
        \pic at (0,0) {nodea={red}{$T_H$}{L}};
        \pic at (2,0) {nodec={black}{}{C}};
        \pic at (4,0) {nodeb={blue}{$T_C$}{R}};
        \draw[->,>=latex](L)--node[midway,above]{$Q_2$}(C);
        \draw[->,>=latex](C)--node[midway,above]{$Q_1$} (R);
        \draw[->,>=latex](C)--node[midway,left]{$W$} ++(0,-2cm);
\end{tikzpicture}
\end{center}

The efficiency of the engine is defined as,

\[ \eta \defined \f{\text{output}}{\text{input}} = \f{W}{Q_2} \]

Note that on a cyclic process $\De U = 0$ since $U$ is a function of state. However it is also equal to,

\[ \De U = \br{Q_1 + Q_2} - W \]

Therefore typically the efficiency is given by,

\[ \eta = 1 + \f{Q_1}{Q_2} \]

\begin{itemize}
    \item $Q_2$ : injected into engine $E$ and is $>0$
    \item $Q_1$ : extracted from engine $E$ and is $<0$
\end{itemize}

This can be re-written as,

\[ \eta = 1 + \f{Q_1}{-\abs{Q_2}} = 1 - \f{\abs{Q_1}}{\abs{Q_2}} \]

\subsubsection{Carnot Cycle/Engine}

It can be shown that no engine is more efficient than a Carnot engine. Upon considering this, one can use entropy to expose the Second Law of Thermodynamics and the natural ``arrow of time''.

\begin{center}
\begin{tikzpicture}[
    scale=1.0,
    decoration={post length=0.1mm, pre length=0.1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
        % Draw axes
        \coordinate (nya) at (0, 0);
        \coordinate (pya) at (0, 4);
        \coordinate (nxa) at (0, 0);
        \coordinate (pxa) at (5, 0);
        \node [above] (lpya) at (pya) {$P$};
        \node [right] (lpxa) at (pxa) {$V$};
        \coordinate (a) at (1.1, 3);
        \coordinate (b) at (3.2, 2.2);
        \coordinate (c) at (4, 1);
        \coordinate (d) at (2, 1.5);
        \draw [->, thick] (nya) -- (pya);
        \draw [->, thick] (nxa) -- (pxa);
        \draw [postaction={decorate}, thick] (a) -- (b);
        \draw [postaction={decorate}, thick] (b) -- (c);
        \draw [postaction={decorate}, thick] (c) -- (d);
        \draw [postaction={decorate}, thick] (d) -- (a);
        \node [above left] (la) at (a) {$a$};
        \node [above right] (lb) at (b) {$b$};
        \node [below] (lc) at (c) {$c$};
        \node [below left] (ld) at (d) {$d$};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tabular}{c|c|c|c}
$ab$ & isotherm ($T_2$) & $Q_2 > 0$ & $W^{ab} > 0$\\
\hline
$bc$ & adiabat & $Q = 0$ & $W^{bc} > 0$\\
\hline
$cd$ & isotherm ($T_1$) & $Q_1 < 0$ & $W^{cd} < 0$\\
\hline
$da$ & adiabat & $Q = 0$ & $W^{da} < 0$\\
\end{tabular}
\end{center}

The total work is given by,

\[ W\tsb{tot} = W^{ab} + W^{bc} + W^{cd} + W^{da} \]

1) First the isotherms:

\[ W \sim \int P\dif V \note{Use $PV = RT$ (1 mole)}\]
\[ W = \intl_1^2 \f{RT}{V} \dif V = RT \ln\br{\f{V_2}{V_1}} \]
\begin{center}
\begin{tabular}{c|c|c}
$ab$ & $RT_2 \ln\br{\f{V_b}{V_a}} > 0$ & $\implies Q_2$\\
\hline
$cd$ & $RT_1 \ln\br{\f{V_d}{V_c}} < 0$ & $\implies Q_1$\\
\end{tabular}
\end{center}
This is to be expected because we are on isotherms and $\Delta U = 0$. \\

2) Second the adiabats:

Along an adiabat,

\[ PV^{\ga} = \text{const.}\]
\[ P_1V_1^{\ga} = P_2V_2^{\ga} \]

Furthermore, relating this to $PV = RT$,

\[ \f{RT_1}{V_1} V_1^{\ga} = \f{RT_2}{V_2} V_2^{\ga} \]
\[ T_1 V_1^{\ga-1} = T_2 V_2^{\ga-1} \]

Applying this to our diagram,

\[ T_2 V_b^{\ga-1} = T_1 V_c^{\ga-1} \quad \text{and} \quad T_1 V_d^{\ga-1} = T_2 V_a^{\ga-1} \]

Which after rearrangement yields a constraint for the volumes,

\[ \f{V_b}{V_a} = \f{V_c}{V_d} \]

Therefore,

\[ \eta\tsb{Carnot} = \eta_C = 1 + \f{Q_1}{Q_2} \]
\[ \eta_C = 1 + \f{RT_1\ln\br{V_d/V_c}}{RT_2\ln\br{V_b/V_a}} = 1 - \f{T_1}{T_2}\f{\ln\br{V_c/V_d}}{\ln\br{V_b/V_c}} \]
\[ \eta_C = 1 - \f{T_1}{T_2} \]

\subsubsection{Second Law Statements}

\begin{enumerate}
    \item \textbf{Clausius:} You can not build a machine or engine operating in a cycle can be constructed whose sole effect is to transfer heat from a cold to hot.
    \item \textbf{Kelvin-Planck:} It is impossible to construct a device whose sole effect is to produce work by only absorbing heat from a single reservoir. Two reservoirs are needed.
\end{enumerate}

Now let us use Clausius statement to prove Carnot's theorem as per the Carnot Engine; the most efficient engine is a Carnot engine.

\heading{Example}

Suppose we have an engine $E$ that extracts heat from a hot $T_2$ reservoir $Q_2 = \SI{100}{cal}$, performs $W = \SI{20}{cal}$ of work and then dumps the remaining $Q_1 = \SI{80}{cal}$ calories of heat into a cold reservoir $T_1$. This is our Carnot engine.

\[ \eta_C = \f{W}{Q_2} = \f{20}{100} = 20\% \]

Now suppose we had a better engine with $\eta_y > \eta_C$. Namely, this new engine has specs $Q_2 = \SI{100}{cal}, W = \SI{40}{cal}, Q_1 = \SI{60}{cal}$.

\[ \eta_y = \f{W}{Q_2} = \f{40}{100} = 40\% \]

Now to prove is can not be the case, let us take our Carnot engine and reverse it's processes (making it a fridge), and doubling it's size. Therefore this Carnot fridge will require $\SI{40}{cal}$ to run. This can be supplied by our ``better'' engine. Therefore, this combined engine has the net effect of absorbing $\SI{100}{cal}$ from a cold reservoir at $T_1$ and injecting $\SI{100}{cal}$ into a hot reservoir at temperature $T_2$, \textbf{with no other effect.} This violates the Clausius hypothesis.

\heading{Corollary}

All engines operating reversibly between two temperatures $T_1$ and $T_2$ have the same efficiency. In other words, the presence of any irreversible process degrades the efficiency.

\[ \eta\tsb{irreversible} < \eta\tsb{Carnot} \eq \label{eq:irreversiblecarnot}\]

\subsubsection{Entropy}

We have found that,

\[ \eta_C = 1 + \f{Q_1}{Q_2} = 1 - \f{T_1}{T_2} \]

Or equivalently,

\[ \f{Q_1}{T_1} + \f{Q_2}{T_2} = 0 \]

For a single Carnot engine, the sum of heat at step $n$ over the temperature at step $n$ has sum,

\[  \sum_{n=1}^{k} \f{Q_n}{T_n} = 0 \eq \label{eq:revcarnot}\]

Which gives that $\De Q / T$ over the whole cycle is zero. Taking the limit as $k \ar \inf$, it suggests that the quantity $\indif Q / T$ is an exact differential. Introduce the label of entropy,

\[ \dif S = \f{\indif Q}{T} \quad \text{with} \quad \oint \dif S\tsb{rev} = 0  \]

As you can see, considering a Carnot engine also suggests the presence of a quantity $S$ call entropy. Just as before, $S$ is a state function. \\

How can we compute entropy? \\

\heading{Example (Constant Volume)}

Let's consider heat injected at constant volume, and attempt to calculate $\De S$.

\[ \De U = \De Q - \De W \note{At constant volume, $\De W = 0$} \]

But we also have $\De Q = C_v \De T$ where $C_v$ is the heat capacity of the material. This combination yields,

\[ \dif S = \f{\indif Q}{T} = \f{C_v}{T}\dif T \]

\[ \intl_1^2 \dif S = \intl_{T_1}^{T_2} \f{C_v}{T} \dif T = c_v \ln\br{\f{T_2}{T_1}} \]

\[ \br{S_2 - S_1} = C_v \ln\br{\f{T_2}{T_1}} \]

\heading{Example (Constant Temperature)}

Let's consider change of entropy for an isothermal process.

\[ S_2 - S_1 = \intl_1^2 \f{\indif Q}{T} \]

But from the first law we have for constant temperature,

\[ \dif U = \indif Q - P \dif V = 0  \]

Thus,

\[ \indif Q = P \dif V \]
\[ \intl_1^2 \dif S = \intl_1^2 \f{P}{T}\dif V = \intl_1^2 \f{nR}{V}\dif V = nR \ln\br{\f{V_2}{V_1}} \eq \label{eq:isothermvolumechangeentropy} \]

\heading{Summarize}

\begin{enumerate}
        \item No engine is more efficient than Carnot.
        \item As a by-produce, we discovered a quantity $\Delta S$ defined as:
        \begin{itemize}
                \item $\Delta S = \f{\De Q}{T}$ ($\dif S = \f{\indif Q}{T}$ for the infinitesimal)
                \item $\dif S$: defined at each point in $(P,V,T)$ diagram
                \item $\dif S$: is exact differential. $S_2 - S_1$ only depends on the \textbf{endpoints} not the \textbf{path}
        \end{itemize}
\end{enumerate}

Let us show this by comparing two paths,

\begin{center}
\begin{tikzpicture}[scale=1.0]
        % Draw axes
        \draw [<->,thick] (0,4) node (yaxis) [above] {$P$}
                |- (5,0) node (xaxis) [right] {$V$};
        \coordinate (f) at (4,1);
        \coordinate (m) at (1,1);
        \coordinate (i) at (1,3);
        \node [right] (lf) at (f) {$T_2$};
        \node [above] (li) at (i) {$T_1$};
        \node [below] (lm) at (m) {$T'$};
        \node [blue, label={[blue]10:$3$}] (label) at (2.5, 2) {};
        \draw [blue, thick, ->] plot [smooth, tension=1] coordinates { (i) (f)};
        \draw [green, thick, ->] plot [smooth, tension=1] coordinates { (i) (m)};
        \node [green, label={[green]10:$1$}] (label) at (0.5, 1.5) {};
        \draw [green, thick, ->] plot [smooth, tension=1] coordinates { (m) (f)};
        \node [green, label={[green]10:$2$}] (label) at (2, 1) {};
        \node [below] (vi) at (1,0) {$V_1$};
        \node [below] (vf) at (4,0) {$V_2$};
        \node [left] (pf) at (0,1) {$P_2$};
        \node [left] (pi) at (0,3) {$P_1$};
        \draw [gray, dashed] (i) -- (vi);
        \draw [gray, dashed] (i) -- (pi);
        \draw [gray, dashed] (f) -- (vf);
        \draw [gray, dashed] (f) -- (pf);
\end{tikzpicture}
\end{center}

Along the green path,

\[ \Delta S_1 = \intl \br{\f{\indif Q}{T}}_1 = C_v \intl_{T_1}^{T'} \f{\dif T}{T} = C_v \ln\br{\f{T'}{T_1}} \]
\[ \Delta S_2 = \intl \br{\f{\indif Q}{T}}_2 = C_p \intl_{T'}^{T_2} \f{\dif T}{T} = C_p \ln\br{\f{T_2}{T'}} \]

Thus the total change is given by,

\begin{align*}
\De S &= \De S_1 + \De S_2 \\
&= C_v \ln\br{\f{T'}{T_1}} + C_p \ln\br{\f{T_2}{T'}} \\
&= C_v \ln\br{\f{T'}{T_1}} + \br{C_v + R} \ln\br{\f{T_2}{T'}} \\
&= C_v \ln\br{\f{T_2}{T_1}} + R \ln\br{\f{T_2}{T'}} \\
&= \cancelto{0}{C_v \ln\br{\f{T_2}{T_1}}} + R \ln\br{\f{T_2}{T'}} \\
&= R \ln\br{\f{T_2}{T'}} \\
\end{align*}

Note the cancellation $T_1 = T_2$ since (3) is an isotherm. \\

Since path (2) is at constant $P$

\[ \De S = R \ln\br{\f{V_2}{V_1}} \]

Which is identical to the change in entropy along the isotherm derived above. Thus confirming that $\De S$ is independent of path.

\subsubsection{Clausius Inequality}

\begin{center}
\begin{tikzpicture}[
    scale=1.0,
    decoration={post length=1mm, pre length=1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
        % Draw axes
        \coordinate (nya) at (0, 0);
        \coordinate (pya) at (0, 4);
        \coordinate (nxa) at (0, 0);
        \coordinate (pxa) at (5, 0);
        \node [above] (lpya) at (pya) {$P$};
        \node [right] (lpxa) at (pxa) {$V$};
        \coordinate (a) at (1.2, 3);
        \coordinate (b) at (3.2, 2.2);
        \coordinate (c) at (4, 1);
        \coordinate (d) at (2, 1.5);
        \coordinate (ab) at (2.2, 2.4);
        \coordinate (bc) at (3.5, 1.5);
        \coordinate (cd) at (3, 1.15);
        \coordinate (da) at (1.5, 2.3);
        \draw [->, thick] (nya) -- (pya);
        \draw [->, thick] (nxa) -- (pxa);
        \draw [red, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (a) (ab) (b)};
        \draw [blue, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (c) (cd) (d)};
        \draw [thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (b) (bc) (c)};
        \draw [thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (d) (da) (a)};

        \node [above left] (la) at (a) {$a$};
        \node [above right] (lb) at (b) {$b$};
        \node [above right, red] (lab) at (ab) {$T_2$};
        \node [below left, blue] (lcd) at (cd) {$T_1$};
        \node [below] (lc) at (c) {$c$};
        \node [below left] (ld) at (d) {$d$};
\end{tikzpicture}
\end{center}

Recall from equation \eqref{eq:revcarnot}, we have that for a reversible Carnot cycle,

\[ \f{Q_2}{T_2} + \f{Q_1}{T_1} = 0  \]

Now consider the case of heat extraction and heat disposal along a \textit{tiny Carnot cycle}.

\[ \f{\indif Q_2}{T_2} + \f{\indif Q_1}{T_1} = 0  \]

% \TODO{Insert PV circle liney diagram from notes he said he'd upload}
\begin{center}
    \includegraphics[height=2in]{figures/reversible_composition.jpg}
\end{center}

The idea here is to consider any physical process or cycle as being made up of numerous tiny adiabatic curves and isotherms. We \textit{approximate} the real cycle as being composed of a series of reversible processes. In the limit of the number of ``switches'' goes to infinity, the real engine process is made exactly reversible. \\

Essentially, we are mapping out the contour of any given cycle by following many isotherms and adiabats. Then for any given sub-process here,

\[ \sum_{i=1}^{n} \f{\indif Q_i}{T_i} = 0 \]

Or in the limit as $n \ar \inf$,

\[ \ointl \f{\indif Q\tsb{rev}}{T} = 0 \]

Again we observe that $Q\tsb{rev}/T$ is an \textbf{exact} differential and we can call it $\dif S$ as done before. $S$ is a state variable. \\

Any reversible cycle must obey the following equation,

\[ \ointl \f{\indif Q}{T} = 0 \note{For a reversible cycle.} \]

What about for an irreversible cycle? An irreversible cycle is some process that somewhat has a step that can not be performed backward. What we have shown is that the efficiency of an engine with an irreversible cycle $\eta'$ must be less than that of Carnot $\eta_C$ (see \eqref{eq:irreversiblecarnot}).

\[ \eta' < \eta_C\]

Using equation \eqref{eq:irreversiblecarnot}, we can show that,

\[ \f{Q'_1}{Q'_2} < \br{\f{Q_1}{Q_2}}\tsb{rev Carnot} \]

Or that,

\[ \f{\indif Q'_1}{T_1} + \f{\indif Q'_2}{T_2} < 0 \note{For an irreversible cycle.} \]

Which lead to the generalization for an infinite number of steps as,

\[ \sum_{i=1}^{n} \f{\indif Q'_i}{T_i} < 0 \ar \ointl \f{\indif Q'}{T} < 0  \]

This leads to the Clausius Inequality:

\[ \ointl \f{\indif Q'}{T} \leq 0 \]

Which achieves equality only when the \textbf{entire} cycle is irreversible. Inequality is always maintained if \text{any} part of the cycle is irreversible. This can be understood as another formulation of the Second Law of Thermodynamics. \\

There are two cases ``applications'' of Clausius inequality: \\

1) Consider a single two-step \textit{reversible} cycle

\begin{center}
\begin{tikzpicture}[
    decoration={post length=0.1mm, pre length=0.1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
\coordinate (1) at (0,0);
\coordinate (2) at (2,2);
\coordinate (A) at (0.8, 1.2);
\coordinate (B) at (1.2, 0.8);
\draw [blue, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (1) (A) (2)};
\draw [blue, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (2) (B) (1)};

\fill (1) circle (1pt) node[below left] {$1$};
\fill (2) circle (1pt) node[above right] {$2$};
\node [above left, blue] (lA) at (A) {$A$};
\node [below right, blue] (lB) at (B) {$B$};

\end{tikzpicture}
\end{center}

Therefore,

\[ \oint \f{\indif Q}{T} = 0 \]
\[ \intl_1^2 \br{\f{\indif Q}{T}}_A + \intl_2^1 \br{\f{\indif Q}{T}}_B = 0 \]

Rearrangement gives,

\[ \intl_1^2 \br{\f{\indif Q}{T}}_A = - \intl_2^1 \br{\f{\indif Q}{T}}_B = \intl_1^2 \br{\f{\indif Q}{T}}_B\]

Which implies that entropy is conserved,

\[ \br{S_2 - S_1}_A = \br{S_2 - S_1}_B \]

2) Consider a two-step \textit{irreversible/reversible} process

\begin{center}
\begin{tikzpicture}[
decoration={post length=0.1mm, pre length=0.1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
\coordinate (1) at (0,0);
\coordinate (2) at (2,2);
\coordinate (A) at (0.8,1.2);
\coordinate (B) at (1.2,0.8);
\draw [red, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (1) (A) (2)};
\draw [blue, thick, postaction={decorate}] plot [smooth, tension=1] coordinates { (2) (B) (1)};

\fill (1) circle (1pt) node[below left] {$1$};
\fill (2) circle (1pt) node[above right] {$2$};
\node [above left, red] (lA) at (A) {Irrev.};
\node [below right, blue] (lB) at (B) {Rev.};

\end{tikzpicture}
\end{center}

Therefore by the Clausius Inequality we have,
\[ \oint \f{\indif Q}{T} \leq 0 \]

\[ \intl_1^2 \br{\f{\indif Q}{T}}\tsb{irrev} + \intl_2^1 \br{\f{\indif Q}{T}}\tsb{rev} \leq 0 \]
\[ \intl_1^2 \br{\f{\indif Q}{T}}\tsb{irrev} \leq - \intl_2^1 \br{\f{\indif Q}{T}}\tsb{rev} = \intl_1^2 \br{\f{\indif Q}{T}}\tsb{rev} \]

Thus we can conclude that with $\dif S \br{\f{\indif Q}{T}}\tsb{rev}$,

\[ \intl_1^2 \br{\f{\indif Q}{T}}\tsb{irrev} \leq \intl_1^2 \br{\dif S}\tsb{rev} = S_2 - S_1 \]

Which gives that our entropy increases each cycle,

\[ \br{S_2 - S_1}\tsb{rev} \geq \intl_1^2 \br{\f{\indif Q}{T}}\tsb{irrev} \]

Now suppose that our system is undergoing an adiabatic process along the whole path. This means that no heat is exchanges during the process ($\indif Q = 0$). This gives us,

\[ \br{S_2 - S_1}\tsb{irrev} \geq 0 \note{For an irreversible, adiabatic process.}\]

In conclusion, the entropy can only increase for an \textit{isolated} (our adiabatic example) system undergoing an irreversible process. This is a restatement of the Second Law of Thermodynamics; entropy always increases for an isolated system. Of course this is all subject to the constraint imposed on the system: $N, V, E$ might be constrained or imposed to be constant. Or more clearly: \\

At \textit{equilibrium}, entropy must reach its maximum value. Otherwise it would increase because it has to.

\subsubsection{Perspective on Entropy}

Consider a container with two sub-sections. The left hand side will have a gas and the right hand side will have a vacuum (each side having volume $V$). At time $t=0$, the barrier between the two containers is removed and the gas has access to the entire container of volume $2V$. At equilibrium, the gas occupies the entire container. Evidently, this is an irreversible process (the gas does not have the luxury to return to it's original state). Some comments:

\begin{itemize}
    \item irreversible process
    \item gas does no work against the vacuum
    \item no heat exchange
    \item $\De U = \De Q - \De W = 0$ (Conservation of energy $\implies$ temperature is constant)
\end{itemize}

This process is irreversible, so there is no way to directly compute it's change in entropy throughout the entire process. However, the entropy can be calculated using properties about the two endpoints and a \textit{reversible} process that connects these endpoints. \\

The \textit{reversible} process we will consider as a replacement will be letting the gas expand via a piston held initially at the midway point of the container that is controlled externally. We will slowly move the piston until the gas fills the entire container. This new process is consider isothermal that takes use from state (1) (half gas, half vacuum) to state (2) (total gas). Along this process we can compute the change in entropy $S_2 - S_1$ along the reversible path. Turns out we already computed this in equation \eqref{eq:isothermvolumechangeentropy}.

\[ \De S = S_2 - S_1 = nR \ln\br{\f{V_2}{V_1}} \]

Let $nR = k_BN_A$ where $k_B$ is the Boltzmann constant and $N_A$ is Avogadro's number. Note that $V_1 = V$ and $V_2 = 2V$,

\[ \De S = k_BN_A \ln\br{\f{2V}{V}} = k_B \ln\br{2^{N_A}} \]

Thus we expect,

\[ \De S \propto \untext{\br{\text{const.}}}{units $J/K$} \times \ln \br{2^N} \]

Note that the `$2^N$' term measures the number of microstates that the system received or now has access to to due to the process. Consider the microstates counted by the possibility that the particle can be on the left or the right of the container.\\

\begin{itemize}
        \item All particles on left except 1
        \begin{itemize}
                \item LLLLL$\ldots$LLRLLL$\ldots$LLLLL
                \item $N$ ways for this to occur
        \end{itemize}
        \item All particles on the left except 2
        \begin{itemize}
                \item LLLLL$\ldots$LLRLLL$\ldots$LLRL$\ldots$LLLLL
                \item $N(N-1)/2$ ways for this to occur
        \end{itemize}
\end{itemize}

The most probable macrostate occurs when the total number of microstates reaches a maximum. Using the Stirling's approximation, this occurs when there is a 50/50 split between the left and right with total number of microstates,

\[ \Om \sim 2^N + \text{small corrections} \]

Note that disordered (``mixed'') states (50/50) can be realized in so many more ways than ``ordered states''. \\

With this view, entropy can be viewed as a measure of the number of microstates through the rational of the second law. These ideas are what lead Boltzmann to consider the possibility,

\[ S \sim \ln \Om \]

With further corrects and units,

\[ S = k_B \ln \Om +S_0 \]

Where the constant $S_0$ can be shown to be zero with the Third Law of Thermodynamics.

\subsubsection{Conclusion}

The Second Law of Thermodynamics is very much a manifestation of the probability/statistics of the large number of states microscopic objects can realize. \\

The law $\De S \geq 0$ spontaneously for an isolated system means that we must seek microscopic configurations that \textit{maximize} S, as when this is achieved, the system must have reached equilibrium.

\section{S.M. Basis for T.D.}

Statical Mechanics Basis for Thermodynamics.

\subsection{Contact Between S.M. \& T.D.}

\begin{enumerate}
        \item Systems where $V \ar \inf$ and $N \ar \inf$ and the density $N/V$ remains constant
        \begin{itemize}
                \item Thermodynamically large system
        \end{itemize}
        \item Equilibrium Thermodynamics / S.M.
        \begin{itemize}
                \item Consider two regions $(1,2)$ of a container separated by a diathermal wall
                \item $(N_1, V_1, E_1)$ and $(N_2, V_2, E_2)$ number of particles, volume, energy respectively
                \item Interested in total number of microstates
                \item Composite system $\Om^{(0)} = \Om^{(1)} \times \Om^{(2)}$
        \end{itemize}
\end{enumerate}
This composite system has $E^{\br{0}} = E_1 + E_2 = $ constant. We are interested in $\Om$: the number of microstates. At \textit{any time}, system (1) is just as likely to be in any of its microstates. This is also true for system (2).
\[ \Om^{(0)}\br{E_1, E_2} = \Om_1(E_1) \times \Om_2(E_2) \]
\[ \Om^{(0)}\br{E_1, E^{(0)} - E_1} = \Om_1(E_1) \times \Om_2(E^{(0)} - E_1) \]
The most likely macrostate is the one with the most number of microstates. It is important to note that is true at \textit{any time}. To maximize $\Om^{(0)}$,
\[ \pder{\Om^{(0)}}{E_1} = 0 \implies \bre{\pder{\Om_1}{E_1}}_{E_1 = \bar{E}_1} \Om_2(\bar{E}_2) + \Om_1(\bar{E}_1) \bre{\pder{\Om_2}{E_2}}_{E_2 = \bar{E}_2} \pder{E_2}{E_1} = 0 \]
Note since $E^{(0)}$ is fixed,
\[ \pder{E_2}{E_1} = -1 \]
Which reveals,
\[ \f{1}{\Om_1(E_1)} \bre{\pder{\Om_1}{E_1}}_{E_1 = \bar{E}_1} = \f{1}{\Om_2(E_2)} \bre{\pder{\Om_2}{E_2}}_{E_2 = \bar{E}_2} \]
Notice the logarithmic nature of this equation to show that,
\[ \underbrace{\bre{\pder{\ln\Om_1}{E_1}}_{E_1 = \bar{E}_1}}_{\be_1} = \underbrace{\bre{\pder{\ln\Om_2}{E_2}}_{E_2 = \bar{E}_2}}_{\be_2} \]
Therefore in summary, equilibrium requires $\be_1 = \be_2$. Recall that for a reversible process,
\[ \dif U = \indif Q - \indif W \]
Thus,
\[ \dif E = \indif Q - \indif W \]
\[ \dif E = T \dif S - P \dif V \]
Where the definition of entropy can be expressed,
\[ \pdert{S}{N,V}{E} = T \]
Which gives for system 1,
\[ \br{\pder{E_1}{\ln\Om_1}} \br{\pder{\ln\Om_1}{S_1}} = T_1 \]
And the analogous expression for system 2,
\[ \br{\pder{E_2}{\ln\Om_2}} \br{\pder{\ln\Om_2}{S_2}} = T_2 \]
Combining these two systems gives,
\[ \pder{S_1}{\ln\Om_1} = \f{1}{\be_1T_1} \qquad \pder{S_2}{\ln\Om_2} = \f{1}{\be_2T_2} \eq \label{eq:entropymotiv}\]
Equations \eqref{eq:entropymotiv} should be telling for the following assumption. \\

\textbf{Assumption:} There exists an intimate relationship between Statistical Mechanics and Thermodynamics for all and any system. \\
We will propose that,
\[ \f{1}{\be_1T_1} = \f{1}{\be_2T_2} = \f{1}{\be T} = \text{ constant/universal } \]
Thus,
\[ S = \text{constant}\cdot \ln \Om \eq \label{eq:blankboltzmann} \]
Where the unknown constant is actually Boltzmann's constant, $k_B$ with,
\[  \be = \f{1}{k_B T} \]
What about the relationships between other quantities associated with systems (1) and (2)? First consider volume.

\[ \underbrace{\bre{\pder{\ln \Om_1}{V_1}}_{N_1, E_1, V_1 = \bar{V}_1}}_{\eta_1} = \underbrace{\bre{\pder{\ln \Om_2}{V_2}}_{N_2, E_2, V_2 = \bar{V}_2}}_{\eta_2} \]
Which gives,
\[ \eta_1 = \eta_2 \]

These new variables describe the equilibrium of the two systems in terms of volume. Is it required to introduce a new constant similar to above to relate volume to entropy? Consider,
\[ \dif E = T \dif S - P \dif V \eq \label{eq:nonchempot}\]
Thus,
\[ \pder{S}{E} = \f{1}{T} \qquad \pdert{S}{V}{E} = \f{P}{T} \]

However, we could introduce another term to \eqref{eq:nonchempot} to include flux of particles; a chemical potential term. We could add more terms,

\[ \dif E = T \dif S - P \dif V  + \mu \dif N - H \dif M\]
Which would produce more relations like,
\[ \pder{S}{N} = - \f{\mu}{T}\]

From this we get,

\[ \pder{\ln\Om}{V} = \eta \]

Which by chain rule,

\[ \pder{\ln\Om}{S}\pder{S}{V} = \eta \]

But we know from \eqref{eq:entropymotiv} that $\pder{\ln\Om}{S} = \be T$. Therefore,

\[ \pder{S}{V} = \f{1}{\be T} \eta = \f{P}{T} \]

Therefore rearrangement gives,

\[ \eta = \f{P}{k_B T} \]

In summary, once we have the result \eqref{eq:blankboltzmann}, with the universal constant $k_B$, all expected equilibrium between generalized forces must be satisfied. No new constants or relations need to be constructed to explore the relationship between entropy and microscopic quantities. \\

An equilibrium of energy and volume between systems (1) and (2) it must be that,
\[ T_1 = T_2 \qquad P_1 = P_2 \]

Furthermore, and equilibrium of energy and number of particles it must be that,
\[ T_1 = T_2 \qquad \mu_1 = \mu_2 \]

\textbf{Remark:} Equilibrium properties in the extensive ($E, V, N$) variables of the system translate in an equally in the intensive $(T, P, \mu)$ variables. \\

In summary, the equation $S = k_B \ln \Om$ recovers all expected descriptions of thermodynamical equilibrium.

\subsection{Classical Ideal Gas from S.M.}

For this analysis, we will begin with the consideration at $\hbar = 0$. In other words, we will consider the particles to point like and ignore their wave like nature and treat interactions between particles ($V(r_{ij})$) is very small. Note however, in physics, one cannot claim something is ``small'' unless the quantity is dimensionless. Everything is small compared to something else. Thus when we say $\hbar$ is small, we mean that the DeBrogile wavelength is small compared to the thermal length scale. Similarly for the potential energies between the particles, $V/k_BT \ll 1$. \\

\subsubsection{Volume}
Consider the volume the gas occupies as a fixed volume $V$. Also notice that it is acceptable at this point to say that the number of microstates is proportional to the volume.

\[\Om \propto V^N \]

Where $N$ is the number of particles. But how can be compare microstates which are dimensionless to the volume which could have dimensions $m^3$? In practice we assume,

\[ \Om = \text{ constant } \times \br{\f{V}{\Lambda^3}}^N \]

Where $\Lambda$ is a length scale of the system. We will return to this idea later when considering quantum gases. Note from above, we have,

\[ \pdert{S}{V}{E} = \f{P}{T} \]

Combined with $S = k_B\ln\Om$ and $\Om = c V^N$,

\[ PV = N k_B T \]

This is quite remarkable. We just derived an expression for an ideal gas using $S = k_B\ln\Om$ and some simple motivations for the form of $\Om$. Typically, this ideal gas law is written with,

\[ N k_B = N_A R \]

Where $N_A$ is Avogadro's number $\SI{6.02e23}{}$ and $R$ was known that $R = \SI{8.31}{\J\per\mole\per\K}$ which gives a value for,

\[ k_B = \SI{1.38e-23}{\J\per\K} \]

\subsubsection{Energy \& Particle in a Box}

As a reminder, an ideal gas has no interaction $V=0$ and modeling the particles as particles in a box. As such, they are fundamentally quantum mechanical in nature. For a particle in a box, recall from Q.M. that for a potential,

\[ U(x) = \piecewise{0}{0 \leq x \leq L}{\inf}{\text{otherwise}} \]

The energy is given by,
\[ H = \f{p^2}{2m} \]

Where,

\[ \vec{p} = \f{\hbar}{i} \vdel \qquad H = -\f{\hbar^2}{2m}\del^2\]

The Time Independent Schrdinger Equation (TISE) gives,

\[ H \psi = \vep \psi \]

We can use this to find the energy $\vep$,

\[ -\f{\hbar^2}{2m}\del^2 \psi = \vep \psi \]

Which yields generic solution,

\[ \psi = A \sin \br{kx} \]

Where $k L = n \pi$ and $A = \sqrt{2/L}$ and the energy in the $n$-th state is given by,

\[ \vep = \f{\hbar^2}{2mL^2} \pi^2 n^2 \note{For 1D box}\]

However, the generalization for 3d space $\R^3$, the energy is determined by three quantum numbers $(n_x, n_y, n_z)$ all strictly positive integers $\Z_{>0}$,

\[ \vep = \f{\hbar^2}{2mL^2} \pi^2 \bc{n_x^2 + n_y^2 + n_z^2}\]

This energy $\vep$ is characterized by $3$ independent integers -for a single particle. For $N$ particles in this box $V = L^3 \subset \R^3$ we will require $3N$ independent numbers. Classically, the Lagrangian of the system is,

\[ L = T - V \]

Where $T$ is the kinetic energy and $V$ is the potential energy. The kinetic energy is given by,

\[ T = \f{p_x^2}{2m} + \f{p_y^2}{2m} + \f{p_z^2}{2m} \]

Evidently for a single particle, $n$ cannot be zero because then that would yield the trivial solution $\psi(x) = 0$. So why is it that for the 3d example, it is still required that $n_x, n_y, n_z > 0$ and each cannot equal zero? It is because the solution is solved using separation of variables. In any of the components of the solution are the trivial solution $n_i = 0$ for a particular $i$, the complete solution becomes the trivial solution. \\

Therefore using this three number $n_x, n_y, n_z$, we can describe the energy of a given particle for all time. However, for a fixed $\vep$ that can be measured, how many distinct set of $n_i$'s can produce this measured energy $\vep$?

\[ \f{\hbar^2}{2mL^2} \pi^2 \bc{n_x^2 + n_y^2 + n_z^2} = \vep\]

Since the $n_i$'s are integers, solving this problem for a fixed $\vep$ is the same as solving the Diophantine equation,

\[ x^2 + y^2 + z^2 = R^2 \eq \label{eq:spheredio}\]

Which geometrically is equivalent to asking the question:\\

\textit{Where does the surface of the sphere intersect with a 3d grid of unit spacing?} \\

Therefore, the total number of microstates for a fixed energy $\vep$ is the same as determining the number of solutions to \eqref{eq:spheredio}. Let us call,

\[ \vep^* = \f{2m}{\hbar^2\pi^2}V^{2/3} \vep \]

Where $\vep^*$ acts as a dimensionless energy that characterizes the problem ($R^2 = \vep^*$).

\[ \Om = \text{\# of solutions to \eqref{eq:spheredio}} \]

In general (like all Diophantine equations), this problem is very hard. It is known as the \textbf{microcanonical ensemble} problem.

\subsection{Microcanonical Ensemble}

The Microcanonical ensemble problem involves finding positive integers $n_x, n_y, n_z$ such that,
\[ n_x^2 + n_y^2 + n_z^2 = \f{2mL^2}{\hbar^2 \pi^2} \vep = \f{2mV^{2/3}}{\hbar^2 \pi^2} \vep \defined \vep^* \]
Classically the total energy energy of the system is given by the sum of the kinetic energies of each of the particles.
\[ E = K_1 + K_2 + K_3 + \cdots + K_N \]
Which when broken into each component,
\[ E = \br{\f{p_{1x}^2 + p_{1y}^2 + p_{1z}^2}{2m}} + \br{\f{p_{2x}^2 + p_{2y}^2 + p_{2z}^2}{2m}} + \cdots + \br{\f{p_{Nx}^2 + p_{Ny}^2 + p_{Nz}^2}{2m}} \]

For fixed $E$ in general, this constitutes a $3N$ dimensional problem,
\[ \sum_{i=1}^N \f{\hbar^2 \pi^2}{2 m V^{2/3}} \br{n_{ix}^2 + n_{iy}^2 + n_{iz}^2} = E \]

Which can be expressed with shorthand notation as,
\[ \sum_{r=1}^{3N} \f{\hbar^2 \pi^2}{2 m V^{2/3}} n_{r}^2 = E \]

Where $n_{r}$ is the $r$-th degree of freedom of the system. This can be express even more compactly by absorbing constants,
\[ \sum_{r=1}^{3N} n_{r}^2 = E^* \defined E \f{2 m V^{2/3}}{\hbar^2 \pi^2} \eq \label{eq:canonensemble} \]

Notice that the term $E V^{2/3}$ completely characterizes the number of microstates $\Om$ as it determines the number of solutions of \eqref{eq:canonensemble}. What we notice that without calculating $\Om$, we can determine the form of entropy.

\[ S = S\br{N,V,E} = S\br{N, E V^{2/3}} \]

For a system undergoing a reversible adiabatic change, we have both $\indif Q = 0$ which implies that $\dif S = 0$ for fixed temperature which implies that entropy is constant. Therefore for a reversible adiabatic process, in order to ensure that entropy is constant, it must be that,
\[ V^{2/3} \cdot E = \text{const.} \note{For reversible/adiabatic process} \]

Now using the known expression,

\[ P = - \pdert{E}{V}{N,S} \]

And using $c$ as a constant,

\[ E = c V^{-2/3} \]

Which gives,
\[ PV^{5/3} = \text{constant} \note{For reversible/adiabatic process.} \]

Or more generally, for quantum/classical systems,

\[ PV^\ga = \text{constant} \note{$\ga = 5/3$ for monoatomic system} \eq \label{eq:adiabtic} \]

Note that \eqref{eq:adiabtic} does not hold for relativistic systems as $E = pc$ where $c$ is the speed of light.

\subsubsection{Solving the Microcanonical Ensemble}
\label{sec:solvemircoensemble}

One will notice that as $E^*$ increases, the number of solution $\Om$ varies wildly. It can be precisely $0$ and some values for $E^*$ and very many for other, nearby energies. Instead of calculating only the $n_r$ for which $E$ is precisely given, we will instead compute all the states for which $E' < E$ to simplify the problem. In principle, we want to replace the problem of finding,

\[ \Om(N, V, E) \]

We will instead compute (for 1 particle),

\[ \sum_{'} \br{\vep'} = \sum_{\vep' \leq \vep} \Om(1, V, \vep') \]

Where,

\[ \sum_{n_r'}^{3} n_r^2 = \vep^* \]

Which can be converted to a continuous problem of calculating the volume of this sphere,
\[ \intl_V \dif V \]

Which is a problem of finding the volume of a sphere of radius $\sqrt{\vep^*}$. In this case of 1 particle, $V$ has dimension $3$. Therefore,

\[ V(R) = \f{4 \pi R^3}{3} \]
\[ V(\vep^*) = \f{4 \pi }{3}\br{\vep^*}^{3/2} \]

Now since we only care about the cases where $n_x, n_y, n_z$ are positive, this quantity \textit{overestimates} by $2^3= 8$ times the desired amount. Therefore the sum over all states where $0 \leq \vep'\leq\vep^*$ is

\[ \sum_{'} = \f{1}{8} \f{4\pi}{3} \br{e^*}^{3/2} = \f{\pi}{6} \br{e^*}^{3/2} \]

Now we must repeat this calculation for not a 3d sphere but a 3Nd-sphere.

\heading{Mathematical Interlude}

In 3-d, a volume element is given by, $\dif V = \dif x \dif y \dif z \defined \dif x_1\dif x_2\dif x_3$. \\

In n-d, a volume element is given by,

\[ \dif V_n = \prod_{i=1}^{n} \dif x_i \]

Therefore in n-d the volume of a sphere is given by,

\[ V_A = \intl \dif^n r = \intl \cdots \intl \dif x_1 \dif x_2 \cdots \dif x_n = \intl \cdots \intl \prod_{i=1}^{n} \dif x_i \]

And performing this sum under the constraint that,

\[ 0 \leq \sum_{i=1}^{n} x_i^2 \leq R^2 \]

\begin{center}
\begin{tabular}{|c|c|}
        \hline
        Dimensions & Volume of Ball \\
        \hline
        $2$ & $\pi R^2$ \\
        $3$ & $\f43 \pi R^3$ \\
        $\cdots$ & $\cdots$ \\
        $n$ & $c_n R^n$ \\
        \hline
\end{tabular}
\end{center}

Therefore, we need to find $c_n$ in general to get the volume of a hypersphere. Since $V_n = c_n R^n$, consider

\[ \dif V_n = \dif \br{c_n R^n} = \underbrace{c_n \cdot n \cdot R^{n-1}}_{S_n} \dif R \]

\textbf{Trick:} We will take a function that we like for which we can do exactly the integrals in $\dif x_i$ (i.e. Cartesian Coordinates) and perform the integral in spherical coordinates.

\[ \intl f(\bc{x_i}) \prod_{i=1}^n \dif x_i = \intl f(R) S_n \dif R = \intl f(R) n c_n R^{n-1} \dif R \]

Now make use of Gaussian integrals used in section \ref{sec:gaussianintegrals},

\[ f(x) = e^{-x^2} \]
\[ f(\bc{x_i}) = \prod_{i=1}^{n} e^{-x_i^2} \]

Gives,

\[ \intlf e^{-x^2} \dif x = \sqrt{\pi} \]

Which for generic $n$ with separating the terms $x_i$ gives,

\[ \prod_{i=1}^{n} \intlf e^{-x_i^2} \dif x_i = \intlf e^{-x_1^2} \dif x_1 \intlf e^{-x_2^2} \dif x_2 \cdots \intlf e^{-x_n^2} \dif x_n = \intl_0^{\inf} f(R) n c_n R^{n-1} \dif R \]

The RHS has $n$ terms that each evaluate to $\sqrt{\pi}$,

\[ \pi^{\f{n}{2}} = \intl_0^{\inf} f(R) n c_n R^{n-1} \dif R = n c_n \intl_0^{\inf} f(R) R^{n-1} \dif R \]

Since $f(R)$ is just a Gaussian in $R$,

\[ \pi^{\f{n}{2}} = n c_n \intl_0^{\inf} e^{-\br{\sum_{i=1}^{n}x_i^2}} R^{n-1} \dif R = n c_n \intl_0^{\inf} e^{-R^2} R^{n-1} \dif R \]

Using the identity,

\[ \intl_0^{\inf} e^{-\al y^2} y^\nu \dif y \defined \f{1}{2\al^{\f{\nu+1}{2}}}  \Ga\br{\f{\nu +1}{2}} \note{For $\nu > -1$}\]

Where $\Ga(x)$ is the \textit{Gamma function} which is equivalent to factorials for $x$ integers, we get,

\[ \pi^{\f{n}{2}} = n c_n \f12 \Ga \br{\f{n}{2}} \]

Thus,

\[ c_n = \f{\pi^{\f{n}{2}}}{\br{\f{n}{2}}!} \]

Therefore,

\[ V_n (R) = \f{\pi^{\f{n}{2}}}{\br{\f{n}{2}}!} R^n \eq \label{eq:vn}\]

Therefore for $n = 3N$ and $R = \sqrt{E^*}$ the total number of states contained inside the hypersphere with radius $R$ is given by,

\[ {\sum}_N (N, V, E) = \br{\f{V}{h^3}}^N \f{\br{2\pi m E}^{3N/2}}{\br{\f{3N}{2}}!} \]

Where ${\sum}_N$ doesn't explicitly represent a \textit{sum}. Instead it is the total number of microstates within the hypersphere. It is a \textit{function} of $N, V, E$. Note this includes the factor of $\f12^{3N}$ to only count the positive integer solutions.\\

What we want to do is to derive the thermodynamic properties for E. In reality however, no system is perfectly isolated from surroundings. In practice $E$ is not exactly defined. We will construct the idea of $\De$ or uncertainty in $E$ is only because of the imperfect nature of the system. Assume that the energy is given by $E \pm \f{\De}{2}$. We will also assume that $\De \ll E$. Since initially we wanted to know $\Om(N, V, E)$ we really want to know the the number of states associated with a measure of the energy within the uncertainty. This will be the difference of two ${\sum}_N$ values.

\[ \Ga_{N, V, \bar{E} = E} = {\sum}_N \br{N, V, E + \f{\De}{2}} - {\sum}_N \br{N, V, E - \f{\De}{2}} \eq \label{eq:deltaE}\]

Now in the limit that $\De/E \ar 0$,

\[ \lim_{\De/E \ar 0} \Ga_{N, V, \bar{E} = E} = \Ga_{N, V \bar{E} = E} = \lim_{\De/E \ar 0} \bs{{\sum}_N \br{N, V, E + \f{\De}{2}} - {\sum}_N \br{N, V, E - \f{\De}{2}}} = \pder{{\sum}_N}{E} \cdot \De \]

Therefore the total number of states associated with $E= \bar{E}$ is given by,

\[ \Ga_{N, V, \bar{E} = E} = \pder{\sum_N}{E} \cdot \De = \f{3N}{2} \f{\De}{E} \sum_N(N, V, E =\bar{E}) \eq \label{eq:gahypershell}\]

Using the Boltzmann equation for entropy and Stirling's approximation with $N \gg 1$ (see section \ref{sec:stirling}),

\[ S = k_B \ln\Ga_N = k_B \bs{N \ln \bc{\f{V}{h^3} \br{\f{4\pi m E}{3N}}^{3/2}} + \f{3N}{2} + \bc{\ln \br{\f{3N}{2}} + \ln \br{\f{\De}{E}}}}  \]

This gives us the entropy of an ideal gas at energy $E$ within a window $E \pm \De /2$.\\

What thermodynamic properties does this expose?
\[ S(N,V,E) = k_B \ln\Ga_N \approx k_B \bs{N \ln \bc{\f{V}{h^3} \br{\f{4\pi m E}{3N}}^{3/2}} + \f{3N}{2}} \eq \label{eq:notextensive} \]

Notice that this depends on $\ln(N)$ and also $N^{-3/2}$ which seems to indicate that theses are not extensive variables. We will return to this problem soon. Rearranging for energy,

\[ E(S, V, N) = \f{2h^2N}{4 \pi m V^{2/3}} \exp\br{\f{2S}{3Nk_B} - 1} \]

Thermodynamically, we have,
\[ T = \pdert{E}{S}{V,N} \]

Which reveals,
\[ E = N \br{\f32 k_B T} \]

And the heat capacity at constant volume is given by,
\[ c_V = \pdert{E}{T}{N,V} = \f32 N k_B = \f32 nR \]

And analogously,
\[ c_P = T \pdert{S}{T}{N,P} = \f53 N k_B \]

Furthermore, thermodynamic pressure is given by,
\[ P = -\pdert{E}{V}{N,S} = \f23 \f{E}{V} \]

Combining these two results, we can see that,
\[ PV = nRT \]

Which means we have recovered all known results for ideal gases from only the \textit{explicit} form of $\sum_N$ (or $\Ga_N$).\\

Consider an isothermal change of entropy of entropy, and we recover,

\[ S_2 - S_1 = N k_B \ln\br{\f{V_2}{V_1}} \]

Or for a reversible adiabatic change/process,
\[ E,T \sim V^{-2/3} \]
\[ PV^\ga = \text{const.} \]
\[ TV^{\ga -1} = \text{const.} \]

We also recover that,

\[ \dif E = -p \dif V = -\f23 \f{E}{V} \dif V \]

In conclusion we have discovered that measuring and counting microstates $\Om_N \approx \sum_N, \Ga_N$ we can then use $S = k_B \ln \Om$ to recover and describe thermodynamic properties. All that remains is to discuss the problem that $V$ and $N$ do \textit{not} appear to be extensive variables in equation \eqref{eq:notextensive}. This problem is known as \textbf{Gibb's Paradox}.

\subsection{Gibb's Paradox}
How can we fix or understand the origin of the lack of extensivity in the formula $S = k_B \ln \Om$? We should expect $S$ to scale with $V$ but in \eqref{eq:notextensive} it does not. Does this mean that \eqref{eq:notextensive} is somehow wrong? \\

To expose this paradox, consider a container segmented by a wall into 2 regions with properties $N_1, V_1, T_1$ and $N_2, V_2, T_2$ with total volume $V = V_1 + V_2$. Now measure the entropy using \eqref{eq:notextensive} for each system $S_1$, $S_2$ and then adiabatically remove the wall between the segments of the container and recalculate entropy $S_T$ of the combined system. Does $S_T = S_1 + S_2$?

Using $E = \f32 N k_B T$

\[ S_i = N_i k_B \ln V_i + \f32 N_i k_B \bc{1 + \ln\br{\f{2 \pi m_i k_B T}{h^2}}} \quad i = 1,2 \]

While,

\[ S_T = \sum_{i=1}^{2} N_i k_B \ln V\]

Consider the case where the particle densities are the same and thus is also the same for the mixture after removing the wall.

\[ \f{N_1}{V_1} = \f{N_2}{V_2} = \f{N_1+ N_2}{V_1 + V_2} \]

Therefore the change in entropy is given by,

\[ \De S = S_T - \br{S_1 + S_2} \]

Which is explicitly,

\[ \De S = k_B \bc{N_1 \ln\br{\f{N_1 + N_1}{N_1}} + N_2\ln\br{\f{N_1 + N_1}{N_2}}} \eq \label{eq:entropyup} \]

Now lets assume $m_1 = m_2$ or that we have identical particles. From this, the macrostate of the system looks like it doesn't change when the wall is removed by we still have by equation \eqref{eq:entropyup} that,

\[ \De S > 0 \]

This makes no sense because if the particles are the same, we have not \textit{mixed} anything. \\

The resolution is to write (using Stirling's formula),

\[ \De S = k_B \bc{\ln \bs{\br{N_1 + N_2}!} - \ln \bs{N_1!} - \ln \bs{N_2!}} \]

To fix this problem we need to reduce $S$ or $\Om$ by a factor of $N!$.

\[ S \text{ reduced by } k_B \ln N!\]
\[ S_1 \text{ reduced by } k_B \ln N_1!\]
\[ S_2 \text{ reduced by } k_B \ln N_2!\]

This is an \textit{ad hoc} fix. It does not have any profound explanation of justification. The mechanism that describes this, is the notion of indistinguishably. The notion of indistinguishably is needed to be taken into account in order to properly count the number of microstates of a system. Proper analysis of this idea requires the theory of Quantum Statistics. Now we can go back and determine the proper formula for \eqref{eq:notextensive}.

\[ S(N,V,E) = N k_B \ln\br{\f{V}{N}} + \f{3Nk_B}{2} \bc{\f53 + \ln\br{\f{2 \pi m k_B T}{h^2}}} \eq \label{eq:sackurtetrode}\]

This is the formula for the entropy of a non-degenerate ideal gas. This is known as the \textbf{Sackur-Tetrode equation}. One should note that this equation is quite remarkable. It is composed of only $N, V, E$ and a set of constants. This equation gives a proper extensivity of $S$. \\

For more, look at the theory of super-solid helium.

\section{Ensemble Theory}

Let's step back and review some of the statements we have made. Here are come general facts:
\begin{itemize}
    \item Every microstate has equal probability of being selected
    \item As time passes, the system will explore all of the microstates
    \item Experimental measurements are made of the time averaged of the true, non-stationary value of physical quantities $f$
    \begin{itemize}
        \item $f = E, \text{pH}, \text{etc.}$
        \item For example, measuring the pH level of a chemical solution will be the average measure over a macroscopic duration of time
        \item For systems where the dynamics of the system have time scale much shorter than the measurement frequency, the measured quantity ends of being an averaged value
    \end{itemize}
    \item Making a ``movie'' of a physical system (many measurements) and replacing the time average by a ``still frame'' average
    \begin{itemize}
        \item $\bs{f}\tsb{time} = \f1T \intl_{t_0}^{t_0 + T} f(t) \dif t$: the square brackets with subscript time indicate an average measure over time
        \item $\ba{f}\tsb{exp} = \bs{f}\tsb{time}$: Natural one should expect the time averaged value to be measure experimentally
        \item In doing thus, there must be some sort of averaging of the true real-time value using ``snapshots'' of $f$
    \end{itemize}
\end{itemize}
The question becomes, what are the options for performing this ``snapshot'' averaging? One could make periodic snapshots at fixed frequency or random intervals or combinations there of. It is the spirit of ensemble theory to determine how to construct an adequate way to compute $\ba{f}\tsb{exp}$ withing explicitly doing $\int f(t) \dif t$. Some options include,
\begin{itemize}
    \item microcanonical ensemble
    \item canonical ensemble
    \item grand ensemble
\end{itemize}
The procedure introduced by Gibb's involves utilizing phase space and classical systems.\\

\textbf{Remark by \AuthorName:} This is intimately connected to the \textit{Erds discrepancy problem}. (I think?)

\subsection{Phase Space of Classical System}
\subsubsection{Dynamics \& Phase Space}

Imagine we have a system of $N$ \textit{distinguishable} particles,
\[ i = 1,2,3,\ldots,N \]
that lives in the space of 3 dimensions $\R^3$. We will fix for the system $N,V,E$. This is a microcanonical system/construction. We will also consider for the system that the particles are localized (they are were they are) and they do move (they have dynamics). The position of a particle $\mu$ is given by,
\[ \vr_\mu = x \hx + y \hy + x \hz \]
And each particle $\mu$ has same mass $m$. The momentum of particle $\mu$ is given by,
\[ \vp_\mu = m \dvec{r}_\mu \]
To index the entire system of $N$ particles, we will consider this index map,
\[ \vr_\mu = r_{\al,\mu} \mapsto r_i \quad \al = 1,2,3 \quad  \mu = 1,2,\ldots, N \quad  i = 1,2,\ldots,3N \]
Similarly for $\vp_\mu$
\[ \vp_\mu = p_{\al,\mu} \mapsto p_i \quad \al = 1,2,3 \quad  \mu = 1,2,\ldots, N \quad  i = 1,2,\ldots,3N \]
The product of the two spaces $\bc{r_i} \times \bc{p_i}$ is a $6N$ dimensional space known as the \textbf{phase space} of the system. To determine the dynamics of the system, we need to solve the following problem.
\[ \mathcal{L} = K\br{\bc{\dot{q}_i}} - V\br{\bc{q_i, \dot{q}_i}} \]
Where $\mathcal{L}$ is the \textbf{Lagrangian}, $K$ is the kinetic energy that depends on velocities, and $V$ is the potential energy which depends on position and sometimes velocities. Will will make the switch to the \textit{Hamiltonian} formalism of classical mechanics through a \textbf{Legendre Transform},
\[ H = \sum_{i}^{3N} \dot{q}_i \pder{\mathcal{L}}{\dot{q}_i} - \mathcal{L} = \sum_{i}^{3N} \dot{q}_i p_i - \mathcal{L} \]
Where $p_i = \pder{\mathcal{L}}{\dot{q}_i}$ is the generalized momentum. In principle, all classical problems can be described under this formalism. The ``bottle neck'' of this formalism is typically solving the equations of motion,
\[ \dot{p}_i = - \pder{H}{q_i} \qquad \dot{q}_i = \pder{H}{p_i} \eq \label{eq:hamiltonmechanics} \]
For generic physical quantity $f\br{q,p;t}$ that depends on time, positions an momentum of all the particles, all that would be required is to solve the equations of motion, and predictions could be made. However, this is very \textit{difficult}. Ensemble Theory allows for the construction of proper approximations for generic $f$. \\

\heading{$\mu$-space vs. $\Ga$-space}

We will now introduce the notion of $\mu$-space. In this $6N$ dimensional space, we assign to each particle $\mu$ a single point,

\[ \vr_\mu , \vp_\mu \mapsto \br{r_x, r_y, r_z, p_x, p_y, p_z} \]

For $N$ particles, there are $N$ points in the $\mu$-space which are characterized by $6N$ numbers. The alternative description is phase space ($\Ga$-space). Points in phase space are representative points on the \textit{entire} system at once,
\[ \text{state vector} = \br{q_1, q_2, \ldots , q_{3N} ; p_1, p_2, \ldots , p_{3N}} \]
At a given time $t$, there is $1$ point in phase space that is $6N$ dimensional. We will subscribe to the notation,
\[ \br{q_i\br{t}, p_i\br{t}} = \br{q,p} \]

\subsubsection{Trajectories in Phase Space}
How can be discuss or describe the time evolution of the system using phase space? The state vector of the system $\br{q,p}$ changes over time and this describes how each and every particles position and momentum changes with time.\\

As a function of time, the representative point traces a trajectory which \textbf{never} intersects itself. This is because the time evolution of the system is governed by the Hamiltonian $H$ and the solutions to the equations of motion are unique for given initial conditions. If there were intersection, take the intersection point as the initial condition and there would be two potential trajectories available for the system. Note however, that the system can loop back on itself and form cyclic trajectories. \\

Furthermore because of the fixed energy $E$ condition, there is a loss of 1 degree of freedom. Thus the trajectory is constrained to a $\br{6N-1}$ dimensional surface or hypersurface. Similar to the microcanonical ensemble problem solved in section \ref{sec:solvemircoensemble} (specifically equation \eqref{eq:deltaE}), we will treat this hypersurface or hypershell as a volume between the surfaces prescribed by the two energies,
\[ E + \f{\De}{2}, E - \f{\De}{2} \]

In general, the representative points $\br{q,p}$ are bounded. The positions $q$ are bounded by volume and the momentums $p$ by total energy through $K = \f{p^2}{2m}$.\\

\textbf{Remark:} One refers to classical solutions for ``real particles'', as opposed to \textit{virtual particles} (\textit{off shell}), as being \textit{on shell}, meaning they are found \textit{exactly} on the hypersurface defined by fixed energy $E$. \\

\subsection{Essential Replacement for Ensemble Theory}

So what do we mean when discussing ``ensembles''? Well one thing to notice is there a issue of difficulty with trying to solve the equations of motion \textit{explicitly}. We would like to instead compute a notion of average,
\[\ba{f(q,p)}\tsb{exp} = \f{1}{T} \intl_{t_0}^{t_0 + T} f\br{q(t),p(t)} \dif t \]
Where $T$ is a duration that is ``long enough''. The equations of motion for the system are given by Hamiltonian mechanics \eqref{eq:hamiltonmechanics},
\[ \dot{q}_i = \pder{H}{p_i} \qquad \dot{p}_i = -\pder{H}{q_i} \]
Solving these $3N + 3N = 6N$ equations is done explicitly in molecular dynamics and in the study of galaxy formation by this is in general very difficult. We would like to replace this with something more tractable. \\

For a given macrostate $\br{N,V,E}$, we have $6N$ microscopic variables. In this sense the macroscopic description is \textit{incomplete}. We would like to take advantage of this over abundance of ``micro-information'' and average/integrate it out. What this means is that we need some kind of density or probability description of the microstates. \\

Another thing to consider is we don't really have $E$ totally precise. Instead we can examine the hypershell of energy. Recall our previous description of a hypershell,
\[ \bar{E} - \f{\De}{2} \leq E \leq \bar{E} + \f{\De}{2} \]

\subsubsection{Ergodic Hypothesis}
\label{sec:ergodic}
With the Ergodic Hypothesis comes both good news and bad news.

\begin{center}
\begin{tabular}{c|c}
    Good news & Bad news \\
    \hline
    It cannot be proven & It cannot be proven \\
\end{tabular}
\end{center}

\textbf{Statement}: Given long enough time the representative point of an isolated system will come \textit{arbitrarily close} to any given point on the energy hypershell/phase space. Equivalently, given enough time, the system will explore all possible locations on the $6N-1$ hypersphere (this is somewhat justified as the trajectories cannot intersect themselves). \\

\textbf{Comments \& Examples}:

For the system of a simple harmonic oscillator,
\[ E = \f{p_1^2}{2} + \f{q_1^2}{2} \]
The Ergodic hypothesis is satisfied. It explores all points on the ellipse in phase space.\\

For a system of springs coupled,
\[ E = \f{p_1^2}{2} + \f{p_2^2}{2} + \f{q_1^2}{2} + \f{q_2^2}{2} + q_1q_2 \]
It can be shown that finite regions of phase space are not visited by solution trajectories. In general, the Ergodic Hypothesis is far from trivial or obvious but it is believed to apply in many, many cases. Note that a famous example of the the potential failing of the Ergodic Hypothesis is the case of glass panes, clays, or gels. We don't yet have a description of how these systems behave microscopically. \\

\subsubsection{Mixing Hypothesis}

Note that for the $3$-body problem, we have chaotic solutions. This is also the case for $N > 3$. If we consider a system with many, many degrees of freedom ($N \gg 1$), we should expect that there are many degrees of freedom that will tend to be chaotic and explore all sorts of potential values on-shell. This can be taken as motivation for the Ergodic Hypothesis. \\

This explosion in time of an initial distribution of representative points suggests we should focus on a notion of density of representative points in phase space. This density will be time-dependent.

\subsubsection{Statistical Ensemble}
The idea of statistical ensembles is to make \textit{mental copies} of the system of interest. What this means is to ascribe a representative point to each member of the ensemble. Therefore each member of the ensemble will track a representative point. In doing so, we can \textit{replace} the time-averaging of the \textit{real} system by performing an averaging of the long time results of many duplicated and repeated experiment.

\newcommand{\bwd}[2]{
\begin{tikzpicture}[scale=0.2]
        \draw (-1,-1)--(-1,1)--(1,1)--(1,-1)--(-1,-1);
        \fill [red] (#1, #2) circle (5pt);
\end{tikzpicture}
}

\begin{center}
\begin{tabular}{|c|c|c|}
    \hline
    $k$ & $t$ & $t + \dif t$ \\
    \hline
    1 & \bwd{0.5}{0.25} & \bwd{0.4}{0} \\
    2 & \bwd{-0.5}{0.25} & \bwd{0.4}{0} \\
    3 & \bwd{0.5}{-0.43} & \bwd{-0.2}{0} \\
    4 & \bwd{-0.5}{-0.2} & \bwd{-0.7}{0} \\
    5 & \bwd{-0.5}{0.5} & \bwd{0.8}{0} \\
    6 & \bwd{0.3}{0.3} & \bwd{0.3}{0} \\
    7 & \bwd{-0.6}{0.4} & \bwd{0}{0} \\
    8 & \bwd{0}{-0.7} & \bwd{-0.8}{0} \\
    $\vdots$ & $\vdots$ & $\vdots$ \\
    \hline
\end{tabular}
\end{center}

Each of these $k$ tracks the location of the representative point over time. These boxes can be thought of ``trackers'' or ``members'' of the phase space. They act as a single vector that represents the entire system.

\begin{center}
\begin{tikzpicture}[scale=1.0]
        \draw (-1,-1)--(-1,1)--(1,1)--(1,-1)--(-1,-1);
        \draw (2.5,-0.5)--(2.5,0.5)--(3.5,0.5)--(3.5,-0.5)--(2.5,-0.5);
        \draw [->] (1.25,0)--(2.25,0);
        \fill [blue] (0.6,0) circle (1pt);
        \fill [blue] (-0.8,0.4) circle (1pt);
        \fill [blue] (0.6,0.9) circle (1pt);
        \fill [blue] (-0.3,-0.2) circle (1pt);
        \fill [blue] (0.7,0) circle (1pt);
        \fill [blue] (0.8,0.1) circle (1pt);
        \fill [blue] (-0.8,-0.1) circle (1pt);
        \fill [blue] (-0.2,-0.9) circle (1pt);
        \fill [blue] (-0.2,0.4) circle (1pt);
        \fill [blue] (0.3,-0.3) circle (1pt);
        \fill [red] (3.2,0.1) circle (1pt);
        \node [below] (lps) at (3,-0.5) {$\Ga$ Phase Space ($\R^{6N}$)};
        \node [below] (lss) at (0,-1.0) {$\R^3$ Real Space};
\end{tikzpicture}
\end{center}

We need an object/quantity that gives us a measure of the distribution representative points.
\[ \rho \br{\underbrace{p(t), q(t)}_{6N \text{ variables}}; t} \]
This is density of representative points per unit of phase space volume $\dif^{3N} q \dif^{3N} p$. Consider this quantity,
\[ \bc{\rho \br{q,p;t} \dif^{3N} q \dif^{3N} p} \]

This gives us the number of points around the $6N$ dimensional point $(q,p)$ in phase space. $\rho(q,p;t)$ symbolizes the way that the members of the ensemble (of representative points) are distributed overall possible microstates $(q_i, p_i)$ at different instants in time.

\[ \ba{f}\tsb{ensemble} \defined \f{\int f(q,p;t) \rho(q,p;t) \dif^{3N}q\dif^{3N}p}{\int\rho(q,p;t) \dif^{3N}q\dif^{3N}p} \eq \label{eq:fensemble} \]

The idea here is to replace the time averaging associated with the real experiment $\ba{f}\tsb{exp}$ to the ensemble average characterized by the ensemble member weight $\rho$.
\[ \ba{f}\tsb{exp} = \ba{f}\tsb{ensemble} \]

This $\rho$ acts a weight for all possible $k$'s that could potentially make up the solution. The goal is to compute this $\rho(q,p;t)$. To do this, we will make use of the fact that $\rho$ is constrained by the equations of motion. We must invoke the time dependence of $q,p$. Using \eqref{eq:hamiltonmechanics},
\[ \dot{q}_i = \pder{H}{p_i} \qquad \dot{p}_i = -\pder{H}{q_i} \]
Gives us a corresponding description of $\rho\br{q,p;t}$. \\

\textbf{Important Remark}: If we are concerning ourselves with statical mechanics, we are limiting ourselves the constraint that,
\[ \pder{\rho}{t} = 0 \]
We have to consider how $\rho(p,q)$ is compatible with the equations of motion. The solution to this is known as \textbf{Liouville's Theorem}.

\subsubsection{Liouville's Theorem}
What are the options or possible forms for $\rho(p, q)$? First remember that each member, namely \bwd{0.4}{0.3}, of the ensemble \textit{drives} the time evolution of its own representative point. As time marches on, the number of representative points that contribute to the whole set of members of the ensemble evolves. In fact, there is an effective flow of members in the ensemble moving in and out of the shell of solutions.

\[ \# : \rho \br{p,q}\dif^{3N}q\dif^{3N}p \]

Thus this integral over this quantity varies in time,
\[ \pder{}{t}\intl \rho \br{p,q}\dif^{3N}q\dif^{3N}p \eq \label{eq:reflow} \]

The notion of flow (in/out of membership) describes the flow (in/out) of the phase space. With this notion of flow, there is a conservation law on the representative points; or a flow of representative point density. By analogy, this can be considered an effective ``current''.
\[ \intl \vJ\tsb{R.P.} \cdot \hn \dif \sigma = \intl \rho \br{p,q} \vv \cdot \hn \dif \sigma \eq \label{eq:rpflux} \]
Where $\vv$ is a velocity of the rate of displacement of representative points.
\[ \vv = \br{\dot{q}, \dot{p}} \]
Here $\vv$ is not a \textit{physical velocity} in the sense of particles moving. It is constructed velocity associated with the time derivative of a representative point. The $\dot{q}$ components are interpreted as the \textit{actual} velocity of each of the particles, and the $\dot{p}$ terms are the \textit{actual} forces acting on a given particle. Instead this vector $\vv$ is a flux of representative points through the surface of the solution shell. The conservation implies that \eqref{eq:rpflux} balances \eqref{eq:reflow}.
\[ \pder{}{t}\intl_\w \rho \br{p,q}\dif^{3N}q\dif^{3N}p = - \intl_\sigma \rho \br{p,q} \vv \cdot \hn \dif \sigma \]
Notice the negative sign here; the LHS is positive whenever the density inside $\w$ is \textbf{increasing}, indicating the corresponding flux should be negative. Where $\w$ is the phase space volume, and $\sigma = \di \w$ is the surface bounding $\w$. Recognize the use of divergence theorem,
\[ \intl_\sigma \rho \br{p,q} \vv \cdot \hn \dif \sigma = \intl_\w \div \br{\rho \vv} \dif \w \]
Where the volume element is given by $\dif \w = \dif^{3N}q\dif^{3N}p$. Combining these results,
\[ \pder{}{t}\intl_\w \rho \br{p,q}\dif \w + \intl_\w \div \br{\rho \vv} \dif \w = 0 \]
Since $\w$ is presumed to have no explicit time dependence,
\[ \intl_\w \pder{\rho}{t} + \div \br{\rho \vv} \dif \w = 0 \]

Which by the Reymond-Dubois Lemma gives us what is known as the \textbf{Phase Space Continuity Equation},
\[ \pder{\rho}{t}  + \div \br{\rho \vv} = 0 \]

More explicitly breaking up the $6N$ components of the divergence term into the relevant $3N,3N$ chunks,
\[ \pder{\rho}{t} + \sum_{i=1}^{3N} \br{\pder{}{q_i} \br{\rho \dot{q}_i} + \pder{}{p_i}\br{\rho \dot{p}_i}} = 0 \]
Which by product rule,
\[ \pder{\rho}{t} + \sum_{i=1}^{3N} \br{\pder{\rho}{q_i} \dot{q}_i + \pder{\rho}{p_i} \dot{p}_i} + \sum_{i=1}^{3N} \br{\rho\pder{\dot{q}_i}{q_i} + \rho\pder{\dot{p}_i}{p_i} } = 0 \]
Using the equations of motion \eqref{eq:hamiltonmechanics},
\[ \pder{\dot{q}_i}{q_i} = \pder{}{q_i} \br{\pder{H}{p_i}} = \f{\di^2H}{\di q_i \di p_i} \]
Assuming that $H$ is analytical in its components, we can swap order of operations,
\[ \f{\di^2H}{\di q_i \di p_i} = \f{\di^2H}{\di p_i \di q_i} = \pder{}{p_i} \br{\pder{H}{q_i}}\]
Therefore by the dynamics of the system,
\[ \pder{\dot{q}_i}{q_i} = -\pder{\dot{p}_i}{p_i} \]
Making these terms drop out.
\[ \pder{\rho}{t} + \sum_{i=1}^{3N} \br{\pder{\rho}{q_i} \dot{q}_i + \pder{\rho}{p_i} \dot{p}_i} = 0 \]
Again using the equations of motion \eqref{eq:hamiltonmechanics},
\[ \pder{\rho}{t} + \underbrace{\sum_{i=1}^{3N} \br{\pder{\rho}{q_i} \pder{H}{p_i} - \pder{\rho}{p_i} \pder{H}{q_i}}}_{\bc{\rho, H}} = 0 \]
Where $\bc{\rho, H}$ is known as the \textbf{Poisson Brackets} for this problem. By analogy in Quantum Mechanics the Poisson brackets become a commutator.
\[ \bc{\rho, H} \Leftrightarrow \bs{\rho, H} \]
This is known as \textbf{Liouville's Theorem},
\[ \pder{\rho}{t} + \bc{\rho, H} = 0 \]
\textbf{Observations}:
\begin{enumerate}
    \item $\pder{\rho}{t} = 0$ is already imposed for the regime of equilibrium statistical mechanics
    \item If $\rho$ is a constant, everything becomes zero $\bc{\rho, H} = 0$
    \item Making $\rho\br{p,q}$ not explicitly dependent on $\br{p,q}$ enforces that $\rho = \rho\br{H\br{q,p}}$
    \begin{itemize}
        \item If $\rho$ is to have dependence on $N,V,T$ we can makes the choice that $\rho = \exp\br{-\be H\br{q,p}}$. In doing so we will show that $\be = 1/k_bT$ (this is known as the canonical ensemble)
    \end{itemize}
\end{enumerate}
\subsection{Microcanonical Ensemble Revisited}
As a reminder, we have a hypershell in phase space of energy $E \pm \De / 2$ with $\rho\br{q,p}$ is constant on the hypershell. Using out new framework with \eqref{eq:fensemble} independent of time,
\[ \ba{f}\tsb{ensemble} \defined \f{\int_{\w} f(q,p) \rho(q,p) \dif^{3N}q\dif^{3N}p}{\int_{\w}\rho(q,p) \dif^{3N}q\dif^{3N}p} \eq \label{eq:indepensembleavg} \]
Where $\w$ indicates out integrals are performed with respect to energy within $E \pm \De / 2$. Using the notion of time averaging,
\[ \ba{f}\tsb{exp} = \bs{f\br{t}}\tsb{time} = \f{1}{T}\intl_{t_0}^{t_0 + T} f(t) \dif t \]
\textbf{1)} The first question that arises: Is the ensemble averaging equal to the time averaging technique?
\[ \ba{f}\tsb{ens} \stackrel{?}{=} \bs{f(t)}\tsb{time} \]
\textbf{2)} The second question that arises: Is $\rho\br{q,p} = $ constant indicate the probability of finding a representative point in a give volume $\dif \w$ in phase space the same as finding a representative point in a different volume $\dif \w'$? More clearly, does a uniform distribution $\rho$ indicate a uniform probability? A priori equal probability to find the system in any microstate. Up until know, we presumed this to be an axiom or postulate of statistical mechanics (see section \ref{sec:postsm}). \\

\textbf{Answer to 1)}:
Given the ensemble average $\ba{f}\tsb{ens}$ from \eqref{eq:indepensembleavg}, since the ensemble is stationary $\pder{\rho}{t} = 0$, then nothing is changed doing a time average after an ensemble average.
\[ \ba{f} = \ba{f}\tsb{ens} \]
Then perform a time average on the ensemble average,
\[ \ba{f}\tsb{ens} = \bs{\ba{f}\tsb{ens}}\tsb{time} \]
As the time average and ensemble average are independent (i.e. the Ergodic Hypothesis is satisfied; see section \ref{sec:ergodic}), swap the order of averaging,
\[ \ba{f} = \bs{\ba{f}\tsb{ens}}\tsb{time} = \ba{\bs{f}\tsb{time}}\tsb{ens} \]
Now notice that the time averaging of any quantity $f(t)$ over a long time $T$ must be the same for each member of the ensemble. Again utilizing the Ergodic hypothesis, any representative point will explore the whole phase space for each member of the ensemble over long enough time. In this is the case, there is \textit{nothing} gained by performing the ensemble averaging after the time averaging. Therefore we can write,
\[ \ba{\bs{f}\tsb{time}}\tsb{ens} = \bs{f}\tsb{time} \]
Combining these results,
\[ \bs{f(t)}\tsb{time} = \ba{f}\tsb{ens} \]
Therefore the experimental averaging is given by,
\[ \ba{f}\tsb{exp} = \bs{f(t)}\tsb{time} = \ba{f}\tsb{ens} \]

\textbf{Answer to 2)}:
What is the relationship between $\Ga\br{N,V,E}$ the number of microstates for fixed $N, V, \bar{E}$ and $\w$ the volume of phase space (where again $\bar{E} - \De / 2 < E < \bar{E} + \De / 2$)? There is a relationship between $\w$ and $\Ga$, namely if you enlarge the phase space volume the total number of potential microstates increases. We should expect,
\[ \Ga \propto \w \]
However $\Ga$ is dimensionless, while $\w$ is not. This was reconciled between through $S = k_B \ln \Ga$ where $k_B$ caries the appropriate dimensions. Specifically $\w$ has the units $q^{3N}p^{3N}$. This motivates the scaling factor $\w_0$ such that,
\[ \Ga = \f{\w}{\w_0} \]
The units of $\w$ in a typical case of one particle is given by,
\[ \bs{\w} \sim \bs{q}\bs{p} \sim \br{\SI{}{\m}}\br{\SI{}{\N}\cdot \SI{}{\s}} \sim \SI{}{\J\s} \]
Which is equivalent to the units of angular momentum similar to Planck's constant. Therefore we can motivate the factor that $\w_0 = z h^N$ where $z$ is a dimensionless constant. Recall that the calculation of the Sackur-Tetrode equation \eqref{eq:sackurtetrode} involved Planck's constant $h$.

\subsection{Two Examples}
\subsubsection{Compute Entropy}

The first example that will be examined is computing $S$ from some phase space integrals involving the previously motivated relationship $S \sim k_B \ln \f{\w}{\w_0}$. We have from the density function $\rho\br{q,p}$ of representative points,
\[ \w = \underbrace{\int \cdots \int}_{\text{fixed } N,V,E \br{\pm \De / 2}} \dif^{3N} q \dif^{3N} p \cdot \rho\br{q,p} \]
Examining the case of $\rho = \rho^*$ constant and separating the integrals for position and momentum,
\[ \w = \rho^* \cdot \bc{\int \cdots \int \dif^{3N} q} \bc{\int \cdots \int \dif^{3N} p} \]
Where the $\dif^{3N} q$ integral is subject to volume constraints and the $\dif^{3N} p$ integral is subject to energy constraints. The position constraint gives,
\[ \int \cdots \int \dif^{3N} q = V^N \]
Where as the energy constraints give,
\[ E - \f{\De}{2} \leq \sum_{i=1}^{3N}\f{p_i^2}{2m} \leq E + \f{\De}{2} \]
Where $i$ counts the degrees of freedom. We can then perform a change of variables $p \ar y$ to get,
\[ \int \cdots \int \dif^{3N} p = \int \cdots \int \dif^{3N} y \]
With modified energy constraint,
\[ 2m\br{E - \f{\De}{2}} \leq \sum_{i=1}^{3N}y_i^2 \leq 2m\br{E + \f{\De}{2}} \]
This new integral over $y$ becomes the surface of a $3N$-dimensional hypersphere with thickness given by,
\[ \de y_- = \sqrt{2m\br{E - \f{\De}{2}}} \quad \de y_+ = \sqrt{2m\br{E + \f{\De}{2}}} \eq \label{eq:shellbounds} \]
In order to determine the surface area of this hypersphere, we will leverage the result computed for the volume of a hypershell before. Therefore the surface area is approximated (for thickness thin enough) that the surface area is given by,
\[ \text{volume} = \br{\text{thickness}} \times \br{\text{surface area}} \]
The thickness of this shell is given by the difference in the bounds given \eqref{eq:shellbounds},
\begin{align*}
\de y_- &= \sqrt{2m\br{E - \f{\De}{2}}} \\
&= \sqrt{2mE}\br{1 - \f{\De}{2E}}^{1/2} \\
&= \sqrt{2mE}\br{1 - \f{\De}{4E} + \cdots} \note{Taylor expansion $\br{1+x}^n \approx 1 + nx$} \\
&= \sqrt{2mE} - \f12 \f{\De\sqrt{m}}{\sqrt{2E}}
\end{align*}
Similarly for the upper bound,
\[ \de y_+ = \sqrt{2mE} + \f12 \f{\De\sqrt{m}}{\sqrt{2E}} \]
Therefore the thickness is given by,
\[ \de y_+ - \de y_- = \f{\De\sqrt{m}}{\sqrt{2E}} \]
Using the surface area given by a derivative in \eqref{eq:vn} with $n = 3N$,
\[ S_n \br{R} = \der{V_n\br{R}}{R} = \f{n \pi^{n/2}}{\br{\f{n}{2}}!} R^{n-1} = \f{3N \pi^{3N/2}}{\br{\f{3N}{2}}!} R^{3N-1} = \f{2\pi^{3N/2}}{\br{\f{3N}{2} - 1}!} R^{3N-1} \]
In this example, the radius is $R = \sqrt{2mE}$ so the volume of the shell becomes the thickness times the surface area,
\[ \text{surface area} = \f{2\pi^{3N/2}}{\br{\f{3N}{2} - 1}!} \times \br{2mE}^{\f{3N - 1}{2}} \]
\[ \w = V^N \times \De \br{\f{m}{2E}}^{1/2} \times \f{2\pi^{3N/2}}{\br{\f{3N}{2} - 1}!} \times \br{2mE}^{\f{3N - 1}{2}} \eq \label{eq:omega} \]
\[ \w = V^N \times \br{\f{\De}{E}} \times \f{\br{2 \pi m E}^{3N/2}}{\br{\f{3N}{2} - 1}!} \]
Comparing with \eqref{eq:gahypershell} we get the expected result for $\w_0$,
\[ \f{\w}{\Ga} \defined \w_0 = h^{3N} \]
\subsubsection{Simple 1D Classical Harmonic Oscillator}
Recall that for a 1D classical harmonic oscillator,
\[ H\br{q,p} = \f{kq^2}{2} + \f{p^2}{2m} = E\]
Where $U = \f{kq^2}{2}$ is the potential energy $V$ of the system, $\f{p^2}{2m}$ is the kinetic energy $T$ of the system and $E$ is the total energy of the system which is a constant of motion. For simplicity, we will use the natural angular frequency $\w'$ of the system as $m \br{\w'}^2 = k$. Note that this $\w'$ with the prime is different from the phase space volume introduced in \eqref{eq:omega}. This can be a source of confusion, just be mindful of the different quantities. Therefore the phase space volume for a simple harmonic oscillator is just the \textit{area} of an ellipse ($A = \pi ab$). The equation of the ellipse is given by,
\[ \br{\f{q^2}{2E/m\w'^2} + \f{p^2}{2mE}} = 1 \]
The solutions to the equations of motion are given by,
\[ q(t) = A \cos \br{\w' t + \phi} \quad p(t) = - A \sin \br{\w' t + \phi} \]
With area given by,
\[A = \pi \sqrt{2E/m\w'^2} \sqrt{2mE} = \f{2\pi E}{\w'}\]
Therefore subject to the constraints $E - \De/2 \leq H\br{q,p} \leq E + \De/2$,
\[ \w = \int \dif q \dif p = A_{E + \De/2} - A_{E - \De/2}\]

\begin{center}
\begin{tikzpicture}[
    scale=1.0,
    decoration={post length=1mm, pre length=1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
        % Draw axes
        \coordinate (nya) at (0, -2.5);
        \coordinate (pya) at (0, 2.5);
        \coordinate (nxa) at (-2.5, 0);
        \coordinate (pxa) at (2.5, 0);
        \node [above] (lpya) at (pya) {$p$};
        \node [right] (lpxa) at (pxa) {$q$};
        \draw [<->, thick] (nya) -- (pya);
        \draw [<->, thick] (nxa) -- (pxa);
        \draw [thick, blue] (0,0) circle [x radius=2, y radius=1.5];
        \draw [dashed,thick, red] (0,0) circle [x radius=1.9, y radius=1.4];
        \node [above right, red] (Elabel) at (1.9,0) {$E$};
        \draw [thick, blue] (0,0) circle [x radius=1.8, y radius=1.3];
        \node [above right, blue] (Elabel) at (1.8,0.5) {$\De$};
\end{tikzpicture}
\end{center}

We are interested in the area between the outer and inner radii. Therefore,
\[ \w = \f{2\pi \br{E + \f{\De}{2}}}{\w'} - \f{2\pi \br{E - \f{\De}{2}}}{\w'} = \f{2\pi \De}{\w'} \]
Noticing that the phase space volume (PSV) contains the characteristic frequency $2\pi/\w'$. \\

Now suppose we are interested in determining the ``elementary PSV'' $\w_0$. Taking as an initial guess from the quantum mechanical simple harmonic oscillator,
\[ E_n = \hbar \w' \br{n + \f12} \implies \De E = E_{n+1} - E_n = \hbar \w' \]

Therefore the elementary phase space volume becomes,
\[ \br{\text{PSV}}\tsb{elementary} = \w_0 = \f{2\pi}{\w'} \hbar \w' = 2 \pi \hbar = h \]

This motivates the elementary phase space volume to be roughly $h$. Another approach to determining the elementary phase space volume is to use the notion that $E \gg De \gg \hbar \w'$. Therefore the number of eigenstates that would fit into $\De$ would be given by, $\# \sim \De / \hbar \w'$. Therefore,
\[ \br{\text{PSV}}\tsb{elementary} \sim  \f{\f{2\pi}{\w'}\De}{\#} = \f{\f{2\pi}{\w'}\De}{\f{\De}{\hbar \w'}} = 2 \pi \hbar \sim h \]
Or alternatively, one can motivate this irreducible unit of phase space volume $h$ by using the uncertainty principle $\De x \De p \sim \hbar$.

\subsubsection{Reflection}
Both of the above examples illustrate that computation of phase space volumes are a pain in the neck. We should be looking to \textit{replace} it with something else. If we have the relation $S = k_B \ln \br{\Ga}$ which gives,
\[ S = k_B \ln \br{\Ga} = k_B \ln \br{\f{\w}{\w_0}} = k_B \ln \br{\w} - \untext{k_B \ln \br{\w_0}}{const.}\]
The pain of computing $\w_0$ can be ignored or neglected because most thermodynamical variables are related through derivatives of $S$. In these cases, the constant contribution has zero derivative so it can be skipped.

\section{Canonical Ensemble}
\subsection{Preamble}
Up until now we have considered the rather restricted or simple case of systems with fixed $N,V,E$. This is a natural choice for a first exploration, as it involves isolated systems. Computing microstates and phase space volume under these restrictions are known as the \textbf{microcanonical ensemble}. \\

There are two comments to be made with regards to the microcanonical ensemble. The first being that $E$ is rarely perfectly fixed, and is rarely known exactly experimentally. The means performing an analysis with fixed energy $E$ is somewhat non-physical. Secondly, no system is perfectly isolated; keeping $E$ fixed in a real system is also artificial and difficult. Doing this would require a measurement device that measures energy directly, and makes adjustments to the system that an experimenter can control (like $V, T, N$) in order to ensure $E$ is fixed. \\

An even easier way of controlling the system, is to fix the temperature. This modifies the ensemble to be restricted by $N, V, T$. This is known as the \textbf{canonical ensemble}. As one would expect, other ``ensembles'' can be constructed by fixed other combinations of quantities of the system.

\subsection{Tackling the Canonical Ensemble}
In any ensemble problem the goal is to determine the structure of $\rho\br{q,p}$. Typically, we should expect $\rho$ to have the form,
\[ \rho\br{q,p} \propto e^{-H\br{q,p}/k_BT} \]
Where $H\br{q,p}$ is the Hamiltonian or the total energy of the system.
\begin{center}
\begin{tikzpicture}
    \filldraw[fill=lightgray, draw=black] (-1,-1) rectangle (1,1);
    \draw[dashed, red] (-2,-2) rectangle (2,2);
    \node[above, red] (lobject) at (0,2) {Object};
    \node (lreservoir) at (0,-1.5) {Reservoir};
    \node (lsystem) at (0,0) {System};
\end{tikzpicture}
\end{center}

We will denote the energy of the reservoir $E^{\br{r}}$ and the energy of the system $E^{\br{s}}$. The total energy of the object will be constant at $E = E^{\br{r}} + E^{\br{s}}$ where typically $E^{\br{r}} \gg E^{\br{s}}$. We want to define some microstates for the reservoir and system. Typically one would expect that the reservoir has many more microstates. Let the probability $P_j$ that the probability that the system is in a particular microstate $j$. The probability $P_j$ is proportional to the number of sates in the reservoir that are consistent with the system being in the microstate $j$. The reason for this is that since the whole object is isolated, all the microstates between the system and reservoir are equally probable. \\

If the system is in state $j$ then the number of microstates available to the reservoir is constrained by the fact that the total energy $E = E^{\br{r}} + E^{\br{s}}$ is constant. The system with microstate $j$ is said to have energy $E^{\br{s}} = E_j$. If the system changes microstate and it's own energy changes by an amount $\De E$, then the reservoir must change energy by an amount $-\De E$. Where $\De E$ is the change in energy of the system when transiting from microstate $j$ to microstate $i$. Therefore,
\[ \De E = E_i - E_j \]

We can then ask the question: What is the ratio of probability that the system is in microstate $j$ vs. microstate $i$.
\[ \f{P_i}{P_j} = \f{\Om^{(s)} \times \Om^{(r)}\br{E^{(r)} - \De E}}{\Om^{(s)} \times \Om^{(r)}\br{E^{(r)}}} \]
But since we are considering the system to be in a particular microstate $\Om^{(s)} = 1$,
\[ \f{P_i}{P_j} = \f{\Om^{(r)}\br{E^{(r)} - \De E}}{\Om^{(r)}\br{E^{(r)}}} \eq \label{eq:ohmohm}\]
Evidently, we can related the number of microstates available to the reservoir to the entropy of the reservoir using $S^{(r)} = k_B \ln \Om^{(r)}$. Therefore we can rewrite \eqref{eq:ohmohm} using logarithms,
\[ \f{P_i}{P_j} = \exp\bc{\ln \br{\Om^{(r)}\br{E^{(r)} - \De E}} - \ln \br{\Om^{(r)}\br{E^{(r)}}}} \]
Which can be written,
\[ \f{P_i}{P_j} = \exp\bc{\f{S^{(r)}\br{E^{(r)} - \De E} - S^{(r)}\br{E^{(r)}}}{k_B}} \]
However since $E^{(r)} \gg E^{(s)}$, then $E^{(r)} \gg \De E^{(s)}$. A Taylor series expansion gives,
\[ S^{(r)}\br{E^{(r)} - \De E} - S^{(r)}\br{E^{(r)}} = - \De E \pder{S^{(r)}}{E^{(r)}} \]
But thermodynamically,
\[ \f{1}{T^{(r)}} = \pder{S^{(r)}}{E^{(r)}}\]
Therefore we have the familiar relation (using the fact that in equilibrium $T = T^{(r)} = T^{(s)}$),
\[ \f{P_i}{P_j} = \exp\bc{-\f{\De E}{k_B T}} \]
Or for individual probabilities,
\[ P_i \propto \exp\bc{-\f{E_i}{k_B T}} \eq \label{eq:eoverkbt}\]
The proportionality sign `$\propto$' is present because this relationship is still missing in general a normalization factor.
\subsubsection{Example}
Let us perform an example. Consider a system where we have a single spin $1/2$ particle in a magnetic field $\vB$. The interaction energy is,
\[ H = - \vec{\mu} \cdot \vB \]
Where $\vec{\mu}= - g \mu_B \vS$. Taking the magnetic field to point in one direction $\vB = B \hz$,
\[ H = g \mu_B B_z S_z \]
Where $S_z$ can either be $+ \f12$ or $-\f12$. Consider $E_e$ corresponding to $S_z = +\f12$ and $E_0$ corresponding to $S_z = - \f12$. Then we have,
\[ P_e \propto e^{-E_e / k_B T} \qquad P_0 \propto e^{-E_0 / k_B T} \]
Since $P_e + P_0 = 1$, we get,
\[ P_e = \f{P_e}{P_e + P_0} = \f{e^{-E_e / k_B T}}{e^{-E_e / k_B T} + e^{-E_0 / k_B T}} \]
In general for appropriate choice of $\be \De$,
\[ P_e = \f{e^{-\be \De}}{1 + e^{-\be \De}} \qquad P_0 = \f{1}{1 + e^{-\be \De}} \]

\begin{center}
\begin{tikzpicture}[
    scale=1.0,
    decoration={post length=1mm, pre length=1mm, markings, mark=at position 0.5 with {\arrow{>}}}
]
        % Draw axes
        \coordinate (nya) at (0, 0);
        \coordinate (pya) at (0, 4.2);
        \coordinate (nxa) at (0, 0);
        \coordinate (pxa) at (8, 0);
        \node [above] (lpya) at (pya) {$P$};
        \node [right] (lpxa) at (pxa) {$k_BT/\De$};
        \draw [->, thick] (nya) -- (pya);
        \draw [->, thick] (nxa) -- (pxa);
        \draw [dashed, gray] (0,2) -- (8,2);
        \node [left] (lhalf) at (0,2) {$\f{1}{2}$};
        \node [below, red] (lpz) at (2,1.5) {$P_0$};
        \node [above, blue] (lpe) at (2,2.5) {$P_e$};
        \draw[domain=0.001:8,smooth,variable=\x,red]  plot ({\x},{4*(e^(-(1/\x)))/(1+e^(-(1/\x)))});
        \draw[domain=0.001:8,smooth,variable=\x,blue]  plot ({\x},{4*(1)/(1+e^(-(1/\x)))});
\end{tikzpicture}
\end{center}

\subsection{Canonical Ensemble Description}

Consider an ensemble of identical systems $1,2,3,\ldots,\ti{N}$ that share a total energy $\ti{E}$. We will label each system with an index $r$ where each system $r$ has energy $E_r$. We will call $n_r$ the number of the systems in the ensemble that happen to have the same energy $E_r$. The two restrictions can be written,
\[ \sum_{r} n_r = \ti{N} \quad \sum_{r} n_r E_r = \ti{E} \]
And set of $\bc{n_r}$ that satisfies both of these two equations represents a possible ``mate''/``type'' of distribution of the ``systems'' in the ensemble. Each of these solution $\bc{n_r}$ can be realized in many different ways through permutations of $\bc{n_r}$,
\[ W\br{\bc{n_r}} = \f{\ti{N}!}{n_1!n_2!\cdots n_r!}\]

We aim to find the most probable distribution $W\br{\bc{n_r}}$. We can do so by extremizing $\ln W$ with respect to $\br{n_r}$.

\begin{align*}
    \ln W &= \ln\br{\ti{N}!} - \ln\br{\prod_{r}n_r!} \\
    &= \ln\br{\ti{N}!} - \sum_{r}\ln\br{n_r!} \note{Logarithm identities} \\
    &= \ti{N}\ln\br{\ti{N}} - \ti{N} - \sum_{r}\bs{n_r\ln\br{n_r} - n_r} \note{Stirling's approximation}\\
    &= \ti{N}\ln\br{\ti{N}} + \underbrace{\sum_{r} n_r - \ti{N}}_{=0} - \sum_{r}n_r\ln\br{n_r} \\
    &= \ti{N}\ln\br{\ti{N}} - \sum_{r}n_r\ln\br{n_r}
\end{align*}
Therefore varying $\ln W$ with respect to $n_r$,
\[ \de\ln W = \de\bc{\ti{N}\ln\br{\ti{N}} - \sum_{r}n_r\ln\br{n_r}} = -\sum_{r}\br{\de n_r \ln n_r + n_r \f{1}{n_r} \de n_r} \note{Product rule}\]
Therefore,
\[ \de\ln W = -\sum_{r}\br{\ln n_r + 1}\de n_r \eq \label{eq:sumzerorestr}\]
Setting this equal to zero $0 = \de \ln W$ gives and the two modified constraints $\sum_r \de n_r = 0$ and $\sum_r \de n_r \cdot E_r = 0$, use \textit{Lagrange multipliers} to solve this equation. For each constraint we subscribe two multipliers,
\[ \al \ar \sum_r \de n_r = 0 \qquad \be \ar \sum_r \de n_r \cdot E_r= 0 \]
Therefore \ref{eq:sumzerorestr} becomes,
\[ \sum_r \bc{\br{-\ln n_r - 1} - \al - \be E_r} \de n_r = 0 \]
In order to solve this equation, we need for all $\de n_r$,
\[ \br{-\ln n_r - 1} - \al - \be E_r = 0 \]
After rearrangement,
\[ \ln n_r = -\br{1+\al} - \be E_r \]
Or that the distribution of $n_r$ must be,
\[ n_r = e^{-\br{1+\al}}e^{-\be E_r} \]
Thus extremizing $W$ with respect to $\bc{n_r}$ gives,
\[ n_r \propto e^{-\be E_r} \]
What remains is to show $\be = 1/k_B T$. The derivation that lead to \eqref{eq:eoverkbt} suggests that $\be = 1 / k_B T$. Considering the probability that the system is,
\[ P \br{n_r} \propto e^{-\be E_r} \]
Normalizing the total probability to be unity gives the value of $\al$,
\[ P \br{n_r} = \f{e^{-\be E_r}}{\sum_ie^{-\be E_i}} \eq \label{eq:overpartition} \]
\subsection{Physical Significance of Various Statistical Variables in the Canonical Ensemble}
First we shall ask what is the average energy $E$. Recall that here $E$ is not necessarily fixed. To follow notation from the textbook, we will relabel the average energy of the system $U$. Then,
\[ U = \ba{E_r} = \sum_r E_r P\br{n_r} \]
In terms of notation, it makes no difference to write $P\br{n_r}$ or $P\br{E_r}$ because for each $n_r$ there is a corresponding $E_r$.
\[ U = \ba{E_r} = \sum_r E_r P\br{E_r} \]
Therefore using \eqref{eq:overpartition},
\[ U = \f{\sum_r E_r e^{-\be E_r}}{\sum_r e^{-\be E_r}} \eq \label{eq:avginternalenergy} \]
The denominator here is known as the \textbf{canonical partition function}.
\[ Z \defined \sum_r e^{-\be E_r} \]
\textbf{Example}: Suppose we are dealing with a spin system with Hamiltonian given by $\hat{H} = - D \hat{S}_z^2$ with $S = 1$ where $D$ is a difference in energy levels. Then the time independent Schrdinger equation yields,
\[ \hat{H} \ket{n} = E \ket{n} \]
With matrix for $\hat{S}_z$,
\[ \hat{S}_z = \mtrx{1 & 0 & 0 \\ 0 & 0 & 0  \\ 0 & 0 & -1 } \]
This gives energy levels,
\[ E = \begin{cases}
    -D & \br{S_z = + 1} \\
    -D & \br{S_z = - 1} \\
    0 & \br{S_z = 0} \\
\end{cases} \]
Using this spectrum of energies, all that remains is to plug these states into \eqref{eq:avginternalenergy} to get,
\[ \sum_r e^{-\be E_r} = e^{\be D} + e^{\be D} + e^{0} = 2e^{\be D} + 1 \]
Therefore the average internal energy is given by,
\[ U = \f{\sum_r E_r e^{-\be E_r}}{\sum_r e^{-\be E_r}} = \f{-De^{\be D} - De^{\be D} + 0e^{0}}{2e^{\be D} + 1} = \f{-2De^{\be D}}{2e^{\be D} + 1}\]
Using this we can determine the specific heat of the system,
\[ c_V = \pdert{U}{T}{V} \quad c_N = \pder{U}{T} \]
Specific heat is important because these are quantities that are measurable or tractable to experimentalists. Using chain rule,
\[ \pder{U}{T} = \pder{\be}{T} \cdot \pder{U}{\be} \]
Where $\be = 1/k_B T \implies \pder{\be}{T} = - 1 / k_B T^2$.

\subsubsection{Logarithm of Partition Function}

One can convince themselves that,
\[ \ba{U} = -\pder{\ln Z}{\be} \]
Namely the proof is as follows,
\[ \ln Z = \ln\br{\sum_r e^{-\be E_r}} \implies \pder{\ln Z}{\be} = -\f{\sum_r E_r e^{-\be E_r}}{\sum_r e^{-\be E_r}} = -\ba{U} \]
This means that all that needs to be calculated is $Z$ and the remaining thermodynamical parameters of the system are generated by derivatives of $\ln Z$.

\subsubsection{Helmholtz Free Energy}

The Helmholtz free energy $A$ (sometimes $F$) is defined to be,
\[ A = U - TS \eq \label{eq:definitionofhelmholtzfreeenergy}\]
Using the first law (with added chemical change),
\[ \dif U = T \dif S - P \dif V + \mu \dif N \]
Since we have $\dif A = \dif U - T \dif S - S \dif T$, it is a function of state like $S$ or $U$. Combining these two terms yields,
\[ \dif A = - \cancel{T \dif S} - S \dif T + \cancel{T \dif S} - P \dif V + \mu \dif N\]
\[ \dif A = - S \dif T - P \dif V + \mu \dif N\]
This suggests the following relations,
\[ P = -\pdert{A}{V}{N,T} \quad S = -\pdert{A}{T}{N,V} \quad \mu = \pdert{A}{N}{V,T} \eq \label{eq:Agenerator} \]
$A$ is very useful as its derivatives generate quantities of interest. From \eqref{eq:definitionofhelmholtzfreeenergy} we obtain,
\[ U = A + TS = A - T \pder{A}{T}{N,V} = - T^2 \bc{\pder{}{T} \br{\f{A}{T}}}_{N,V} \]
Furthermore using this equation, it can more suggestively be written as,
\[ U = \pder{\br{A/T}}{\br{1/T}} \]
Moreover since $\be = 1/ k_B T$ one can derive,
\[ \ln \br{\sum_r e^{-\be E_r}} = - \f{A}{k_B T} \]
This analysis leads to the postulate that from $A = - k_B T \ln Z$ the rest of thermodynamics follows.
\[ A = - k_B T \ln Z \implies \text{Thermodynamics} \]
If one can compute the partition function $Z$ for a given system, this relationship gives $A$ and then using \eqref{eq:Agenerator} we can obtain quantities of $P, S$ and $\mu$ and indirectly $T, U$ etc.
So where and how does $Z$ depend on $V,T,N$? We will need to know this to explore quantities \eqref{eq:Agenerator}. Dependence on temperature $T$ is directly related to the definition of $Z$ and $\be = 1/ k_B T$. The dependence on $V$ is determined by how the energy depends on the volume of the system. Recall from a 1d particle in a box $V = L^3$,
\[ \varepsilon = \f{\hbar^2}{2 m} \f{\pi^2}{L^2} n^2 = \f{\hbar^2}{2 m} \f{\pi^2}{V^{2/3}} n^2 \]
Or that $\varepsilon \propto V^{-2/3}$.

\subsection{Summary of Z}
From this, we have learned that if we can compute the partition function $Z$ and take derivatives, \textbf{all physical quantities} can be computed. To obtain the the partition there are a few steps.
\begin{enumerate}
    \item Determine the Hamiltonian $H$ of the system (active areas of research)
    \item Solve the quantum mechanical system $H \psi_n = E_n \psi_n$ (in principle, only $E_n$ is needed and not $\psi_n$)
    \item Compute the sum $\sum_n e^{-\be E_n}$ (computationally very expensive)
\end{enumerate}
In practice, the first step is very difficult for newly discovered systems. Furthermore, computing the third step precisely using a naive brute force approach is nearly impossible. We need to develop smarter ways to obtain the partition function $Z$.

\subsection{Alternative Form for Z \& Degeneracy of States}
\label{sec:densityofstates}
In any notation the partition function is just a \textit{sum over states},
\[ Z = \sum_n e^{-\be E_n} = \sum_r e^{-\be E_r} = \sum_a e^{-\be E_a} \eq \label{eq:sumoverstates}\]
Let us apply this to the hydrogen atom. The energy levels for the hydrogen atoms is given by,
\[ E_n = -\f{\SI{13.6}{\eV}}{n^2} \]
Where $n$ is the principle quantum number. Notice that this $n$ comes from a quantum mechanical consideration of the boundaries conditions for $H \psi_n = E_n \psi_n$ where $\psi = \psi_{nlm}\br{r,\te,\phi}$ is indexed by many number $n,l,m$. Writing the states $\ket{nlm}$, the first ground state is written $\ket{1,0,0}$. For $n = 2$, there are four states $\ket{2,0,0},\ket{2,1,0},\ket{2,1,-1},\ket{2,1,1}$. There are many states that share the same energy $E_n$ as $n \ar \inf$. Therefore we can compute the partition function,
\[ Z = e^{-\be \br{-\SI{13.6}{\eV}}} + e^{-\be \br{-\SI{13.6}{\eV}/2^2}} + e^{-\be \br{-\SI{13.6}{\eV}/2^2}} + \cdots \]
We can combine the many repeated sums over the same energy level,
\[ Z = e^{-\be \br{-\SI{13.6}{\eV}}} + 4e^{-\be \br{-\SI{13.6}{\eV}/2^2}} + \cdots\]
Or using the term \textbf{degeneracy of state n}, $g_n$,
\[ Z = \sum_n g_n e^{-\be E_n} \eq \label{eq:sumoverpotentialenergies} \]
It is very important to note that \eqref{eq:sumoverpotentialenergies} is different from \eqref{eq:sumoverstates}. \eqref{eq:sumoverpotentialenergies} sums over the distinct energies of the system, while \eqref{eq:sumoverstates} sums over the states of the system. Another sloppy notation that is invoked is,
\[ Z = \sum_\vep g\br{\vep} e^{-\be \vep} \]
Labels the energy levels not by indices but by the actual energies $\vep$ themselves. This is permitted because $\vep$ are distinct. If the spacing between the energy levels $\vep, \vep + \De \vep$ is small enough, we reduce to an integral,
\[ Z = \int_\vep g\br{\vep} e^{-\be \vep} \dif \vep \]
In this case $g\br{\vep}$ is counting how many states are there between $\vep$ and $\vep + \De \vep$. In principle, the bounds for $\vep$ are from $0$ to infinity and $Z$ is a function of temperature $T$; therefore,
\[ Z\br{T} = \intl_0^\inf g\br{\vep} e^{-\be \vep} \dif \vep \]
Note that in the Pathria textbook, the notation used for this quantity is $Q_N\br{T}$ but we will use the more standard $Z_n\br{T}$. We can recover in general a discrete case by having $g\br{E} = \de\br{E-E_0} + \de\br{E-E_1} + \cdots$ (where $\de$ is the Dirac-Delta function). This gives,
\[ Z\br{T} = \intl_0^\inf \br{\de\br{E-E_0} + \de\br{E-E_1} + \cdots} e^{-\be \vep} \dif \vep \]
\[ Z\br{T} = e^{-\be E_0} + e^{-\be E_1} + \cdots \]
This function $g\br{\vep}$ is sometimes called the \textbf{density of states}. Using Dirac-Deltas $g\br{\vep}$ can be applied to continuous and discrete distributions.
\subsection{Classical Systems / Free Particles}
Recall from the Microcanonical ensemble \eqref{eq:indepensembleavg} gives the average value for a physical quantity $f$. Using $\rho = \exp\br{-\be H\br{p,q}}$,
\[ \ba{f} = \f{\int_{\w} f\br{p,q}e^{-\be H\br{p,q}} \dif^{3N} q \dif^{3N} p}{\int_{\w} e^{-\be H\br{p,q}} \dif^{3N} q \dif^{3N} p}
 \]
Where with proper scaling factor $h^{3N}$ and the Gibb's Paradox correct $N!$,
\[ \dif \w =  \f{\dif^{3N} q \dif^{3N} p}{h^{3N} N!} \]
Note that in this course we typically choose our measurable quantity $f$ to be position, energy, momentum, etc. However in principle, we can take $f$ to be \textbf{any} measurable quantity.\\

\textbf{Example}: As an example, let us look at the scenario of free classical particles with Hamiltonian given (most of the time) by,
\[ H\br{p,q} = \text{kinetic energy} + \text{potential energy} \]
More explicitly,
\[ T = \sum_{i=1}^{N} \f{\abs{\vp_i}^2}{2m} \qquad V = \sum_{i > j} V_{ij} \br{r_{ij}}\]
Note that is \textit{not} always possible to write potential energies are functions of the distance between particles $r_{ij}$ directly. Nonetheless it is valid for numerous examples. As an example for particles in magnetic field, with $\div \vB = \vec0$ we can write $\vB$ as the curl of a vector potential,
\[ \vB = \curl \vA \]
Using this vector potential, the Hamiltonian can be written,
\[ H = \f{\abs{\vp - e \vA}^2}{2m} \]
This is a simple example where $T$ and $V$ are coupled together. Moving forward the distance between two distinct particles can be written,
\[ \abs{\vr_{ij}}^2 = \abs{\vr_i - \vr_j}^2 \]
Where $\vr_i$'s are the generalized position components in the Hamiltonian formalism,
\[ \vr_i = \br{r_{ix},r_{iy},r_{iz}} = \br{q_{ix}, q_{iy}, q_{iz}} \]
Computing the partition function,
\[ Z = \int_{\w} e^{-\be H} \dif \w = \int_{\w} e^{-\be H} \f{\dif^{3N} q \dif^{3N} p}{h^{3N} N!} \]
Subbing in the general Hamiltonian,
\[ Z = \int_{\w} e^{-\be\bc{\sum_{i=1}^{N} \f{\abs{\vp_i}^2}{2m}} - \be \bc{\sum_{i > j} V_{ij} \br{r_{i},r_{j}}}} \f{\dif^{3N} q \dif^{3N} p}{h^{3N} N!} \]
Decoupling the momentum and position terms (for cases unlike the above where the potential energy \textbf{does not} depend on momentum),
\[ Z = \int_{\w_p} e^{-\be\bc{\sum_{i=1}^{N} \f{\abs{\vp_i}^2}{2m}}} \f{\dif^{3N} p}{h^{3N} N!} \times \int_{\w_q} e^{- \be V \br{q}} \dif^{3N} q \]
Here we have written $\sum_{i=1}^{N} V_{ij}\br{r_{i},r_{j}} = V\br{q}$. The second component is often very intractable. The first term can be decomposed more,
\[ \int_{\w_p} e^{-\be\bc{\sum_{i=1}^{N} \f{\abs{\vp_i}^2}{2m}}} \f{\dif^{3N} p}{h^{3N} N!} = \int_{\w_p} e^{-\be\br{\f{p_{1,x}^2}{2m} + \cdots + \f{p_{2,y}^2}{2m} + \cdots}} \f{\dif^{3N} p}{h^{3N} N!} = \bs{\intl_{-\inf}^{\inf} e^{-\be\f{p_{\al}^2}{2m}} \dif p_\al}^{3N} \f{1}{h^{3N} N!} \]
We have seen the integral before in \eqref{sec:gaussianintegrals},
\[ \intl_{-\inf}^{\inf} e^{-a x^2}\dif x = \sqrt{\f{\pi}{a}} \]
Therefore taking the potential energy $V$ be zero (this can be validated if $k_B T \gg V$ which makes the interaction energy negligible compared to the thermal energy),
\[ Z_N = \br{\br{2m \pi k T}^{1/2}}^{3N}\f{1}{h^{3N} N!} \times \underbrace{\int_{w_q}e^{0}\dif^{3N} q}_{V^N} \]
Where $V^N$ is the physical volume to the power of $N$ and it \textbf{not} the potential energy. This is bad notation. More suggestively written gives,
\[ Z_N = \f{1}{N!}\bc{\f{V}{h^3} \br{2m \pi k T}^{3/2}}^{N} \]
From this we can look at the Helmholtz free energy,
\[ A = - k_B T \ln Z_N \]
And one can recover,
\[ PV = N k_B T \]
As well as the Sackur-Tetrode equation is $S$ is computed.\\

What about a combination of $N_1$ particles with mass $m_1$ and $N_2$ particles with mass $m_2$ where $m_2 \neq m_1$. This derivation is modified in such a way that,
\[ \iint e^{-ax^2 - by^2}\dx\dy = \int e^{-ax^2}\dx\int e^{-ay^2}\dy = \sqrt{\f{\pi}{a}}\sqrt{\f{\pi}{b}}\]
The partition function of two groups noninteracting particles is multiplicative.
\[ Z = Z_1 \times Z_2 \]
Since the entropy is characterized by the logarithm of the partition functions, there corresponding entropies are additive.\\

\textbf{Remark:} The notation of $\dif^{3N} p$ can tend to be confusing. Expanding this out,
\[ \dif^{3N} p = \dif p_1 \dif p_2 \cdots \dif p_{3N}  \]
Here there are $3N$ terms to consider and often leads to $3N$ integrals. The $p_i$ in this context indicates a particular component of a particular particle. For example, $p_i$ could be the $y$ component of the $53$rd particle. Alternatively, when $p_i$'s are written in this way,
\[ H = \sum_{i=1}^N \f{\abs{\vp_i}^2}{2m} = \f{p_{1,x}^2 + p_{1,y}^2 + p_{1,z}^2}{2m} + \cdots + \f{p_{N,x}^2 + p_{N,y}^2 + p_{N,z}^2}{2m} \]
Each $\vp_i$ represents the total momentum vector of the particle. Be careful of this distinction. It should be evident from the context of the problem. \\

For example, calculating the canonical partition function is in general written,
\[ Z = \f{1}{h^{3N} N!} \int \dif^{3N}q\int \dif^{3N}p e^{-\be H\br{q,p}} \]
But it is common that $H\br{p,q} = H\br{p}$ whenever the potential energy is zero $V = 0$. Then,
\[ Z = \f{V^{N}}{h^{3N} N!} \int \dif^{3N}p e^{-\be \br{\f{p_1^2}{2m} + \f{p_2^2}{2m} + \cdots + \f{p_{3N}^2}{2m}}} \]
\[ Z = \f{V^{N}}{h^{3N} N!} \br{2m\pi k_B T}^{3N/2} \]

\subsection{Examples of C.E.}

\subsubsection{Two Masses on Springs}
Consider a system of two masses $m_1$ and $m_2$ connected by springs to each other and a wall along a linear 1d line. Therefore each mass can only move in along the $x$-axis. If the spring constants are $c_1$, $c_2$ and $k$ with $k$ being between the two masses with $x_1$ being the position of the first mass and $x_2$ being the position of the second mass.

\[ H = \br{\f{p_1^2}{2m_1} + \f{p_2^2}{2m_2}} + \br{c_1 \f{x_1^2}{2} + c_2 \f{x_2^2}{2} + k \f{\br{x_2 - x_1}^2}{2}} \]

Therefore the canonical partition function can be computed,
\[ Z = \f{1}{h^2 2!} \iint \dif p_1 \dif p_2 e^{-\be\br{\f{p_1^2}{2m_1} + \f{p_2^2}{2m_2}}} \times \iint \dif x_1 \dif x_2 e^{-\be \bc{c_1 \f{x_1^2}{2} + c_2 \f{x_2^2}{2} + k \f{\br{x_2 - x_1}^2}{2}}} \]

The kinetic energy contribution is a Gaussian integral (see section \ref{sec:gaussianintegrals}) as seen many times before,
\[ Z = \br{2\pi m_1 k_B T}^{1/2}\br{2\pi m_2 k_B T}^{1/2} \times \iint \dif x_1 \dif x_2 e^{-\be \bc{c_1 \f{x_1^2}{2} + c_2 \f{x_2^2}{2} + k \f{\br{x_2 - x_1}^2}{2}}} \]
And the position contribution is also a Gaussian integral, just slightly modified. The solution is left as an exercise to the reader. \\

\textbf{Remark:} As can be seen in this two examples, even simple problems can lead to complicated integrals. \\

\subsubsection{Rigid Polar Rotator} Consider a dilute gas of $\text{H}_2\text{C}$ or $\text{CO}$ with electric dipole moment $\vec{\mu}$ in an electric field $\vE$. What is the partition function $Z\br{T}$? One would suspect that is \textit{should} on the general form,
\[ Z = \idotsint_{\w} e^{-\be H\br{\bc{q,p}}} \bs{ \cdots } \]
What is the Hamiltonian? Since the gas is \textit{dilute}, it can be approximated that there are no interactions between the particles. As such, the partition function and Hamiltonian can be determined for a single particle $i$ and then combined together appropriately,
\[ Z = \f{\br{Z_i}^N}{N!} \qquad H = \sum_{i=1}^{n} H_i \]
We will hence forth consider looking at a single particle,
\[ H_i \br{p,q} = \f{\abs{\vp}^2}{2m} - \vec{\mu} \cdot \vE \]
Since $V = - \vec{\mu} \cdot \vE$. For convenience let use take the electric field to be pointing in the $\hat{z}$ direction such that $\vE = E \hz$. Therefore,
\[ H_i \br{p,q} = \f{\abs{\vp}^2}{2m} - \mu E \cos \te \]
Where $\te$ is the angle between $\mu$ and the $+\hz$ axis. The partition function for an individual molecule is given by,
\[ Z_i = \f{1}{h^5}\int e^{-\be H\br{q,p}} \dif q \dif p \]
Taking the generalized position $q$ to be $\te$ (as is common in many textbooks),
\[ Z_i \stackrel{?}{=} \f{1}{h^5}\int e^{-\be H\br{\te,p}} \dif \te \dif p \dif p_\te \]
Notice the $\dif p_\te$ \textbf{needs} to be there because for every generalized coordinate has corresponding momentum $p_\te$. More properly there is dependence on many generalized coordinates. The position of the molecule is given by $x,y,z$ and the generalized linear momentum $p_x, p_y, p_z$. Also the orientation of the molecule is characterized by $\te, \phi$ and generalized angular momentum $p_\te, p_\phi$. There are 10 coordinates total that the Hamiltonian depends which makes the partition function,
\[ Z_i = \underbrace{\iiiint}_{4} \dif p_\te \dif p_\phi \dif \te \dif \phi \underbrace{\idotsint}_{6} \dif x \dif y \dif z \dif p_x \dif p_y \dif p_z e^{-\be H \br{\te, \phi, p_\te, p_\phi, x, y, z, p_x, p_y, p_z}} \]
Note, this integral is not decoupled. It is just written this way to illustrate the separate dependence on each of the sets of terms. The Hamiltonian has linear components and rotational components characterized by,
\[ H = \f{\abs{\vp}^2}{2m} - \mu E \cos \te + H\tsb{rot} \]
Noting that is still for a single molecule. The rotation terms are given by,
\[ H\tsb{rot} = \f{m}{2}\br{\underbrace{\ell \dot{\te}}_{v_\te}}^2 + \f{m}{2} \br{\underbrace{\ell \sin \te \dot{\phi}}_{v_\phi}}^2 \]
As an exercise, make sure these quantities have units of energy. An alternative way of writing this,
\[ H\tsb{rot} = \f{p_\te^2}{2m \ell^2} + \f{p_\phi^2}{2m \ell^2 \sin^2 \te} \]
The total Hamiltonian can be written (still for a single molecule),
\[ H = \f{1}{2m}\br{p_x^2 + p_y^2 + p_z^2} + \bc{\f{p_\te^2}{2m \ell^2} + \f{p_\phi^2}{2m \ell^2 \sin^2 \te}} - \mu E \cos \te \]
As such for a single molecule integration over $ $,
\[ Z = \f{1}{h^5}\br{2\pi m k_B T}^{3/2} V \times \bc{\cdots} \]
The other six integrals are given by,
\[ \int_{-\inf}^{\inf} e^{-\f{\be p_\te^2}{2mr^2}} \dif p_\te = \br{2m\pi r^2 k_B T}^{1/2} \]
\[ \int_{-\inf}^{\inf} e^{-\f{\be p_\phi^2}{2mr^2\sin^2 \te}}  \dif p_\phi = \br{2m\pi r^2 k_B T \sin^2 \te}^{1/2} \]
Note that the generated $\sin \te$ term is important here.
\[ \int_{0}^{2\pi} \dif \phi = 2\pi \]
\[ \int_{0}^{\pi} \dif \te \sin \te e^{-\be\br{-\mu E \cos \te}} = ? \]
The canonical partition function is reduced to,
\[ Z = \f{1}{h} V \br{2\pi m k_B T}^{3/2} \times \bs{\br{2m \pi r^2 k_B T}^{1/2}}^2 2 \pi \int_{0}^{\pi} \sin \te e^{\be \mu E \cos \te} \dif \te \]
The last integral can be determined using the substitution $x = \cos \te$ and $\dif x = - \sin \te \dif \te$,
\[ \int_{0}^{\pi} \sin \te e^{\be \mu E \cos \te} \dif \te = \int_{-1}^{1} e^{\be \mu E x} \dif x = \f{1}{\be \mu E} \sinh \br{\be \mu E} \]
Thus,
\[ Z = Z\tsb{trans} \times Z\tsb{rot} \times  \f{1}{\be \mu E} \sinh \br{\be \mu E} \]
Now that $Z$ in known, we can compute all thermodynamic properties of the system by taking partial derivatives of $Z$.\\

Instead of doing this, let us determine the average polarization of the gas in the $z$ direction $\ba{\mu^z}$.
\[ \ba{\mu^z} = \ba{\mu \cos \te} = \f{\int_{0}^{2\pi} \dif \phi \int_{0}^{\pi} \mu\sin \te  \cos \te e^{\be \mu E \cos \te} \dif \te}{\int_{0}^{2\pi} \dif \phi \int_{0}^{\pi} \sin \te e^{\be \mu E \cos \te}
\dif \te} \]
Alternatively but completely equivalently this fraction can be reformulated as a derivative,
\[ \ba{\mu^z} = \f{1}{\be}\f{1}{Z} \der{Z}{E} = \f{1}{\be}\pder{}{E} \ln Z  \]
Evidently, this is more tractable that computing an integral. Therefore,
\[ \ln Z = \ln \br{\sinh \br{\be \mu E}} - \ln \br{\be \mu E} + \text{const} \]
Which yields derivatives,
\[ \pder{}{E}\ln Z = \f{\mu \be}{\sinh\br{\be \mu E}}\cosh\br{\be \mu E} - \f{\be \mu}{\be \mu E} \]

\[ \ba{\mu^z} = \f{\mu\cosh\br{\be \mu E}}{\sinh\br{\be \mu E}} - \f{\mu}{\be \mu E} \]
\[ \ba{\mu^z} = \mu\bc{\coth\br{\be \mu E} - \f{1}{\be \mu E}} \]
\[ \f{\ba{\mu^z}}{\mu} = \coth\br{ \f{\mu E}{k_B T}} - \f{k_B T}{\mu E} \]

\begin{center}
\begin{tikzpicture}
        \begin{axis}[
            standardplot,
            xmin=0, xmax=4, ymin=0, ymax=1.5,
            xlabel={$\f{k_BT}{\mu E}$},
            ylabel={$\f{\ba{\mu^z}}{\mu}$},
            ytick={1},
            yticklabels={$1$},
        ]
        \draw[domain=0.001:4,smooth,thick, variable=\x,red]  plot ({\x},{((1 + e^(-2/\x))/(1 - e^(-2/\x)) - \x)});
        \end{axis}
\end{tikzpicture}
\end{center}

Typically, $\f{\mu E}{k_B} \approx \SI{0.5}{\milli\K}$. \\

\textbf{Response Function}: The response function is typically defined,
\[ \chi_E = \pder{\ba{\mu^z}}{E} = \f{\mu}{ k_B T} \]
This is known as the curie behavior $\f{\mu}{ k_B T}$.

\subsubsection{Polarizable Molecule}

Consider a polarizable molecule with uni-axial direction $\hn$. The molecule has electric dipole $\vec{\mu}_E$ with makes the Hamiltonian the interaction it has with an electric field $\vE$,
\[ H = - \vec{\mu}_E \cdot \vE \]
Similarly to above, the position of the molecule is characterized by $x,y,z$ and its orientation is characterized by $\te,\phi$ through $\hn$. The dipole is induced by the electric field, and points in the direction of $\vn$.
\[ \vec{\mu}_e \propto \br{\vE \cdot \hn} \hn \]
 More explicitly using the polarizability $\al$,
\[ H = - \al \br{\vE \cdot \hn} \hn \cdot \vE = - \al \br{\vE \cdot \hn}^2 = -\al \vE \cdot \bs{\hn \hn} \cdot \vE \]
Where the term $\bs{\hn\hn}$ is a direct vector product or a rank two tensor (sometimes called dyadic).
\[ \br{\hn\hn} = \mtrx{n_x & n_y & n_z}\mtrx{n_x \\ n_y \\ n_z} = \mtrx{n_xn_x & n_xn_y & n_xn_z \\n_yn_x & n_yn_y & n_zn_z \\n_zn_x & n_zn_y & n_zn_z} \]
Taking the angle between $\hn$ and $\vE$ to be $\te$, the Hamiltonian can be conveniently written
\[ H = - \al E^2 \cos^2 \te = - \ti{\al} E^2 \br{\f32\cos^2 \te - \f12} \]
Where $\ti{\al} = \f23 \al$. Which gives,
\[ \ba{\cos^2 \te} = \intl_{0}^{\pi} \cos^2 \te \sin \te \dif \te \dif \phi = 4\pi \]
Recognize that the $\br{\f32 \cos^2 \te - \f12}$ term is the second Legendre polynomial $P_{20} \sim Y_{lm}\br{\te, \phi}; l=1,m=0$.
The lesson here is that even for simple systems such as this, the mathematics can become complicated quite quickly. Also consider an electric quadrapole consider a charge distribution such as in $\text{N}_2$,
\[  \textbf{+} \quad \textbf{-} \quad \textbf{-} \quad \textbf{+} \]
Where the positive charges represent the nuclei each nitrogen, and the negative charges are the electrons.What does this couple with? This would also involve spherical harmonics. Moving back to our dipole, let $Q$ be the $\te$ dependence on $H$,
\[ Q = \f32 \cos^2 \te - \f12 \]
$Q$ acts as the effective degree of freedom of this Hamiltonian. The average energy can be determined by knowing the expectation of $Q$,
\[ \ba{H} = - \al E^2 \ba{Q} \]
\[ \ba{Q} = \f{1}{Z}\int_{0}^{2\pi} \dif \phi \int_{0}^{\pi} Q\br{\te}\sin \te \dif \te e^{-\be \bs{-\al E^2 \br{\f32 \cos^2\te - \f12}}} \]
All contributions to the partition function are identical to the magnetic dipole case done previously except for the angular contribution
\[ Z_{\te,\phi} = \int_{0}^{2\pi} \dif \phi \int_{0}^{\pi} \sin \te \dif \te e^{-\be \bs{-\al E^2 \br{\f32 \cos^2\te - \f12}}} \]
\[ Z_{\te,\phi} = 2\pi e^{\be \al E^2 / 2} \int_{0}^{\pi} \sin \te \dif \te e^{\be \al E^2 \f32 \cos^2\te} \]
Again making the substitution $x = \cos \te$,
\[ Z_{\te,\phi} = 2\pi e^{\be \al E^2 / 2} \int_{-1}^{1} e^{g x^2} \dif x \]
Where $g = \f32 \al E^2 \be$. The remaining integral is \textbf{not} a Gaussian integral. It is known as a \textbf{Dawson integral}.
\[ D\br{g} = \int_{-1}^{1} e^{g x^2} \dif x \]
Now that $Z$ is known, all of thermodynamics follows by performing derivatives on $\ln Z$. Note that $D\br{g}$ can't be computed analytically, but the derivatives with respect to temperature and related quantities modify the integrand of $D\br{g}$ to where it \textit{might} be computable. \\

As an exercise, calculate $\ba{Q}$ in the limit of large temperature. At high temperature, $\be$ and thus $g$ is very small. \textit{Hint}: Taylor series expand out $D\br{p}$ and integrate term by term to desired order.

\subsubsection{Permanent Electric Dipole}

Consider a permanent electric dipole with Hamiltonian composed of the coupling with the electric field and the rotation kinetic energies,
\[ H = H\tsb{rot} - \vec{\mu} \cdot \vE \]
This dipole molecule will be considered a rigid rotator with moment of inertia $I$. As before, consider the molecule $\text{N}_2$ as an example molecule.
\[ H = \f{\hbar^2 \hat{L}^2}{2 I} - \mu E \cos \te \]
Where $\hat{L}$ is the angular momentum operator. Noting that the two components of the Hamiltonian here do not commute. This makes solving the equations of motion difficult because it requires a decoupling through diagonalization of the ODEs.
\[ \hat{L} \sim \pder{}{\te} \qquad \cos \te \sim 1 - \f{\te^2}{2} \]

For a free rotor,
\[ H\tsb{rot} \Psi = E \Psi \]
Where $\Psi \sim Y_{lm}\br{\te, \phi}$ the eigenfunctions are spherical harmonics. $l$ in this case is quantized $l = 0, 1, 2,\ldots$. The eigenvalues or eigenenergies are determined by $l$ alone,
\[ E_l = \f{\hbar^2}{2I}l\br{l+1} \qquad l = 0,1,2,\cdots \]
Where the $m$ quantum number is bound by $l$,
\[ m = 0, \pm 1, \pm 2, \ldots \pm l \]
As such, there are $\br{2l+1}$ states that have energy $E_l$. Summing over the states,
\[ Z = \sum_{\text{states}} e^{-\be \br{\f{\hbar^2}{2I}l\br{l+1}}} \]
Or summing over the energies,
\[ Z = \sum_{l} \br{2l + 1} e^{-\be \br{\f{\hbar^2}{2I}l\br{l+1}}} \]
Note that although decreasing the temperature of the system makes lower energy states \textit{more probable}, the number of microstates \textit{increases} as $l$ increases. These two effects are competing effects.\\
Including the electric field introduces a \textit{Stark effect} analogous to the \textit{Ziemann effect} in magnetic fields. The energy levels of the states are modified by $f_{lm}$,
\[ E_{lm} = \f{\hbar^2}{2I} l\br{l+1} + f_{lm}\br{E} \]

This makes $Z$ way harder to compute. Operationally, everything is the same as in previous examples, but it might require more mathematics. The Stark effect in rigid rotors in found in most Quantum textbooks.

\subsubsection{Spin in Magnetic Field}

For a permanent magnetic moment in a magnetic field, the Hamiltonian is,
\[ H = - \vec{\mu}_m \cdot \vB - g \mu_B \vJ \cdot \vB \]{
Where $g_L$ is known as the \textbf{Land factor}. For an electron $g_{e^-} \approx 2$. In general,
\[ g_L = \f32 + \f{S\br{S+1} - L\br{L+1}}{2J\br{J+1}} \]
For the magnetic field oriented on the $z$-axis $\vB = B \hz$,
\[ H = - g_L \mu_B J^{z} B \]
Where $J^z$ takes on values $-J, -J+1, \ldots J-1, J$. $J^z$ is known as the magnetic quantum number $m_J$.
\[ Z = \sum_{m_J = -J}^{m_J = J} e^{\be g_L \mu_B m_J B} \]
As a trick, let $x = e^{-\be g_L \mu_B B}$
\[ Z = \sum_{m = -J}^{m = J} x^{-m} \]
This is just the geometric series (slightly modified) with analytic sum,
\[ \sum_{m = -J}^{J} x^{-m} = \f{x^{-J}\br{x^{2J+1} + 1}}{x-1} \]
Which gives the partition function the form
\[ Z = \f{\sinh\br{\br{1 + \f{1}{2J}}U}}{\sinh\br{\f{U}{2J}}} \]
Where the term $U$ is just $U=\be g_L\mu_B B$. For $J = \f12$,
\[ Z_{J=\f12} = 2 \cosh\br{U} \]
\subsubsection{Two Level Systems}
Consider a general two level system with two possible energies $0$ and $\vep$ with corresponding degeneracies $g_0$ and $g_\vep$.
\[ Z = \sum_{\text{states}} e^{-\be E\tsb{states}} = \sum_{\text{energies}} g e^{-\be E} \]
For two levels,
\[ Z = g_0 e^{-\be \cdot 0} + g_\vep e^{-\be \vep} \]
As an exercise, one can show that the specific heat has behavior,
\[ \f{C}{k_B} \br{\be E} = f\br{\f{g_\vep}{g_0}, e^{-\be \vep}} \]
Specific heat always has the general plot with increasing temperature,

\begin{center}
\begin{tikzpicture}
        \begin{axis}[
            standardplot,
            xmin=0, xmax=4, ymin=0, ymax=2,
            xlabel={$\f{k_BT}{\vep}$},
            ylabel={$C$},
        ]
        \addplot[domain=0.001:4,smooth,thick, variable=\x,red]  plot ({\x},{4*(1)/(\x^2) * (e^(-1/\x))/(1+e^(-1/\x))^2});
        \end{axis}
\end{tikzpicture}
\end{center}

This is known as a \textbf{Schottky anomaly}. For low temperature,
\[ C \sim e^{-\be/T} \]
and for high temperatures,
\[ C \sim \f{1}{T^2} \]

\subsection{Energy Fluctuations}
Now that we have looked at a number of examples using the canonical ensemble, we will begin to develop some more formalism involved with fluctuations of the canonical ensemble. Recall as a summary that the micro-canonical ensemble involved fixing $N$ and $V$ as well as the total energy $E$ or the energy range $E \pm \De / 2$. The idea was to determine the number of distinct accessible microstates $\Om$ and then computing the entropy of the system, $S = k_B \ln \Om$. Alternatively, the more realistic scenario is the canonical ensemble. Here we fixed the temperature $T$ and allowed the energy to fluctuate. Then, the probability of finding a system in the ensemble in one of the states states (say $E_r$) is given by,
\[ P_r = \f{e^{-\be E_r}}{\sum_r e^{-\be E_r}} \]
Where the denominator is a probability scaling factor and is known as the partition function $Z$.
\[ S = - k_B \sum_r P_r \ln P_r \]
Note the similarity to \textit{Shannon entropy} from information theory. Now what we want to do is to look at \textit{energy fluctuations} in the canonical ensemble. In doing so, we aim to show the approximate equivalency of the two approaches (microcanonical and canonical). We would \textbf{not} like our results for thermodynamically large systems (large $N,T$) to depend on whether or calculations are done in the microcanonical or canonical ensemble.\\

\textbf{Question}: For a fixed temperature, the energy is allowed to fluctuate. The question is, how much or how large can these energies fluctuate?\\

In order to define the fluctuations in energy, we need a notion of where it is fluctuating from. This will be the average energy,
\[ U = \ba{E} = \f{\sum_r E_re^{-\be E_r}}{\sum_r e^{-\be E_r}} \]
This has been seen before. It is ensemble average of the energy.
\[ \pder{\ba{E}}{\be} = -\f{\sum_r E_r^2 e^{-\be E_r}}{\sum_r e^{-\be E_r}} + \f{\br{\sum_r E_r e^{-\be E_r}}^2}{\br{\sum_r e^{-\be E_r}}^2} \]
These terms are just ensemble averages themselves,
\[ \pder{\ba{E}}{\be} = - \ba{E^2} + \ba{E}^2 \]
Note that this is the expectation of the energy deviation,
\[ \ba{\br{\De E}^2} = \ba{\br{E - \ba{E}}^2} = \ba{E^2 - 2E\ba{E} + \ba{E}^2} = \ba{E^2} - 2\ba{E}\ba{E} + \ba{E}^2 = \ba{E^2} - \ba{E}^2 \]
Therefore,
\[ \ba{\br{\De E}^2} =- \pder{\ba{E}}{\be} = - \pder{U}{\be}  \]
This is just the \textbf{variance} of the energy. To take this further, use chain rule,
\[ \pder{U}{\be} = \pder{T}{\be}\pder{U}{T} \]
With $\be = 1/k_B T$ we have $T = 1/ \be k_B$,
\[ \pder{U}{\be} = -\f{1}{\be^2 k_B}\pder{U}{T} = -k_B T^2 \pder{U}{T} \]
\[ \ba{\br{\De E}^2} = k_B T^2 \pder{U}{T} \]
But the specific heat with respect to constant volume is just $C_V = \pdert{U}{T}{V}$,
\[ \ba{\br{\De E}^2} = k_B T^2 C_V \]
Therefore the \textbf{root-mean-squared} or the fluctuations is just,
\[ \f{\sqrt{\ba{\br{\De E}^2}}}{\ba{E}} = \f{ \sqrt{k_B T^2 C_V} }{U} \]
But $C_V \propto N$ is an extensive variable. Also $U \propto N$ and $T$ doesn't scale with temperature. For very large $N$, the root-mean-squared fluctuations in energy go as $1/\sqrt{N}$ and thus are negligible for large $N$. In this limit, the varying energy becomes ultimately zero. Therefore for $N \ar \inf$, the canonical ensemble acts like the microcanonical ensemble. \\

Next we will consider the same example in the case of a \textit{continuous} probability distribution instead of discrete. We now have the interpretation of $P\br{E}$ as a function of $E$. Therefore the probability of energy being in a range $E$ to $E + \dif E$ is given by $P\br{E} \dif E$. Taking $g\br{E}$ as the density of states (see section \ref{sec:densityofstates}).
\[ P\br{E} \dif E \propto g\br{E} e^{-\be E} \dif E \]
There are two energy-dependent factors influencing $P\br{E}$: $g\br{E}$ and $e^{-\be E}$. Let's see whether $P\br{E}$ has an extremum at some point $E^*$.
\[ \pder{}{E}\bre{g\br{E} e^{-\be E}}_{E = E^*} = 0 \]
Instead of computing this directly, use logarithms. Since $\ln$ is monotonically increasing the maximum occurs in the same location $E^*$.
\[ \ln \br{g\br{E} e^{-\be E}} = \ln\br{g\br{E}} + \ln\br{e^{-\be E}} = \ln\br{g\br{E}} + -\be E \]
The maximization condition is then,
\[ \pder{}{E}\bre{\ln \br{g\br{E} e^{-\be E}}}_{E = E^*} = 0 \]
Which becomes,
\[ \pder{}{E}\bre{\ln \br{g\br{E}}}_{E = E^*} = \be \eq \label{eq:estarderv}\]
What is $\ln g\br{E}$? Recall that entropy is the logarithm of the number of accessible microstates. Since $g\br{E}$ is the number of microstates for a particular energy $E$,
\[ S\br{E} = k_B \ln\br{g\br{E}} \]
Subbing this into \eqref{eq:estarderv},
\[ \f{1}{k_B}\pder{}{E}\bre{S\br{E}}_{E = E^*} = \be \]
\[ \pder{}{E}\bre{S\br{E}}_{E = E^*} = \f{1}{T} \]
But as we know from thermodynamics $\pder{S\br{E}}{E} \mid_{E=U} = \f{1}{T}$. This shows us that the ensemble average $U$ is equal to the extremum value $E^*$.
\[ U = E^* \eq \label{eq:probableaverage}\]
As such, $U$ and $E^*$ can be taken interchangably. Irrespective of the Hamiltonian of system, the most probable value of its energy is identical to its mean value. Depending on which is more advantageous we will use $U$ or $E^*$.\\

Consider $\ln \bs{g\br{E} e^{-\be E}} = \ln P\br{E}$, and Taylor expand this around $E = U$. This can be done as we expect $E$ to not vary wildly from $U$ as it fluctuates.
\[ \ln P\br{E} = \bs{\ln e^{-\be E} + \ln g}\big\rvert_{E=U} + \f{1}{2} \pdder{}{E}\bs{\ln e^{-\be E} + \ln g}\big\rvert_{E=U} \br{E-U}^2 + O\br{\br{E-U}^3} \]
Ignoring higher order terms,
\[ \ln P\br{E} = -\be U + \f{S}{k_B} + \f{1}{2} \pdder{}{E} \ln g \big\rvert_{E = U} \br{E - U}^2\]
The right hand term is given by,
\[ \pdder{}{E} \ln g = \f{1}{k_B}\pdder{}{E} S = \f{1}{k_B}\pder{}{E}\pder{S}{E}  = \f{1}{k_B}\pder{}{E}\br{\pder{T}{E} \pder{S}{T}} = \f{1}{k_B} \pder{}{E}\br{\f{1}{T}} = -\f{1}{k_B T^2} \pder{T}{E} = -\f{1}{k_B T^2 C_V} \]
Which gives,
\[ \ln P\br{E} = -\be\br{U - TS} - \f{1}{2} \f{1}{k_B T^2 C_V} \br{E - U}^2\]
Notice the Helmholtz free energy $U - TS$. Looking at $P\br{E} \propto e^{-\be E} g\br{E}$. Then we have,
\[ P\br{E} \sim e^{-\be \br{U - TS}} e^{-\f{\br{E-U}^2}{2 k_B T^2 C_V}} \]
We can then determine the partition function,
\[ Z = Z\br{V,T} \propto e^{-\be \br{U - TS}} \intl_{0}^{\inf} \dif E e^{-\f{\br{E-U}^2}{2 k_B T^2 C_V}} \]
Recall Gaussian integrals \ref{sec:gaussianintegrals},
\[ Z \approx e^{-\be \br{U - TS}} \sqrt{2 \pi k_B T^2 C_V} \]
Recall that $A \defined - k_B T \ln Z$ so we have,
\[ A = \br{U - TS} - \f12 kT \ln \br{2 \pi k_B T^2 C_V} \]
If $U-TS$ is the Helmholtz free energy, the extra piece is a problem. Consider the high $N$ limit. The logarithm term becomes negligible compared to the leading $U$ term. Hence we have $A \approx U - TS$ derived from stat mech. As such, the microcanonical and canonical ensemble approaches are similar. \\

In principle, we could derive all of thermodynamics from statistical mechanics. This is left to the reader as an exercise. Since we derived $A$ from the canonical ensemble using $S$ from microcanonical ensemble, the emergence of a consistent thermodynamic relationship establishes that the two approaches are essentially identical.

\subsection{Equipartition \& Virial Theorems}

\subsubsection{Equipartition}

The Equipartition theorem is really useful to estimate or determine thermodynamic properties of systems by analyzing the degrees of freedom of the system. Consider a Hamiltonian $H \br{\vq, \vp}$. Let's call $x_i$ and $x_j$ any two of the $6N$ generalized coordinates of the system $\br{q,p}$. In the canonical ensemble we are interested in the expectation,
\[ \ba{x_i \pder{H}{x_j}} \]
Explicitly,
\[ \ba{x_i \pder{H}{x_j}} = \f{\int x_i \pder{H}{x_j} e^{-\be H} \dif^{3N} q \dif^{3N} p}{\int e^{-\be H} \dif^{3N} q \dif^{3N} p} \]
Just considering the numerator requires integration by parts,
\[ \int x_i \pder{H}{x_j} e^{-\be H} \dif \w = \int \dif \w_i\bc{ - \f{1}{\be} x_i e^{-\be H}\bigg\rvert_{\br{x_j}_1}^{\br{x_j}_2} + \f{1}{\be}\int  \pder{x_i}{x_j} e^{-\be H} \dif x_j} \dif \w_j  \]
Where $\br{x_j}_1$ and $\br{x_j}_2$ are the extremal values. $\dif w_j$ is the phase space volume excluding $\dif x_j$. The first term banishes because $H$ becomes infinite at extremal values. For example, the potential becomes infinite of the kinetic energy becomes infinite at the boundaries. The second term has a $\de$ function.
\[ \pder{x_i}{x_j} = \de_{ij} \]
Therefore the numerator becomes,
\[ \f{1}{\be} \di_{ij} \int e^{-\be H} \dif \w_j \]
Thus,
\[ \ba{x_i \pder{H}{x_j}} = \f{\f{1}{\be} \di_{ij} \int e^{-\be H} \dif \w}{\int e^{-\be H} \dif \w} \]
After canceling,
\[\ba{x_i \pder{H}{x_j}} = \de_{ij} k_B T \eq \label{eq:equiresult}\]
Notice that this is independent of the Hamiltonian $H$.\\

\textbf{Considerations}:
First for $x_i = x_j = p_i$, \eqref{eq:equiresult} becomes,
\[\ba{p_i \pder{H}{p_i}} = k_B T \]
From Hamiltonian's equations, $\dot{q}_i = \pder{H}{p_i}$,
\[\ba{p_i \dot{q}_i} = k_B T \eq \label{eq:pdotq} \]
Similarly using Hamiltonian's other equation $\dot{p}_i = -\pder{H}{q_i}$
\[\ba{q_i \dot{p}_i} = -k_B T \eq \label{eq:qdotp}\]
Then adding $i=1\ar3N$ for both $p_i$ and $q_i$,
\[ \ba{\sum_i p_i \dot{q}_i} = 3 N k_B T \eq \label{eq:sumpdotq}\]
\[ -\ba{\sum_i q_i \dot{p}_i} = 3 N k_B T \eq \label{eq:sumqdotp}\]
In many cases, $H\br{q,p}$ in generalized quadratic form of the coordinates $\bc{q_i, p_i}$. A \textbf{canonical transformation} can be used to diagonalized $H$ into the following form,
\[ H = \sum_j A_j P_j^2 + \sum_j B_j Q_j^2 \]
Where $A_j, B_j$ are constants and $P_i,Q_i$ are the corresponding to the transformed coordinates. Under this transformation,
\[ \pder{H}{P_j} = A_j 2 P_j \quad \pder{H}{Q_j} = B_j 2 Q_j \]
Summing over all $j$,
\[ \sum_j \br{P_j \pder{H}{P_j} + Q_j \pder{H}{Q_j}} = \sum_j \br{P_j A_J 2 P_j + Q_j B_j 2 Q_j} = 2\sum_j \br{A_J P_j^2 + B_j Q_j^2} = 2H \]
From equation \eqref{eq:pdotq} and \eqref{eq:qdotp},
\[ \ba{H} = \f12 \br{f_A + f_B} k_B T = \f12 f k_B T \]
Where $f_A$ and $f_B$ sum together to give $f$ are the number of $A_i$ an $B_j$ that are non-zero in the Hamiltonian. \textit{This is a very important result}. Each harmonic (quadratic) term in the Hamiltonian contributes exactly $\f12 k_B T$ toward to the energy. As a consequence, each degree of freedom contributes $\f12 k_B$ towards the specific heat $C_V$. This is the classical version of the theorem of Equipartition theorem. \\

Note that at really low temperatures, this breaks down. Consider \textit{phonons} which are lattice vibrations, each behaving as quantum harmonic oscillators. At low temperatures in certain systems, the energy can become quantized and there are corrections to the above result that can be seen experimentally.

\subsubsection{Virial Theorem}

Consider equation \eqref{eq:sumqdotp},
\[ \ba{\sum_i q_i \dot{p}_i} = -3 N k_B T  \]
The Virial theorem introduces the notion of generalized force,
\[ F_{i,\text{generalized}} = \dot{p}_i \]
Thus \eqref{eq:sumqdotp} is the expectation value of the product of coordinate $q_i$ with the force acting on it. It is called the \textbf{Virial}.
\[ \mathcal{V} = \ba{\sum_i q_i \dot{p}_i} \]
The Virial equation is simply then,
\[ \mathcal{V} = -3 N k_B T \eq \label{eq:equivirial} \]
More commonly, it is written as a force integral. \\

\textbf{Example}: Consider an ideal gas that is dilute enough that it is not interacting with itself but it only interacting with the walls of the container it is in for example. The Virial for this system is thus,
\[ \mathcal{V}_0 = \ba{\sum_i q_i F_i}_0  = - P \oiint_S \vr \cdot \dif \vs\]
Where the `$0$' is just to indicate that it is not interacting. Here $P$ is the pressure (uniform) pointing inward on a ball $S$ (hence the minus sign), $\dif \vs$ is the area normal pointing outward, and $\vr$ is the location on the surface of the ball $S$. Applying the divergence theorem, we can relate the surface integral to a volume integral,
\[ \oiint_S \vr \cdot \dif \vs = \iiint_V \div \vr \dif V  \]
Remember that $\div \vr$ is just,
\[ \div \vr = \br{\pder{}{x}\hx + \pder{}{z}\hy + \pder{}{z}\hz} \cdot \br{ x \hx + y\hy + z\hz } = 1 + 1 + 1 = 3 \]
Thus the Virial is just,
\[ \mathcal{V}_0 = - 3 PV \]
Where $V$ is the volume contained inside the bounding surface $S$. From this using \eqref{eq:equivirial},
\[ PV = N k_B T  \implies PV = nRT \]
Which is just the ideal gas law. Note the this only works for an ideal gas. A more complicated gas with internal fores would have a more complicated integral for $\mathcal{V}$. From this, the equation of state would become,
\[ \f{P}{\f{N}{V} k_B T} = 1 + \f{1}{dNk_B T} \ba{\sum_{i<j} \vF\br{\vf_{ij}}\cdot \vr_{ij}}  \]
Where $d$ is just the spacing dimension. This is known as the \textbf{virial equation of state}. It is typically solved computationally. One can make a career in Chemistry solving the equation of state for different systems.

\section{Grand Canonical Ensemble}

Recall that microcanonical ensemble was constructed with fixed energy $E$, the canonical ensemble which has fixed $T$ and energy was allowed to be exchange energy $E$ with the reservoir. The \textbf{grand canonical ensemble (GCE)} is simply a generalization of the canonical ensemble where the number of particles $N$ \textit{and} the total energy $E$ is allowed to be shared via fluctuations with the reservoir. Similar to before, we hope to recover thermodynamic relations using this formalism. The derivations of the grand canonical ensemble follow almost precisely the same as it did in the case of the canonical ensemble, with a few caveats. \\

\begin{center}
\begin{tikzpicture}
    \filldraw[fill=lightgray, draw=blue,dashed] (-1,-1) rectangle (1,1);
    \draw[dashed, red] (-2,-2) rectangle (2,2);
    \node[above, red] (lobject) at (0,2) {Object};
    \node (lreservoir) at (0,-1.5) {Reservoir};
    \node (lsystem) at (0,0) {System};
    \draw[->, thick] (0.1, 0.8)   -- (0.1,1.2);
    \draw[<-, thick] (-0.1, 0.8)  -- (-0.1,1.2);
    \draw[->, thick] (0.8, 0.1)   -- (1.2,0.1);
    \draw[<-, thick] (0.8, -0.1)   -- (1.2,-0.1);
    \draw[->, thick] (-0.1, -0.8) -- (-0.1,-1.2);
    \draw[<-, thick] (0.1, -0.8)  -- (0.1,-1.2);
    \draw[->, thick] (-0.8, -0.1) -- (-1.2,-0.1);
    \draw[<-, thick] (-0.8, 0.1)  -- (-1.2,0.1);
\end{tikzpicture}
\end{center}

Now there the interface between the system and reservoir is some sort of permeable membrane that allows for a flow of particles between the system and reservoir. \\

\textbf{Observation}: Recall that making the switch from the microcanonical ensemble to canonical ensemble made life easier in terms of a procedures/computations because we relaxed the constraint that the system energy $E$ was fixed. The Grand canonical ensemble further relaxes the constraint of $N$ being fixed. This too will further make the computations even \textit{simpler}. \\

As a first example, consider that a slab of $N_2$ just above a substrate where there is an exchange of material between the $N_2$ and the substrate as well as the energy of the two materials as the system evolves. This is an example of a grand canonical ensemble. \\

Alternatively, an example of a quantum system with Hamiltonian,
\[ H = \sum_{i = 1}^{N} \f{p_i^2}{2m} + \f{c}{2} \br{x_{i+1} - x_i}^2 \]
Can be mapped (with algebra) to the equivalent problem of a system of harmonic oscillators,
\[ H = \sum_k = \hbar \w\br{k} \bc{ a_k^\dagger a_k + \f12} \]
Where $N_k = a_k^\dagger a_k$ is known as the \textbf{number operator}. This number is typically not conserved in most contexts, which further motivates the use of the grand canonical ensemble as lots of systems share the properties of fluctuating energy $E$ and number of particles $N$.

\subsection{Derivation}

We will label the number of particles in the in the reservoir $N^{(r)}$ and the energy in the reservoir $E^{(r)}$. The number of particles,energy in the system is $N^{(s)}, E^{(s)}$. Assuming that $N^{(r)} \gg N^{(s)}$ and $E^{(r)} \gg E^{(s)}$ because the reservoir is considered so large. We will assume the total number of particles $N$ and the total energy $E$ to be constant with,
\[ N = N^{(r)} + N^{(s)} \qquad E = E^{(r)} + E^{(s)} \]
Note that the fraction of particles in the system has the form,
\[  \f{N^{(s)}}{N} = \br{1 - \f{N^{(r)}}{N}} \ll 1 \]
And the fraction of energy in the system is,
\[  \f{E^{(s)}}{E} = \br{1 - \f{E^{(r)}}{E}} \ll 1 \]
Similar to the canonical case, if the system is found to be the state $\br{N^{(s)}_i,E^{(s)}_j}$, the probability is proportional to the number of microstates, $\Om^{(r)}\br{N^{(r)}_i,E^{(r)}_j}$ that the reservoir has access to,
\[  P\br{N^{(r)}_i,E^{(r)}_j} \propto \Om^{(r)}\br{N^{(r)}_i,E^{(r)}_j} \]
Well will write this as the exponent of a logarithm to allow for the Taylor series to converge ($P \propto \exp \br{\ln \Om^{(r)}}$). Expanding around the total energy and number of particles $E,N$ to first order,
\[ \ln \Om^{(r)} \br{N^{(r)},E^{(r)}} \approx \ln \Om^{(r)} \br{N,E} + \pder{\ln \Om^{(r)}}{N^{(r)}} \bigg\rvert_{N^{(r)} = N} \br{N^{(r)} - N} + \pder{\ln \Om^{(r)}}{E^{(r)}} \bigg\rvert_{E^{(r)} = E} \br{E^{(r)} - E} \]
Noting that $E^{(r)} - E = - E^{(s)}$ and $S = k_B \ln \Om$, the derivative of $S$ with respect to $N$ is given by the chemical potential $\mu$,
\[ \pder{S}{N} = - \f{\mu}{T} \qquad \pder{S}{E} = \f{1}{T} \]
We obtain,
\[ \ln \Om^{(r)} \br{N^{(r)},E^{(r)}} \approx \ln \Om^{(r)} \br{N,E} + \f{\mu^{(r)}}{k_B T^{(r)}} N^{(s)} - \f{1}{k_B T^{(r)}} E^{(s)} \]
Here $\mu^{(r)} = \mu^{(s)} = \mu$ and $T^{(r)} = T^{(s)} = T$ since we are considering being in chemical and thermodynamic equilibrium respectively. As such we have obtained that,
\[ P_{ij}\br{N^{(s)}_i,E^{(s)}_j} \propto \exp\bs{-\al N^{(s)}_i - \be E^{(s)}_j} \]
Where the constants are defined as,
\[ \al \defined -\f{\mu}{k_B T}  \quad \be = \f{1}{k_B T} \eq \label{eq:albe}\]
Normalizing the probability removes the proportionality. Summing over all accessible states of the system that are allowed $\bc{i,j}$,
\[ P_{ij}\br{N^{(s)}_i,E^{(s)}_j} =  \f{\exp\bs{-\al N^{(s)}_i - \be E^{(s)}_j}}{\sum_{i,j}\exp\bs{-\al N^{(s)}_i - \be E^{(s)}_j}} \]
Or dropping the notation of the system,
\[ P_{ij}\br{N_i,E_j} =  \f{\exp\bs{-\al N_i - \be E_j}}{\sum_{i,j}\exp\bs{-\al N_i - \be E_j}} \eq \label{eq:PGCE}\]

\subsection{Comparison to Canonical Ensemble}
We will now consider a system in the grand canonical \textbf{ensemble}. Imagine an ensemble of $\mathcal{N}$ identical copies of the system of interest. Together they will share a total energy $\mathcal{N} \bar{E}$ and will also share a total number of particles $\mathcal{N} \bar{N}$. As a manner of notation, we will call $n_{i,j}$ the number of systems that have number of particles $N_{i}$ and energy $E_{j}$. Then we obtain the following relations,
\[ \sum_{i,j} n_{i,j} = \mathcal{N} \eq \label{eq:gcc1} \]
\[ \sum_{i,j} n_{i,j}N_i = \mathcal{N} \bar{N} \eq \label{eq:gcc2}\]
\[ \sum_{i,j} n_{i,j}E_j = \mathcal{N} \bar{E} \eq \label{eq:gcc3} \]

\textbf{Remark}: As a comparison to the canonical ensemble, the new constraint associated with the GCE is the constraint of total number of particles \eqref{eq:gcc2}. \\

Any set $\bc{n_{i,j}}$ of the numbers $n_{i,j}$ that satisfy the above constraints (\eqref{eq:gcc1}, \eqref{eq:gcc2}, \eqref{eq:gcc3}) represents the possible \textit{modes of distribution} of particles among the ensemble. Any such mode of the distribution can be realized in $W\bc{n_{i,j}}$,
\[ W \bc{n_{i,j}} = \f{\mathcal{N}!}{\prod_{i,j} \br{n_{i,j} !}} \]
Where $\mathcal{N}!$ is the number of permutations associated with the copies of the system and $\prod_{i,j} \br{n_{i,j} !}$ are the internal number of permutations that maintain the microstates of the system. We will again use Lagrange multipliers in order to extremize this distribution to find the \textit{most probable} mode. Just as before the derivation employs that $N \gg 1$. From this one obtains,
\[ \f{n_{i,j}}{\mathcal{N}} =  \f{e^{-\al N_i - \be E_j}}{\sum_{i,j}e^{-\al N_i - \be E_j}}\]
Which is identical to \eqref{eq:PGCE}. This is fitting as $n_{i,j}$ is the number of systems that happen to have $N_i$ particles and $E_j$ energy, and $\mathcal{N}$ is the total number of systems. One can also show that in the thermodynamic limit $n\tsb{most prob} = \ba{n_{i,j}}$. This is precisely the same idea that was invoked to prove \eqref{eq:probableaverage}. We will use this result moving forward.
\[ n_{i,j} = \ba{n_{i,j}} \]
Using this we can compute the average number of particles,
\[ \bar{N} = \f{\sum_{i,j} N_i e^{-\al N_i - \be E_j}}{\sum_{i,j}e^{-\al N_i - \be E_j}} \]
Using the derivative of the logarithm trick performed many times before,
\[ \bar{N} = -\pder{}{\al}\ln \bc{\sum_{i,j} e^{-\al N_i - \be E_j}} \]
Similarly, the average energy of the system is determined by a derivative of $\be$.
\[ \bar{E} = -\pder{}{\be}\ln \bc{\sum_{i,j} e^{-\al N_i - \be E_j}} \]
Just as in the canonical ensemble, $\be = 1/k_B T$; however what is the physical interpretation/significance of $\al$? Note that this was previously motivated with \eqref{eq:albe} above, but we will re-examine this again now.

\subsection{Physical Significance of $\al, \be$}

For convenience of notation we will define the non-physical quantity $\eta$ as,
\[ \eta \defined \ln \br{\sum_{i,j} e^{-\al N_i - \be N_j}} \eq \label{eq:eta}\]
Thermodynamically, $\al, \be, E_j$ can vary depending on volume. To digest or interpret $\al, \be$, we first need to compute $\dif \eta$.
\[ \dif \eta = - \bar{N} \dif \al - \bar{E} \dif \be - \f{\be}{\mathcal{N}} \sum_{i,j} \ba{n_{i,j}} \dif E_j \eq \label{eq:difeta}\]
Now consider the quantity $\eta + \bar{N} \al + \bar{E} \be$ and its differential,
\[ \dif \br{\eta + \bar{N} \al + \bar{E} \be} \]
By chain rule and \eqref{eq:difeta} this can be expressed,
\[ \dif \br{\eta + \bar{N} \al + \bar{E} \be} = \be \bc{ \f{\al}{\be}\dif \bar{N} + \dif \bar{E} -\f{1}{\mathcal{N}} \sum_{i,j} \ba{n_{i,j}} \dif E_j } \]
Recall from the first law of thermodynamics,
\[ \dif \bar{E} = \dif Q - \dif W + \mu \dif \bar{N} \]
Comparing term by term,
\[ \mu = - \f{\mu}{\be} \qquad \dif W = - \f{1}{\mathcal{N}} \sum_{i,j} \ba{n_{i,j}} \dif E_j \qquad \dif \br{\eta + \bar{N} \al + \bar{E} \be} = \be \dif Q\]
Therefore we obtain that,
\[ \al = -\f{\mu}{k_B T} \]
\subsection{Grand Canonical Partition Function}
Just as found for \eqref{eq:albe}. Further examination of the incremental heat,
\[ \dif \br{\eta + \bar{N} \al + \bar{E} \be} = \be\dif Q = \f{\dif S}{k_B} \]
Therefore we have the entropy of the system in terms of $\eta, \al, \be, \bar{N}$ and $\bar{E}$,
\[ S = k_B \br{\eta + \bar{N} \al + \bar{E} \be} \]
Or rearranged for $\eta$,
\[ \eta = \f{S}{k_B} + \f{\mu\bar{N}}{k_B T} - \f{\bar{E}}{k_BT} = \f{ST + \mu\bar{N} - \bar{E}}{k_BT} \]
Recall the Gibb's free energy $G = \bar{E} - TS + PV = \mu \bar{N}$ and the original definition for $\eta$ \eqref{eq:eta},
\[ \eta = \ln \br{\sum_{i,j} e^{-\al N_i - \be N_j}} = \f{PV}{k_B T} \eq \label{eq:eta}\]
So $\eta$ has a physical interpretation after all. \\

Note that $e^{-\al N_i} = \br{e^{-\al}}^{N_i}$ has the inner term often denoted with a lower case $z$,
\[ z = e^{-\al} = e^{\f{\mu}{k_B T}}\]
Where $z \neq Z$ is \textbf{not} the partition function. It is known as the \textbf{fugacity}. \\

Clearly $\eta$ given by \eqref{eq:eta} plays a crucial role. It can be rewritten as,
\[ \eta  = \ln \br{\sum_{i,j} e^{-\al N_i} e^{-\be E_j}}  = \ln \br{\sum_{i} e^{-\al N_i} \sum_{j}e^{-\be E_j}} \]
The second sum is given by the possible energies of the system $E_j$. Instead of summing over these arbitrary indices, we can replace this with summing over the states $s$ with $E_s$ depending possibly on the number of particles $N_i$,
\[ \eta  = \ln \br{\sum_{i} e^{-\al N_i} \sum_{s}e^{-\be E_s\br{N_i}}} \]
Recalling the canonical partition function $Z = \sum_s e^{-\be E_s}$. Since $Z$ can now depend on the number of particles, we will denote it $Z_{N_i}$.
\[ \eta  = \ln \br{\sum_{i}z^{N_i} Z_{N_i} \br{V,T}} \]
Where $Z_{N_i}$ is the canonical partition function for a given number of particles $N_i$ and it depends on the volume and temperatures of the system. It is often the case that the canonical partition function $Z$ has its label replaced with a $Q$.
\[ \eta  = \ln \br{\sum_{i} z^{N_i} Q_{N_i} \br{V,T}} \]
The term in the logarithm is known as the \textbf{grand canonical partition function}.
\[ Z_{GC} = \sum_{i} z^{N_i} Q_{N_i} \br{V,T} \]
How does it work? Recall that $Q_{N_i} \br{V,T} = \sum_s e^{-\be E_s}$ is identically the canonical partition function as done before. Once one obtains this for each possible number of particles $N_i$, the grand canonical partition function is simply a weighted sum of each of these $Q_{N_i}$'s. Alternatively, this modification can be viewed as a shifting of energy for the corresponding number of particles,
\[ \sum_{i} e^{-\al N_i} \sum_{s}e^{-\be E_s\br{N_i}} = \sum_{i} \sum_{s}e^{-\be\br{E_s - \mu N_i}} \]

\textbf{Example}: Consider an illuminating example of a system of 3 spins ($S_i = \pm 1, \forall i$) with Hamiltonian,
\[ H = J \br{S_1 S_2 + S_2 S_3 + S_3 S_1} \]
Where two states have energy $E = 3J$ and 6 states have energy $E=-J$. Letting $s$ run through the states,
\[ \sum_{s=1}^{s=8} e^{-\be E_s} = 2e^{-\be\br{3J}} + 6e^{-\be\br{-J}} \]
Thus it is more efficient computationally to recognize these repeated values and include the degeneracies in the formula.\\

Returning to the generic case, instead of summing over the index $i$,
\[ \sum_{i} e^{\be \mu N_i} \ar \sum_{N_i = 0}^{\inf} e^{\be \mu N_i} \]
We can sum over the actual number of particles $N_i$. This is precisely analogous to summing over the energies instead of summing over the states as throughly discussed in section \ref{sec:densityofstates}. Altogether,
\[ Z_{GC} = \sum_{N_i = 0}^{\inf} e^{\be \mu N_i} \br{\sum_{E}^{'} g\br{E}e^{-\be E}}_{N_i} \]
Note that there is no corresponding density of states for the number of particles $g\br{N}$. $g\br{E}$ corresponds to the number of states that happen to have the same energy $E$. However if the number of particles are the same, it corresponds to precisely the same state without degeneracies. Moreover notice that the second sum in this formula has a prime to make clear that the sum is made using the constraint that the number of particles is fixed. It behaves, for fixed $N$, precisely as the \textit{canonical} ensemble.
\[ Z_{GC} = \sum_{N = 0}^{\inf} z^{N} Z_{C}\br{N} \]

% TODO write notes about pages 6.7 - 6.13 and some examples (p95-97), (p103-105)

\section{Ideal Quantum Gases}

We will begin now to discuss ideal quantum gases. Just as in the ideal gas case, we may restrict our setting to just deal with \textbf{non-interacting} particles. As such out general Hamiltonian,
\[ H = H\tsb{kin} + \sum_{i>j} V_{ij} \]
May be simply written,
\[ H = H\tsb{kin} \]
This switch can be motivated for the regime of $\be V = \f{V}{k_B T} \ll 1$ or in the case of \textit{dilute} gases. Since the particles are quantum mechanical, they are indistinguishable. Examples of these gases include,
\begin{itemize}
    \item fermions: electrons, protons, neutrons, $S = \f12 \hbar$
    \begin{itemize}
        \item Governed by the Pauli Exclusion principle
    \end{itemize}
    \item bosons: collective excitations, composite objects, photon, phonon
\end{itemize}

An example of a composite particle is a hydrogen atom. Under the correct description, a hydrogen atom is composed of two fermions (electron, proton) and their combination can behave as a boson with spin ($S = \f12 \hbar$).

\subsection{Microcanonical Ensemble For IQG}
To begin, we should first look at ideal quantum gases from the perspective of fixed $N, V, E$ (microcanonical ensemble). A non-quantum ideal gas has a fixed energy of (for each particle at temperature $T$),
\[ E = \f32 N k_B T \]
So how can we modify this expression to deal with \textit{quantum} ideal gases? Recall the results for a particle in a box of length $L$ with Hamiltonian,
\[ H\tsb{kin} = \f{\hbar^2}{2m} \vdel^2 \]
With TISE,
\[ \f{\hbar^2}{2m} \vdel^2 \psi = E \psi \]
Where states $\psi$ are standing waves within the box. The difference between fermions and bosons is that no two fermions can share exactly the same quantum state while bosons can. For example, only two (one with spin up, one with spin down) electrons can occupy the lowest energy state standing wave $\psi_1 = \sin \br{\f{\pi x}{L}}$. Knowing this, it is clear that the number of configurations for fermions is considerably lower than for bosons. \\

What we will do is look at an energy spectrum with energy levels $\vep_i$ with,
\[ \vep_1 < \vep_2 < \cdots < \vep_n \]
Where there are $g_i$ states that share that eigenenergy. For each energy level $\vep_i$, we can have $n_i$ particles with that energy. Therefore to characterize the $i$-th cell, we need $\br{n_i; \vep_i; g_i}$. Under this picture we have constraints:
\begin{enumerate}
    \item number constraint $N = \sum_i n_i$
    \item energy constraint $E = \sum_i n_i \vep_i$
\end{enumerate}
Now under these constraints, our goal is to compute the number of microstates $\Om$ accessible to the system for fixed $N, V, E$.
\[ \Om\br{N,V,E} = {\sum_{\bc{n_i}}}' \mathcal{W} \bc{n_i} \]
Where $\mathcal{W}\bc{n_i}$ is the number of microstates associated with a distribution (of particles) $\bc{n_i}$ consistent with constraints $1,2$. Moreover the prime on the summation symbol $\Sigma'$ indicates the summation over all these ``modes of distribution'' that must fulfill the constraints above. What is $\mathcal{W}\bc{n_i}$?
\[ \mathcal{W}\bc{n_i} = \prod_{i} w \br{i} \]
Where $w \br{i}$ is the number of distinct ways in which $n_i$ indistinguishable particles can be distributed among the (distinguishable) $g_i$ levels/states in the $i$-th cell. The form of $w\br{i}$ will of course depend on the type of particle in question.\\

\subsection{Statistical Distributions}
\label{sec:statdist}
\textbf{Bosons}:
A system of bosons will be called a \textbf{Bose-Einstein statistic (BE)}. This is just given by the typical stars and bars result,
\[ w\tsb{BE} \br{i} = \f{\br{n_i + g_i - 1}!}{n_i !\br{g_i - 1}!} \]
Thus making $\mathcal{W}\bc{n_i}$ given to be,
\[ \mathcal{W}\tsb{BE}\bc{n_i} = \prod_{i} \f{\br{n_i + g_i - 1}!}{n_i !\br{g_i - 1}!} \]

\textbf{Fermions}:
By contrast, the same result for a \textbf{Fermi Dirac statistic (FD)} is quite different. Any particular fermion has $g_i$ possible states in can occupy for each energy cell $\vep_i$. However, the second particle has less choice. The second particle will have access to $g_i - 1$ states since one is already occupied by the first one. This process continues until the $n_i$-th particle picks a state,
\[ g_i \br{g_i - 1}\br{g_i - 2}\cdots\br{g_i - \br{n_i - 1}} \note{$n_i \leq g_i$}\]
One can write this more compactly as the difference between two factorials,
\[ \f{g_i !}{ \br{g_i - n_i}!} \]
This gives the number of ways, one can distribute $n_i$ particles into $g_i$ levels with no more than $1$ particle per level. However since the particles are indistinguishable we have over-counted by the number of permutations of the $n_i$ particles themselves,
\[ w\tsb{FD} = \f{1}{n_i!}\f{g_i !}{ \br{g_i - n_i}!} \]
Making,
\[ \mathcal{W}\tsb{FD}\bc{n_i} = \prod_{i} \f{1}{n_i!}\f{g_i !}{ \br{g_i - n_i}!} \]

\textbf{Maxwell-Boltzmann distribution}: Assuming first that the particles are indistinguishable, for the distribution set $\bc{n_i}$ we can ask: How many ways can $N$ particles be distributed into $n_1$ particles in cell 1 and $n_2$ particles in cell 2 and $n_i$ particles in cell $i$? This is given by the multinomial distribution,
\[ \f{N!}{n_1! \cdot n_2! \cdots n_i!} = \f{N!}{\prod_i n_i !} \]
Now with $g_i$ states we can have $g_i^{n_i}$ configurations of particle distribution of $n_i$ particles among $g_i$ states in cell $i$. Furthermore removing the distinguishability of the $N$ particles requires division by $N!$,
\[ \mathcal{W}\tsb{MB}\bc{n_i} = \f{1}{n_i!} \prod_{i} g_i^{n_i}  \]
For supplementary materials, see Sertagg-Wan Wylen. \\ % Typo in the name TODO

\subsection{Entropy \& Distributions}
\label{sec:a}
By with any of these distributions, we can determine the entropy of the system in the usual way,
\[ S \br{N, V, E} = k_B \ln \Om \br{N, V, E} = k_B \ln \br{{\sum_{\bc{n_i}}}' \mathcal{W}\bc{n_i}} \]
In the thermodynamic limit $N \gg 1, n_i \gg 1, g_i \gg 1$, etc, the sum ${\sum_{\bc{n_i}}}' \mathcal{W}\bc{n_i}$ is very well approximated by $\mathcal{W}\bc{n_i^*}$ where $\bc{n_i^*}$ is the distribution of $\bc{n_i}$ that will maximize $\mathcal{W}\bc{n_i}$. As such we obtain the approximation,
\[ S \br{N, V, E} = k_B \ln \mathcal{W} \bc{n_i^*} \]
Now we have to maximize $\mathcal{W}$ whether it be $\mathcal{W}\tsb{BE}$, $\mathcal{W}\tsb{FD}$ or $\mathcal{W}\tsb{MB}$. Use Lagrange multipliers again and maximize the logarithm of $\mathcal{W}$,
\[ \dif \ln \mathcal{W} \bc{n_i} - \bc{\al \sum_i \dif n_i + \be \sum_i \vep_i \dif n_i} = 0 \]
% TODO do this exercise
Plugging in each of the distributions above $\mathcal{W}$,
\[ n_i^* = \f{g_i}{e^{\al + \be \vep_i} + a} \]
Where we have,
\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        Distribution & $a$ \\
        \hline
        $\mathcal{W}\tsb{BE}$ & $-1$ \\
        $\mathcal{W}\tsb{FD}$ & $+1$ \\
        $\mathcal{W}\tsb{MB}$ & $0$ \\
        \hline
    \end{tabular}
\end{center}

\textbf{Remark}: One thing to notice is that $n_i^*$ is proportional to $g_i$. As such, $\f{n_i^*}{g_i}$ can be interpreted as the most probable number of particles per energy level in the $i$-th cell. This also suggests we can interpret $\f{n_i^*}{g_i}$ as the most probable number of particles \textit{even if} we had a single energy level/state $\vep_i$. \\

\textbf{Comment}: This last statement can be obtained rigorously within the grand canonical ensemble where each single particle energy state is \textit{considered individually} without grouping them in cells with a number $g_i$ much greater than $1$ ($g_i \gg 1$) per cell. \\

What about other ensembles like canonical and grand canonical?

\subsection{Ideal gas in other Q.M. ensembles}

\textbf{Canonical}: First we will look at the case of the canonical ensemble approach to ideal quantum gases. We will restrict our perspective to gases ``free to move''in some volume $V$ subject to their corresponding statistics outlined in section \eqref{sec:statdist}. We will therefore have in mind energy for states $E_s$ and energy states for single particles $\vep$. The reason for this distinction is that for single, isolated particle alone ($N=1$),
\[ H \psi = -\f{\hbar^2}{2m} \del^2 \psi = \vep \psi \]
Since there is only one particle, we are ignoring the interaction potential $V_{ij}$ (mind you we are ignoring them anyway for \textit{ideal} quantum gases). Therefore for particles in a box we have the usual,
\[ \vep = \f{\hbar^2}{2m} \br{k_x^2 + k_y^2 + k_z^2} = \sum_{m=1}^{d} \f{\hbar^2}{2m} k_m^2 \]
With wave function state with the general form,
\[ \psi\br{x,y,z} \sim \f{1}{\sqrt{L}} e^{i\br{\vk \cdot \vr}} \]
Where $\vk = k_x \hat{x} + k_y \hat{y} + k_z \hat{z}$. Boundary conditions yield a quantization of energies
\[\psi\br{x,y,z} = \psi\br{x+L,y,z} \implies e^{ik_x L} = 1 \]
Thus $k_x L$ must be multiple of $2\pi$,
\[ k_x L = 0, \pm 2\pi, \pm 4\pi\ldots  \]
The same relationships hold for $k_y, k_z$. So in order to determine the canonical partition function,
\[ Z_{N}\br{V,T} = {\sum_s}' e^{-\be E_s}  \]
Since we have that $E_s = \sum_{\vep} n_\vep \vep$ ($n_\vep$ is the number of particles with energy $\vep$) we can sum over the energies,
\[ Z_{N}\br{V,T} = {\sum_{\bc{n_\vep}}}' F\bc{n_\vep} e^{-\be \sum_{\vep} n_\vep \vep} \]
The prime on the summation used to indicate that the sum is performed under the constraint that the number of particles is fixed,
\[ \sum_{\vep} n_\vep = N \]
The total energy is also given by the sum of the particles,
\[ \sum_\vep n_\vep \vep = E\]
But we don't care about this ``constraint'' because the total energy is allowed to vary. Moreover, $F\bc{n_\vep}$ is a statistical weight appropriate to the relevant distribution. Therefore we have that,
\[ Z_{N}\br{V,T} = {\sum_{\bc{n_\vep}}}' F\bc{n_\vep} e^{-\be \sum_{\vep} n_\vep \vep} \]
With weights given by,
\begin{center}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Distribution} & $\mathbf{F\bc{n_\vep}}$ \\
    \hline
    B.E. & $1$ \\
    \hline
    F.D. & $1$ if all $n_\vep = 0,1$ \\
         & $0$ otherwise \\
    \hline
    M.B. & $\prod_\vep \f{1}{n_\vep !}$\\
    \hline
\end{tabular}
\end{center}
As it turns out, only the Maxwell-Boltzmann case can be done straight forwardly,
\[ Z_{N}\br{V,T} = {\sum_{\bc{n_\vep}}}' \br{\prod_\vep \f{1}{n_\vep !}} e^{-\be \sum_{\vep} n_\vep \vep} \]
Using multiplicative properties of exponentials,
\[ Z_{N}\br{V,T} = {\sum_{\bc{n_\vep}}}' \br{\prod_\vep \f{1}{n_\vep !}} \br{\prod_\vep e^{-\be \vep}}^{n_\vep} \]
Under the ``prime'' constraint that $\sum_\vep n_\vep = N$ it can be shown,
\[ Z_{N}\br{V,T} = \f{1}{N!}\br{\sum_\vep e^{-\be \vep}}^N \]
Treating the inner term as the partition function for a single state,
\[ Z_{N}\br{V,T} = \f{1}{N!}\br{Z_1\br{V,T}}^N \eq \label{eq:Zdecomp}\]
Again we can solve for the inner term,
\[ \sum_\vep e^{-\be \vep} = \sum_{k_x,k_y,k_z}e^{-\be\f{\hbar^2 k^2}{2m}} \]
For high temperature we can make the approximation that the discrete sum becomes an integral,
\[ \sum_{k_m} \ar \int \f{\dif k_m}{L /2 \pi} \]
Then we get,
\[ \br{\f{L}{2\pi}}^3 \iiint \dif k_x \dif k_y \dif k_z e^{-\be\f{\hbar^2 k^2}{2m}} = \f{V}{8 \pi^3} \int \br{4\pi k^2} e^{-\be \f{\hbar^2 k^2}{2m}} \dif k\]
Gaussian integrals our old friend! We can then write the single partition function,
\[ Z_{1}\br{V,T} = \f{V}{\lambda_T^3} \qquad \lambda_T = \f{h}{\br{2 \pi m k_B T}^{1/2}} \]
Making the combined partition function is given by \eqref{eq:Zdecomp},
\[ Z_{N}\br{V,T} = \br{\f{V}{\lambda_T^3}}^N \f{1}{N!}\]
This is the result for the canonical ensemble. The Grand canonical ensemble partition function immediately follows,
\[ Z\br{z, V, T} = \sum_{N = 0}^{\inf} z^N Q_N\br{V,T} \]
Where $z$ is $z = e^{-\be \mu}$ the fugacity as seen before. Notice this is the Taylor series expansion of $e^x$,
\[ Z\br{z, V, T} = e^{z \f{V}{\lambda_T^3}} \]
As a reminder, the analogous derivations of $Z_N$ for B.E. and F.D. distributions are not as tractable. \textit{However}, computing $Z$ (the grand canonical partition function) is tractable! In general,
\[ Z\br{z,V,T} = \sum_{N = 0}^\inf z^N Z_{N}\br{N,V,T} = \sum_{N=0}^\inf {\sum_{\bc{n_\vep}}}' \bs{\prod_\vep \br{z e^{-\be \vep}}^{n_\vep}} \]
Looks messy! However, now that we have the sum of all $N$, we can \textit{remove} the constraint of the prime on the summation. This gives,
\[ Z\br{z,V,T} = \sum_{n_0}\sum_{n_1}\cdots\sum_{n_\vep} \br{z e^{-\be \vep}}^{n_{0}}\br{z e^{-\be \vep}}^{n_{1}} \cdots\br{z e^{-\be \vep}}^{n_{\vep}} \]
The nested summations are separable,
\[ Z\br{z,V,T} = \sum_{n_0}\br{z e^{-\be \vep}}^{n_{0}} \cdot \sum_{n_1}\br{z e^{-\be \vep}}^{n_{1}} \cdots \sum_{n_\vep} \br{z e^{-\be \vep}}^{n_{\vep}} \]
\[ Z\br{z,V,T} = \prod_\vep \br{\sum_{n_\vep} \br{z e^{-\be \vep}}^{n_\vep}} \eq\label{eq:canonicalresult}\]
Recall that for F.D. $n_\vep = 0,1$ which gives,
\[ Z\tsb{FD}\br{z,V,T} = \prod_\vep \br{1 + z e^{-\be z}} \]
Whereas for B.E. we can utilize the geometric series,
\[ \sum_{n = 0}^{\inf} x^n = \f{1}{1-x} \note{For $\abs{x} < 1$}\]
Letting $x = ze^{-\be \vep} = e^{+\be\mu}e^{-\be\vep}$,
\[ Z\tsb{BE}\br{z,V,T} = \prod_\vep\br{\f{1}{1 - z e^{-\be \vep}}}  \]
Suggestively, these two distributions can be written together,
\[ Z_{\text{FD};\text{BE}}\br{z,V,T} = \prod_\vep\br{1 \pm z e^{-\be \vep}}^{\pm1}  \]
Recalling a result we obtained earlier when looking at $\eta$ in \eqref{eq:eta},
\[ PV = k_B T \ln \br{Z\br{z,V,T}} = \pm k_B T \sum_\vep \ln \br{1 \pm z e^{-\be \vep}} \]
Or using $a = \bc{0, -1, +1}$ as in section \ref{sec:a},
\[ PV = \f{k_B T}{a} \sum_\vep \ln \br{1 + z ae^{-\be \vep}} \]
Other results that follow from \eqref{eq:canonicalresult} include,
\[ \bar{N} = z \pdert{\ln Z}{z}{V,T} = \sum_\vep \f{1}{z^{-1}e^{\be \vep} + a} \]
And,
\[ \bar{E} = - \pdert{\ln Z}{\be}{V,T} = \sum_\vep \f{\vep}{z^{-1}e^{\be \vep} + a} = \sum_\vep \vep e^{-\be \br{\vep - \mu}} \]
Perhaps most interestingly for a particular $\vep'$ (holding all other $\vep$ constant),
\[ \ba{n_{\vep'}} = \f{1}{Z} \bc{- \f{1}{\be} \pdert{Z}{\vep'}{z, T, \ldots \vep}} = \f{1}{e^{\be\br{\vep - \mu} + a}}\]
Which yields,
\[ \ba{n_{\vep'}}\tsb{BE} = \f{1}{e^{\be\br{\vep - \mu} - 1}}\]
\[ \ba{n_{\vep'}}\tsb{FD} = \f{1}{e^{\be\br{\vep - \mu} + 1}}\]
\[ \ba{n_{\vep'}}\tsb{MB} = e^{-\be\br{\vep - \mu}}\]
\[\]
\end{document}



